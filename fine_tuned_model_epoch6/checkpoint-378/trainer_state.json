{
  "best_metric": 0.22777973115444183,
  "best_model_checkpoint": "./fine_tuned_model_epoch6\\checkpoint-320",
  "epoch": 5.913385826771654,
  "eval_steps": 500,
  "global_step": 378,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.015748031496062992,
      "grad_norm": NaN,
      "learning_rate": 0.0002,
      "loss": 15.1967,
      "step": 1
    },
    {
      "epoch": 0.031496062992125984,
      "grad_norm": NaN,
      "learning_rate": 0.0002,
      "loss": 14.6667,
      "step": 2
    },
    {
      "epoch": 0.047244094488188976,
      "grad_norm": NaN,
      "learning_rate": 0.0002,
      "loss": 16.1435,
      "step": 3
    },
    {
      "epoch": 0.06299212598425197,
      "grad_norm": NaN,
      "learning_rate": 0.0002,
      "loss": 14.8392,
      "step": 4
    },
    {
      "epoch": 0.07874015748031496,
      "grad_norm": NaN,
      "learning_rate": 0.0002,
      "loss": 15.1822,
      "step": 5
    },
    {
      "epoch": 0.09448818897637795,
      "grad_norm": 510.213134765625,
      "learning_rate": 0.00019947089947089947,
      "loss": 14.6601,
      "step": 6
    },
    {
      "epoch": 0.11023622047244094,
      "grad_norm": 269.26531982421875,
      "learning_rate": 0.00019894179894179893,
      "loss": 9.4552,
      "step": 7
    },
    {
      "epoch": 0.12598425196850394,
      "grad_norm": 189.62327575683594,
      "learning_rate": 0.00019841269841269844,
      "loss": 4.4501,
      "step": 8
    },
    {
      "epoch": 0.14173228346456693,
      "grad_norm": 71.35624694824219,
      "learning_rate": 0.0001978835978835979,
      "loss": 1.6779,
      "step": 9
    },
    {
      "epoch": 0.15748031496062992,
      "grad_norm": 13.266929626464844,
      "learning_rate": 0.00019735449735449736,
      "loss": 1.1867,
      "step": 10
    },
    {
      "epoch": 0.1732283464566929,
      "grad_norm": 60.59260559082031,
      "learning_rate": 0.00019682539682539682,
      "loss": 1.21,
      "step": 11
    },
    {
      "epoch": 0.1889763779527559,
      "grad_norm": 4.509042739868164,
      "learning_rate": 0.0001962962962962963,
      "loss": 1.1688,
      "step": 12
    },
    {
      "epoch": 0.2047244094488189,
      "grad_norm": 4.070069313049316,
      "learning_rate": 0.0001957671957671958,
      "loss": 1.0605,
      "step": 13
    },
    {
      "epoch": 0.2204724409448819,
      "grad_norm": 4.3397603034973145,
      "learning_rate": 0.00019523809523809525,
      "loss": 1.0964,
      "step": 14
    },
    {
      "epoch": 0.23622047244094488,
      "grad_norm": 3.342273712158203,
      "learning_rate": 0.0001947089947089947,
      "loss": 0.873,
      "step": 15
    },
    {
      "epoch": 0.25196850393700787,
      "grad_norm": 4.352566242218018,
      "learning_rate": 0.0001941798941798942,
      "loss": 0.9661,
      "step": 16
    },
    {
      "epoch": 0.2677165354330709,
      "grad_norm": 3.449371814727783,
      "learning_rate": 0.00019365079365079365,
      "loss": 0.8944,
      "step": 17
    },
    {
      "epoch": 0.28346456692913385,
      "grad_norm": 3.1443464756011963,
      "learning_rate": 0.00019312169312169313,
      "loss": 0.9569,
      "step": 18
    },
    {
      "epoch": 0.2992125984251969,
      "grad_norm": 7.711576461791992,
      "learning_rate": 0.0001925925925925926,
      "loss": 0.7394,
      "step": 19
    },
    {
      "epoch": 0.31496062992125984,
      "grad_norm": 5.648837566375732,
      "learning_rate": 0.00019206349206349208,
      "loss": 0.919,
      "step": 20
    },
    {
      "epoch": 0.33070866141732286,
      "grad_norm": 3.3869428634643555,
      "learning_rate": 0.00019153439153439154,
      "loss": 0.7794,
      "step": 21
    },
    {
      "epoch": 0.3464566929133858,
      "grad_norm": 1.9515947103500366,
      "learning_rate": 0.00019100529100529102,
      "loss": 0.6236,
      "step": 22
    },
    {
      "epoch": 0.36220472440944884,
      "grad_norm": 15.39999008178711,
      "learning_rate": 0.00019047619047619048,
      "loss": 0.7073,
      "step": 23
    },
    {
      "epoch": 0.3779527559055118,
      "grad_norm": 2.249422788619995,
      "learning_rate": 0.00018994708994708997,
      "loss": 0.7598,
      "step": 24
    },
    {
      "epoch": 0.3937007874015748,
      "grad_norm": 3.9220471382141113,
      "learning_rate": 0.00018941798941798943,
      "loss": 0.5963,
      "step": 25
    },
    {
      "epoch": 0.4094488188976378,
      "grad_norm": 3.405858039855957,
      "learning_rate": 0.00018888888888888888,
      "loss": 0.7329,
      "step": 26
    },
    {
      "epoch": 0.4251968503937008,
      "grad_norm": 7.1386332511901855,
      "learning_rate": 0.00018835978835978837,
      "loss": 0.7325,
      "step": 27
    },
    {
      "epoch": 0.4409448818897638,
      "grad_norm": 1.7956933975219727,
      "learning_rate": 0.00018783068783068786,
      "loss": 0.4903,
      "step": 28
    },
    {
      "epoch": 0.4566929133858268,
      "grad_norm": 2.100281238555908,
      "learning_rate": 0.00018730158730158731,
      "loss": 0.54,
      "step": 29
    },
    {
      "epoch": 0.47244094488188976,
      "grad_norm": 1.7802621126174927,
      "learning_rate": 0.00018677248677248677,
      "loss": 0.5837,
      "step": 30
    },
    {
      "epoch": 0.4881889763779528,
      "grad_norm": 1.6986989974975586,
      "learning_rate": 0.00018624338624338623,
      "loss": 0.6301,
      "step": 31
    },
    {
      "epoch": 0.5039370078740157,
      "grad_norm": 1.9565259218215942,
      "learning_rate": 0.00018571428571428572,
      "loss": 0.5873,
      "step": 32
    },
    {
      "epoch": 0.5196850393700787,
      "grad_norm": 1.3513362407684326,
      "learning_rate": 0.0001851851851851852,
      "loss": 0.4897,
      "step": 33
    },
    {
      "epoch": 0.5354330708661418,
      "grad_norm": 1.2251708507537842,
      "learning_rate": 0.00018465608465608466,
      "loss": 0.4661,
      "step": 34
    },
    {
      "epoch": 0.5511811023622047,
      "grad_norm": 2.217172622680664,
      "learning_rate": 0.00018412698412698412,
      "loss": 0.5529,
      "step": 35
    },
    {
      "epoch": 0.5669291338582677,
      "grad_norm": 1.6737593412399292,
      "learning_rate": 0.0001835978835978836,
      "loss": 0.5608,
      "step": 36
    },
    {
      "epoch": 0.5826771653543307,
      "grad_norm": 1.8557223081588745,
      "learning_rate": 0.0001830687830687831,
      "loss": 0.4712,
      "step": 37
    },
    {
      "epoch": 0.5984251968503937,
      "grad_norm": 1.5736889839172363,
      "learning_rate": 0.00018253968253968255,
      "loss": 0.5236,
      "step": 38
    },
    {
      "epoch": 0.6141732283464567,
      "grad_norm": 1.906862735748291,
      "learning_rate": 0.000182010582010582,
      "loss": 0.537,
      "step": 39
    },
    {
      "epoch": 0.6299212598425197,
      "grad_norm": 2.102478504180908,
      "learning_rate": 0.0001814814814814815,
      "loss": 0.6836,
      "step": 40
    },
    {
      "epoch": 0.6456692913385826,
      "grad_norm": 1.6639623641967773,
      "learning_rate": 0.00018095238095238095,
      "loss": 0.4753,
      "step": 41
    },
    {
      "epoch": 0.6614173228346457,
      "grad_norm": 1.6338852643966675,
      "learning_rate": 0.00018042328042328044,
      "loss": 0.4865,
      "step": 42
    },
    {
      "epoch": 0.6771653543307087,
      "grad_norm": 1.1382324695587158,
      "learning_rate": 0.0001798941798941799,
      "loss": 0.3108,
      "step": 43
    },
    {
      "epoch": 0.6929133858267716,
      "grad_norm": 1.8400496244430542,
      "learning_rate": 0.00017936507936507938,
      "loss": 0.5022,
      "step": 44
    },
    {
      "epoch": 0.7086614173228346,
      "grad_norm": 1.923768401145935,
      "learning_rate": 0.00017883597883597884,
      "loss": 0.4411,
      "step": 45
    },
    {
      "epoch": 0.7244094488188977,
      "grad_norm": 1.5442978143692017,
      "learning_rate": 0.0001783068783068783,
      "loss": 0.4005,
      "step": 46
    },
    {
      "epoch": 0.7401574803149606,
      "grad_norm": 1.7601326704025269,
      "learning_rate": 0.00017777777777777779,
      "loss": 0.4627,
      "step": 47
    },
    {
      "epoch": 0.7559055118110236,
      "grad_norm": 1.1970407962799072,
      "learning_rate": 0.00017724867724867727,
      "loss": 0.3709,
      "step": 48
    },
    {
      "epoch": 0.7716535433070866,
      "grad_norm": 1.6476103067398071,
      "learning_rate": 0.00017671957671957673,
      "loss": 0.4233,
      "step": 49
    },
    {
      "epoch": 0.7874015748031497,
      "grad_norm": 1.6010044813156128,
      "learning_rate": 0.0001761904761904762,
      "loss": 0.5384,
      "step": 50
    },
    {
      "epoch": 0.8031496062992126,
      "grad_norm": 1.4559721946716309,
      "learning_rate": 0.00017566137566137565,
      "loss": 0.3667,
      "step": 51
    },
    {
      "epoch": 0.8188976377952756,
      "grad_norm": 1.5574088096618652,
      "learning_rate": 0.00017513227513227516,
      "loss": 0.3777,
      "step": 52
    },
    {
      "epoch": 0.8346456692913385,
      "grad_norm": 1.8701097965240479,
      "learning_rate": 0.00017460317460317462,
      "loss": 0.5132,
      "step": 53
    },
    {
      "epoch": 0.8503937007874016,
      "grad_norm": 1.6576741933822632,
      "learning_rate": 0.00017407407407407408,
      "loss": 0.397,
      "step": 54
    },
    {
      "epoch": 0.8661417322834646,
      "grad_norm": 1.3882824182510376,
      "learning_rate": 0.00017354497354497354,
      "loss": 0.3359,
      "step": 55
    },
    {
      "epoch": 0.8818897637795275,
      "grad_norm": 1.1386624574661255,
      "learning_rate": 0.00017301587301587302,
      "loss": 0.2991,
      "step": 56
    },
    {
      "epoch": 0.8976377952755905,
      "grad_norm": 1.3604185581207275,
      "learning_rate": 0.0001724867724867725,
      "loss": 0.3265,
      "step": 57
    },
    {
      "epoch": 0.9133858267716536,
      "grad_norm": 1.463508129119873,
      "learning_rate": 0.00017195767195767197,
      "loss": 0.3131,
      "step": 58
    },
    {
      "epoch": 0.9291338582677166,
      "grad_norm": 2.2826502323150635,
      "learning_rate": 0.00017142857142857143,
      "loss": 0.2659,
      "step": 59
    },
    {
      "epoch": 0.9448818897637795,
      "grad_norm": 4.253240585327148,
      "learning_rate": 0.0001708994708994709,
      "loss": 0.4524,
      "step": 60
    },
    {
      "epoch": 0.9606299212598425,
      "grad_norm": 18.441482543945312,
      "learning_rate": 0.00017037037037037037,
      "loss": 0.389,
      "step": 61
    },
    {
      "epoch": 0.9763779527559056,
      "grad_norm": 5.223081588745117,
      "learning_rate": 0.00016984126984126986,
      "loss": 0.4794,
      "step": 62
    },
    {
      "epoch": 0.9921259842519685,
      "grad_norm": 7.2971343994140625,
      "learning_rate": 0.00016931216931216931,
      "loss": 0.5129,
      "step": 63
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.7388044595718384,
      "learning_rate": 0.0001687830687830688,
      "loss": 0.1197,
      "step": 64
    },
    {
      "epoch": 1.0,
      "eval_loss": 0.3862014710903168,
      "eval_runtime": 22.4368,
      "eval_samples_per_second": 2.852,
      "eval_steps_per_second": 0.357,
      "step": 64
    },
    {
      "epoch": 1.015748031496063,
      "grad_norm": 2.967313766479492,
      "learning_rate": 0.00016825396825396826,
      "loss": 0.4526,
      "step": 65
    },
    {
      "epoch": 1.031496062992126,
      "grad_norm": 1.767856478691101,
      "learning_rate": 0.00016772486772486772,
      "loss": 0.3852,
      "step": 66
    },
    {
      "epoch": 1.047244094488189,
      "grad_norm": 1.3903801441192627,
      "learning_rate": 0.0001671957671957672,
      "loss": 0.3169,
      "step": 67
    },
    {
      "epoch": 1.0629921259842519,
      "grad_norm": 1.4495691061019897,
      "learning_rate": 0.0001666666666666667,
      "loss": 0.3576,
      "step": 68
    },
    {
      "epoch": 1.078740157480315,
      "grad_norm": 1.6001086235046387,
      "learning_rate": 0.00016613756613756615,
      "loss": 0.3343,
      "step": 69
    },
    {
      "epoch": 1.094488188976378,
      "grad_norm": 1.5953054428100586,
      "learning_rate": 0.0001656084656084656,
      "loss": 0.322,
      "step": 70
    },
    {
      "epoch": 1.110236220472441,
      "grad_norm": 1.369896650314331,
      "learning_rate": 0.0001650793650793651,
      "loss": 0.2788,
      "step": 71
    },
    {
      "epoch": 1.125984251968504,
      "grad_norm": 1.9963284730911255,
      "learning_rate": 0.00016455026455026458,
      "loss": 0.3223,
      "step": 72
    },
    {
      "epoch": 1.141732283464567,
      "grad_norm": 1.8815410137176514,
      "learning_rate": 0.00016402116402116404,
      "loss": 0.2784,
      "step": 73
    },
    {
      "epoch": 1.1574803149606299,
      "grad_norm": 1.8567408323287964,
      "learning_rate": 0.0001634920634920635,
      "loss": 0.3751,
      "step": 74
    },
    {
      "epoch": 1.1732283464566928,
      "grad_norm": 1.627401351928711,
      "learning_rate": 0.00016296296296296295,
      "loss": 0.2931,
      "step": 75
    },
    {
      "epoch": 1.188976377952756,
      "grad_norm": 3.0045711994171143,
      "learning_rate": 0.00016243386243386244,
      "loss": 0.4517,
      "step": 76
    },
    {
      "epoch": 1.204724409448819,
      "grad_norm": 2.2674500942230225,
      "learning_rate": 0.00016190476190476192,
      "loss": 0.3836,
      "step": 77
    },
    {
      "epoch": 1.220472440944882,
      "grad_norm": 1.561284065246582,
      "learning_rate": 0.00016137566137566138,
      "loss": 0.2814,
      "step": 78
    },
    {
      "epoch": 1.236220472440945,
      "grad_norm": 3.238189458847046,
      "learning_rate": 0.00016084656084656084,
      "loss": 0.4045,
      "step": 79
    },
    {
      "epoch": 1.2519685039370079,
      "grad_norm": 1.70847749710083,
      "learning_rate": 0.00016031746031746033,
      "loss": 0.2521,
      "step": 80
    },
    {
      "epoch": 1.2677165354330708,
      "grad_norm": 1.8479081392288208,
      "learning_rate": 0.00015978835978835979,
      "loss": 0.2958,
      "step": 81
    },
    {
      "epoch": 1.2834645669291338,
      "grad_norm": 2.3167378902435303,
      "learning_rate": 0.00015925925925925927,
      "loss": 0.4272,
      "step": 82
    },
    {
      "epoch": 1.2992125984251968,
      "grad_norm": 2.3211147785186768,
      "learning_rate": 0.00015873015873015873,
      "loss": 0.4042,
      "step": 83
    },
    {
      "epoch": 1.3149606299212597,
      "grad_norm": 1.9092819690704346,
      "learning_rate": 0.00015820105820105822,
      "loss": 0.3664,
      "step": 84
    },
    {
      "epoch": 1.330708661417323,
      "grad_norm": 2.0299811363220215,
      "learning_rate": 0.00015767195767195767,
      "loss": 0.3733,
      "step": 85
    },
    {
      "epoch": 1.3464566929133859,
      "grad_norm": 2.14105224609375,
      "learning_rate": 0.00015714285714285716,
      "loss": 0.2953,
      "step": 86
    },
    {
      "epoch": 1.3622047244094488,
      "grad_norm": 2.1230316162109375,
      "learning_rate": 0.00015661375661375662,
      "loss": 0.3777,
      "step": 87
    },
    {
      "epoch": 1.3779527559055118,
      "grad_norm": 1.7160556316375732,
      "learning_rate": 0.0001560846560846561,
      "loss": 0.3863,
      "step": 88
    },
    {
      "epoch": 1.3937007874015748,
      "grad_norm": 1.3300782442092896,
      "learning_rate": 0.00015555555555555556,
      "loss": 0.2316,
      "step": 89
    },
    {
      "epoch": 1.4094488188976377,
      "grad_norm": 1.657050609588623,
      "learning_rate": 0.00015502645502645502,
      "loss": 0.3084,
      "step": 90
    },
    {
      "epoch": 1.425196850393701,
      "grad_norm": 1.5645862817764282,
      "learning_rate": 0.0001544973544973545,
      "loss": 0.228,
      "step": 91
    },
    {
      "epoch": 1.4409448818897639,
      "grad_norm": 1.996552586555481,
      "learning_rate": 0.000153968253968254,
      "loss": 0.275,
      "step": 92
    },
    {
      "epoch": 1.4566929133858268,
      "grad_norm": 1.623932957649231,
      "learning_rate": 0.00015343915343915345,
      "loss": 0.331,
      "step": 93
    },
    {
      "epoch": 1.4724409448818898,
      "grad_norm": 1.4792717695236206,
      "learning_rate": 0.0001529100529100529,
      "loss": 0.2211,
      "step": 94
    },
    {
      "epoch": 1.4881889763779528,
      "grad_norm": 1.6592284440994263,
      "learning_rate": 0.00015238095238095237,
      "loss": 0.2272,
      "step": 95
    },
    {
      "epoch": 1.5039370078740157,
      "grad_norm": 2.012896776199341,
      "learning_rate": 0.00015185185185185185,
      "loss": 0.2886,
      "step": 96
    },
    {
      "epoch": 1.5196850393700787,
      "grad_norm": 2.2803354263305664,
      "learning_rate": 0.00015132275132275134,
      "loss": 0.3216,
      "step": 97
    },
    {
      "epoch": 1.5354330708661417,
      "grad_norm": 2.146841287612915,
      "learning_rate": 0.0001507936507936508,
      "loss": 0.3466,
      "step": 98
    },
    {
      "epoch": 1.5511811023622046,
      "grad_norm": 2.3633720874786377,
      "learning_rate": 0.00015026455026455026,
      "loss": 0.3543,
      "step": 99
    },
    {
      "epoch": 1.5669291338582676,
      "grad_norm": 1.9876399040222168,
      "learning_rate": 0.00014973544973544974,
      "loss": 0.3906,
      "step": 100
    },
    {
      "epoch": 1.5826771653543306,
      "grad_norm": 2.497924566268921,
      "learning_rate": 0.00014920634920634923,
      "loss": 0.3557,
      "step": 101
    },
    {
      "epoch": 1.5984251968503937,
      "grad_norm": 1.9956369400024414,
      "learning_rate": 0.0001486772486772487,
      "loss": 0.3101,
      "step": 102
    },
    {
      "epoch": 1.6141732283464567,
      "grad_norm": 1.8800486326217651,
      "learning_rate": 0.00014814814814814815,
      "loss": 0.3343,
      "step": 103
    },
    {
      "epoch": 1.6299212598425197,
      "grad_norm": 2.403163194656372,
      "learning_rate": 0.00014761904761904763,
      "loss": 0.4309,
      "step": 104
    },
    {
      "epoch": 1.6456692913385826,
      "grad_norm": 1.9790148735046387,
      "learning_rate": 0.0001470899470899471,
      "loss": 0.3408,
      "step": 105
    },
    {
      "epoch": 1.6614173228346458,
      "grad_norm": 2.0619919300079346,
      "learning_rate": 0.00014656084656084658,
      "loss": 0.253,
      "step": 106
    },
    {
      "epoch": 1.6771653543307088,
      "grad_norm": 1.9419212341308594,
      "learning_rate": 0.00014603174603174603,
      "loss": 0.3728,
      "step": 107
    },
    {
      "epoch": 1.6929133858267718,
      "grad_norm": 1.6030851602554321,
      "learning_rate": 0.00014550264550264552,
      "loss": 0.3138,
      "step": 108
    },
    {
      "epoch": 1.7086614173228347,
      "grad_norm": 2.290374755859375,
      "learning_rate": 0.00014497354497354498,
      "loss": 0.3775,
      "step": 109
    },
    {
      "epoch": 1.7244094488188977,
      "grad_norm": 1.847936749458313,
      "learning_rate": 0.00014444444444444444,
      "loss": 0.2526,
      "step": 110
    },
    {
      "epoch": 1.7401574803149606,
      "grad_norm": 1.541277289390564,
      "learning_rate": 0.00014391534391534392,
      "loss": 0.2479,
      "step": 111
    },
    {
      "epoch": 1.7559055118110236,
      "grad_norm": 2.150972843170166,
      "learning_rate": 0.0001433862433862434,
      "loss": 0.3764,
      "step": 112
    },
    {
      "epoch": 1.7716535433070866,
      "grad_norm": 1.8146551847457886,
      "learning_rate": 0.00014285714285714287,
      "loss": 0.295,
      "step": 113
    },
    {
      "epoch": 1.7874015748031495,
      "grad_norm": 1.8173434734344482,
      "learning_rate": 0.00014232804232804233,
      "loss": 0.3353,
      "step": 114
    },
    {
      "epoch": 1.8031496062992125,
      "grad_norm": 2.265099287033081,
      "learning_rate": 0.00014179894179894179,
      "loss": 0.2627,
      "step": 115
    },
    {
      "epoch": 1.8188976377952755,
      "grad_norm": 2.0115604400634766,
      "learning_rate": 0.0001412698412698413,
      "loss": 0.3746,
      "step": 116
    },
    {
      "epoch": 1.8346456692913384,
      "grad_norm": 1.472740888595581,
      "learning_rate": 0.00014074074074074076,
      "loss": 0.2469,
      "step": 117
    },
    {
      "epoch": 1.8503937007874016,
      "grad_norm": 2.08341908454895,
      "learning_rate": 0.00014021164021164022,
      "loss": 0.3549,
      "step": 118
    },
    {
      "epoch": 1.8661417322834646,
      "grad_norm": 1.9720659255981445,
      "learning_rate": 0.00013968253968253967,
      "loss": 0.3251,
      "step": 119
    },
    {
      "epoch": 1.8818897637795275,
      "grad_norm": 1.3540267944335938,
      "learning_rate": 0.00013915343915343916,
      "loss": 0.2085,
      "step": 120
    },
    {
      "epoch": 1.8976377952755905,
      "grad_norm": 1.5701689720153809,
      "learning_rate": 0.00013862433862433865,
      "loss": 0.2391,
      "step": 121
    },
    {
      "epoch": 1.9133858267716537,
      "grad_norm": 1.5458194017410278,
      "learning_rate": 0.0001380952380952381,
      "loss": 0.2419,
      "step": 122
    },
    {
      "epoch": 1.9291338582677167,
      "grad_norm": 2.3126349449157715,
      "learning_rate": 0.00013756613756613756,
      "loss": 0.3371,
      "step": 123
    },
    {
      "epoch": 1.9448818897637796,
      "grad_norm": 2.3960282802581787,
      "learning_rate": 0.00013703703703703705,
      "loss": 0.3274,
      "step": 124
    },
    {
      "epoch": 1.9606299212598426,
      "grad_norm": 1.8354499340057373,
      "learning_rate": 0.0001365079365079365,
      "loss": 0.3156,
      "step": 125
    },
    {
      "epoch": 1.9763779527559056,
      "grad_norm": 1.784040093421936,
      "learning_rate": 0.000135978835978836,
      "loss": 0.2837,
      "step": 126
    },
    {
      "epoch": 1.9921259842519685,
      "grad_norm": 1.7299343347549438,
      "learning_rate": 0.00013544973544973545,
      "loss": 0.2428,
      "step": 127
    },
    {
      "epoch": 2.0,
      "grad_norm": 1.6446466445922852,
      "learning_rate": 0.00013492063492063494,
      "loss": 0.2068,
      "step": 128
    },
    {
      "epoch": 2.0,
      "eval_loss": 0.3193695545196533,
      "eval_runtime": 21.6292,
      "eval_samples_per_second": 2.959,
      "eval_steps_per_second": 0.37,
      "step": 128
    },
    {
      "epoch": 2.015748031496063,
      "grad_norm": 2.0143301486968994,
      "learning_rate": 0.0001343915343915344,
      "loss": 0.3284,
      "step": 129
    },
    {
      "epoch": 2.031496062992126,
      "grad_norm": 1.2145745754241943,
      "learning_rate": 0.00013386243386243385,
      "loss": 0.1608,
      "step": 130
    },
    {
      "epoch": 2.047244094488189,
      "grad_norm": 1.6283142566680908,
      "learning_rate": 0.00013333333333333334,
      "loss": 0.2402,
      "step": 131
    },
    {
      "epoch": 2.062992125984252,
      "grad_norm": 1.661484956741333,
      "learning_rate": 0.00013280423280423283,
      "loss": 0.2647,
      "step": 132
    },
    {
      "epoch": 2.078740157480315,
      "grad_norm": 1.9566259384155273,
      "learning_rate": 0.00013227513227513228,
      "loss": 0.3632,
      "step": 133
    },
    {
      "epoch": 2.094488188976378,
      "grad_norm": 1.9762120246887207,
      "learning_rate": 0.00013174603174603174,
      "loss": 0.3203,
      "step": 134
    },
    {
      "epoch": 2.1102362204724407,
      "grad_norm": 2.1217596530914307,
      "learning_rate": 0.00013121693121693123,
      "loss": 0.3276,
      "step": 135
    },
    {
      "epoch": 2.1259842519685037,
      "grad_norm": 1.592093586921692,
      "learning_rate": 0.00013068783068783071,
      "loss": 0.2016,
      "step": 136
    },
    {
      "epoch": 2.141732283464567,
      "grad_norm": 1.701229453086853,
      "learning_rate": 0.00013015873015873017,
      "loss": 0.2314,
      "step": 137
    },
    {
      "epoch": 2.15748031496063,
      "grad_norm": 1.3775471448898315,
      "learning_rate": 0.00012962962962962963,
      "loss": 0.175,
      "step": 138
    },
    {
      "epoch": 2.173228346456693,
      "grad_norm": 1.916306495666504,
      "learning_rate": 0.0001291005291005291,
      "loss": 0.2592,
      "step": 139
    },
    {
      "epoch": 2.188976377952756,
      "grad_norm": 1.8250353336334229,
      "learning_rate": 0.00012857142857142858,
      "loss": 0.2005,
      "step": 140
    },
    {
      "epoch": 2.204724409448819,
      "grad_norm": 2.41485857963562,
      "learning_rate": 0.00012804232804232806,
      "loss": 0.2939,
      "step": 141
    },
    {
      "epoch": 2.220472440944882,
      "grad_norm": 2.4962105751037598,
      "learning_rate": 0.00012751322751322752,
      "loss": 0.3085,
      "step": 142
    },
    {
      "epoch": 2.236220472440945,
      "grad_norm": 1.8591054677963257,
      "learning_rate": 0.00012698412698412698,
      "loss": 0.1765,
      "step": 143
    },
    {
      "epoch": 2.251968503937008,
      "grad_norm": 1.6137030124664307,
      "learning_rate": 0.00012645502645502646,
      "loss": 0.175,
      "step": 144
    },
    {
      "epoch": 2.267716535433071,
      "grad_norm": 3.219698190689087,
      "learning_rate": 0.00012592592592592592,
      "loss": 0.3181,
      "step": 145
    },
    {
      "epoch": 2.283464566929134,
      "grad_norm": 2.3446872234344482,
      "learning_rate": 0.0001253968253968254,
      "loss": 0.2133,
      "step": 146
    },
    {
      "epoch": 2.2992125984251968,
      "grad_norm": 1.8694204092025757,
      "learning_rate": 0.00012486772486772487,
      "loss": 0.2413,
      "step": 147
    },
    {
      "epoch": 2.3149606299212597,
      "grad_norm": 2.20230770111084,
      "learning_rate": 0.00012433862433862435,
      "loss": 0.2112,
      "step": 148
    },
    {
      "epoch": 2.3307086614173227,
      "grad_norm": 2.4028687477111816,
      "learning_rate": 0.0001238095238095238,
      "loss": 0.2834,
      "step": 149
    },
    {
      "epoch": 2.3464566929133857,
      "grad_norm": 1.9996405839920044,
      "learning_rate": 0.0001232804232804233,
      "loss": 0.2327,
      "step": 150
    },
    {
      "epoch": 2.362204724409449,
      "grad_norm": 2.569622278213501,
      "learning_rate": 0.00012275132275132276,
      "loss": 0.2861,
      "step": 151
    },
    {
      "epoch": 2.377952755905512,
      "grad_norm": 1.8883116245269775,
      "learning_rate": 0.00012222222222222224,
      "loss": 0.2254,
      "step": 152
    },
    {
      "epoch": 2.393700787401575,
      "grad_norm": 1.5323890447616577,
      "learning_rate": 0.0001216931216931217,
      "loss": 0.1526,
      "step": 153
    },
    {
      "epoch": 2.409448818897638,
      "grad_norm": 1.8022186756134033,
      "learning_rate": 0.00012116402116402117,
      "loss": 0.2826,
      "step": 154
    },
    {
      "epoch": 2.425196850393701,
      "grad_norm": 2.161085844039917,
      "learning_rate": 0.00012063492063492063,
      "loss": 0.2439,
      "step": 155
    },
    {
      "epoch": 2.440944881889764,
      "grad_norm": 1.821731686592102,
      "learning_rate": 0.00012010582010582012,
      "loss": 0.2603,
      "step": 156
    },
    {
      "epoch": 2.456692913385827,
      "grad_norm": 1.9040807485580444,
      "learning_rate": 0.00011957671957671959,
      "loss": 0.1881,
      "step": 157
    },
    {
      "epoch": 2.47244094488189,
      "grad_norm": 2.142972946166992,
      "learning_rate": 0.00011904761904761905,
      "loss": 0.297,
      "step": 158
    },
    {
      "epoch": 2.4881889763779528,
      "grad_norm": 2.5287814140319824,
      "learning_rate": 0.00011851851851851852,
      "loss": 0.3299,
      "step": 159
    },
    {
      "epoch": 2.5039370078740157,
      "grad_norm": 1.7834868431091309,
      "learning_rate": 0.000117989417989418,
      "loss": 0.2424,
      "step": 160
    },
    {
      "epoch": 2.5196850393700787,
      "grad_norm": 2.359997272491455,
      "learning_rate": 0.00011746031746031746,
      "loss": 0.241,
      "step": 161
    },
    {
      "epoch": 2.5354330708661417,
      "grad_norm": 1.670274257659912,
      "learning_rate": 0.00011693121693121694,
      "loss": 0.1616,
      "step": 162
    },
    {
      "epoch": 2.5511811023622046,
      "grad_norm": 2.393167495727539,
      "learning_rate": 0.0001164021164021164,
      "loss": 0.2614,
      "step": 163
    },
    {
      "epoch": 2.5669291338582676,
      "grad_norm": 1.5564504861831665,
      "learning_rate": 0.0001158730158730159,
      "loss": 0.1779,
      "step": 164
    },
    {
      "epoch": 2.5826771653543306,
      "grad_norm": 2.4921822547912598,
      "learning_rate": 0.00011534391534391535,
      "loss": 0.284,
      "step": 165
    },
    {
      "epoch": 2.5984251968503935,
      "grad_norm": 2.973318099975586,
      "learning_rate": 0.00011481481481481482,
      "loss": 0.2208,
      "step": 166
    },
    {
      "epoch": 2.6141732283464565,
      "grad_norm": 2.6080288887023926,
      "learning_rate": 0.00011428571428571428,
      "loss": 0.2717,
      "step": 167
    },
    {
      "epoch": 2.6299212598425195,
      "grad_norm": 3.175208806991577,
      "learning_rate": 0.00011375661375661377,
      "loss": 0.3039,
      "step": 168
    },
    {
      "epoch": 2.6456692913385824,
      "grad_norm": 1.8178411722183228,
      "learning_rate": 0.00011322751322751324,
      "loss": 0.194,
      "step": 169
    },
    {
      "epoch": 2.661417322834646,
      "grad_norm": 2.558419942855835,
      "learning_rate": 0.0001126984126984127,
      "loss": 0.3092,
      "step": 170
    },
    {
      "epoch": 2.677165354330709,
      "grad_norm": 3.111704111099243,
      "learning_rate": 0.00011216931216931217,
      "loss": 0.3103,
      "step": 171
    },
    {
      "epoch": 2.6929133858267718,
      "grad_norm": 1.4806218147277832,
      "learning_rate": 0.00011164021164021166,
      "loss": 0.1894,
      "step": 172
    },
    {
      "epoch": 2.7086614173228347,
      "grad_norm": 2.6736481189727783,
      "learning_rate": 0.00011111111111111112,
      "loss": 0.251,
      "step": 173
    },
    {
      "epoch": 2.7244094488188977,
      "grad_norm": 2.990943193435669,
      "learning_rate": 0.00011058201058201059,
      "loss": 0.2515,
      "step": 174
    },
    {
      "epoch": 2.7401574803149606,
      "grad_norm": 2.9705214500427246,
      "learning_rate": 0.00011005291005291005,
      "loss": 0.257,
      "step": 175
    },
    {
      "epoch": 2.7559055118110236,
      "grad_norm": 2.2698256969451904,
      "learning_rate": 0.00010952380952380953,
      "loss": 0.243,
      "step": 176
    },
    {
      "epoch": 2.7716535433070866,
      "grad_norm": 3.3235230445861816,
      "learning_rate": 0.000108994708994709,
      "loss": 0.292,
      "step": 177
    },
    {
      "epoch": 2.7874015748031495,
      "grad_norm": 2.769953489303589,
      "learning_rate": 0.00010846560846560846,
      "loss": 0.3101,
      "step": 178
    },
    {
      "epoch": 2.8031496062992125,
      "grad_norm": 2.8992083072662354,
      "learning_rate": 0.00010793650793650794,
      "loss": 0.2952,
      "step": 179
    },
    {
      "epoch": 2.8188976377952755,
      "grad_norm": 2.080101728439331,
      "learning_rate": 0.00010740740740740742,
      "loss": 0.1819,
      "step": 180
    },
    {
      "epoch": 2.8346456692913384,
      "grad_norm": 1.6260002851486206,
      "learning_rate": 0.0001068783068783069,
      "loss": 0.149,
      "step": 181
    },
    {
      "epoch": 2.850393700787402,
      "grad_norm": 2.729027271270752,
      "learning_rate": 0.00010634920634920635,
      "loss": 0.2261,
      "step": 182
    },
    {
      "epoch": 2.866141732283465,
      "grad_norm": 2.410829544067383,
      "learning_rate": 0.00010582010582010582,
      "loss": 0.294,
      "step": 183
    },
    {
      "epoch": 2.8818897637795278,
      "grad_norm": 2.752202272415161,
      "learning_rate": 0.00010529100529100531,
      "loss": 0.3276,
      "step": 184
    },
    {
      "epoch": 2.8976377952755907,
      "grad_norm": 1.559165120124817,
      "learning_rate": 0.00010476190476190477,
      "loss": 0.1391,
      "step": 185
    },
    {
      "epoch": 2.9133858267716537,
      "grad_norm": 1.8894141912460327,
      "learning_rate": 0.00010423280423280424,
      "loss": 0.1665,
      "step": 186
    },
    {
      "epoch": 2.9291338582677167,
      "grad_norm": 2.252819061279297,
      "learning_rate": 0.0001037037037037037,
      "loss": 0.2301,
      "step": 187
    },
    {
      "epoch": 2.9448818897637796,
      "grad_norm": 2.330960273742676,
      "learning_rate": 0.00010317460317460319,
      "loss": 0.235,
      "step": 188
    },
    {
      "epoch": 2.9606299212598426,
      "grad_norm": 2.5719497203826904,
      "learning_rate": 0.00010264550264550266,
      "loss": 0.2347,
      "step": 189
    },
    {
      "epoch": 2.9763779527559056,
      "grad_norm": 2.098750114440918,
      "learning_rate": 0.00010211640211640212,
      "loss": 0.1751,
      "step": 190
    },
    {
      "epoch": 2.9921259842519685,
      "grad_norm": 2.409226655960083,
      "learning_rate": 0.00010158730158730159,
      "loss": 0.2604,
      "step": 191
    },
    {
      "epoch": 3.0,
      "grad_norm": 2.1993765830993652,
      "learning_rate": 0.00010105820105820107,
      "loss": 0.1795,
      "step": 192
    },
    {
      "epoch": 3.0,
      "eval_loss": 0.2962931990623474,
      "eval_runtime": 75.1933,
      "eval_samples_per_second": 0.851,
      "eval_steps_per_second": 0.106,
      "step": 192
    },
    {
      "epoch": 3.015748031496063,
      "grad_norm": 2.6223654747009277,
      "learning_rate": 0.00010052910052910055,
      "loss": 0.1717,
      "step": 193
    },
    {
      "epoch": 3.031496062992126,
      "grad_norm": 2.2088348865509033,
      "learning_rate": 0.0001,
      "loss": 0.244,
      "step": 194
    },
    {
      "epoch": 3.047244094488189,
      "grad_norm": 2.6597511768341064,
      "learning_rate": 9.947089947089946e-05,
      "loss": 0.2638,
      "step": 195
    },
    {
      "epoch": 3.062992125984252,
      "grad_norm": 2.347801446914673,
      "learning_rate": 9.894179894179895e-05,
      "loss": 0.2374,
      "step": 196
    },
    {
      "epoch": 3.078740157480315,
      "grad_norm": 2.154057264328003,
      "learning_rate": 9.841269841269841e-05,
      "loss": 0.1416,
      "step": 197
    },
    {
      "epoch": 3.094488188976378,
      "grad_norm": 2.5325441360473633,
      "learning_rate": 9.78835978835979e-05,
      "loss": 0.2589,
      "step": 198
    },
    {
      "epoch": 3.1102362204724407,
      "grad_norm": 17.381011962890625,
      "learning_rate": 9.735449735449735e-05,
      "loss": 0.3166,
      "step": 199
    },
    {
      "epoch": 3.1259842519685037,
      "grad_norm": 3.0146589279174805,
      "learning_rate": 9.682539682539682e-05,
      "loss": 0.3416,
      "step": 200
    },
    {
      "epoch": 3.141732283464567,
      "grad_norm": 3.385694980621338,
      "learning_rate": 9.62962962962963e-05,
      "loss": 0.2075,
      "step": 201
    },
    {
      "epoch": 3.15748031496063,
      "grad_norm": 2.563081979751587,
      "learning_rate": 9.576719576719577e-05,
      "loss": 0.1692,
      "step": 202
    },
    {
      "epoch": 3.173228346456693,
      "grad_norm": 1.9800124168395996,
      "learning_rate": 9.523809523809524e-05,
      "loss": 0.1375,
      "step": 203
    },
    {
      "epoch": 3.188976377952756,
      "grad_norm": 3.316166400909424,
      "learning_rate": 9.470899470899471e-05,
      "loss": 0.2463,
      "step": 204
    },
    {
      "epoch": 3.204724409448819,
      "grad_norm": 2.5955145359039307,
      "learning_rate": 9.417989417989419e-05,
      "loss": 0.1191,
      "step": 205
    },
    {
      "epoch": 3.220472440944882,
      "grad_norm": 3.8437066078186035,
      "learning_rate": 9.365079365079366e-05,
      "loss": 0.2283,
      "step": 206
    },
    {
      "epoch": 3.236220472440945,
      "grad_norm": 2.59237003326416,
      "learning_rate": 9.312169312169312e-05,
      "loss": 0.1948,
      "step": 207
    },
    {
      "epoch": 3.251968503937008,
      "grad_norm": 2.5996346473693848,
      "learning_rate": 9.25925925925926e-05,
      "loss": 0.1977,
      "step": 208
    },
    {
      "epoch": 3.267716535433071,
      "grad_norm": 2.1077957153320312,
      "learning_rate": 9.206349206349206e-05,
      "loss": 0.1571,
      "step": 209
    },
    {
      "epoch": 3.283464566929134,
      "grad_norm": 1.6996715068817139,
      "learning_rate": 9.153439153439155e-05,
      "loss": 0.1246,
      "step": 210
    },
    {
      "epoch": 3.2992125984251968,
      "grad_norm": 2.1779911518096924,
      "learning_rate": 9.1005291005291e-05,
      "loss": 0.169,
      "step": 211
    },
    {
      "epoch": 3.3149606299212597,
      "grad_norm": 2.3444745540618896,
      "learning_rate": 9.047619047619048e-05,
      "loss": 0.1197,
      "step": 212
    },
    {
      "epoch": 3.3307086614173227,
      "grad_norm": 1.4443484544754028,
      "learning_rate": 8.994708994708995e-05,
      "loss": 0.1426,
      "step": 213
    },
    {
      "epoch": 3.3464566929133857,
      "grad_norm": 2.249687910079956,
      "learning_rate": 8.941798941798942e-05,
      "loss": 0.1538,
      "step": 214
    },
    {
      "epoch": 3.362204724409449,
      "grad_norm": 2.32265043258667,
      "learning_rate": 8.888888888888889e-05,
      "loss": 0.2024,
      "step": 215
    },
    {
      "epoch": 3.377952755905512,
      "grad_norm": 2.6839542388916016,
      "learning_rate": 8.835978835978837e-05,
      "loss": 0.1732,
      "step": 216
    },
    {
      "epoch": 3.393700787401575,
      "grad_norm": 2.7073261737823486,
      "learning_rate": 8.783068783068782e-05,
      "loss": 0.1523,
      "step": 217
    },
    {
      "epoch": 3.409448818897638,
      "grad_norm": 2.707582712173462,
      "learning_rate": 8.730158730158731e-05,
      "loss": 0.2287,
      "step": 218
    },
    {
      "epoch": 3.425196850393701,
      "grad_norm": 2.6901960372924805,
      "learning_rate": 8.677248677248677e-05,
      "loss": 0.162,
      "step": 219
    },
    {
      "epoch": 3.440944881889764,
      "grad_norm": 3.0494306087493896,
      "learning_rate": 8.624338624338625e-05,
      "loss": 0.1301,
      "step": 220
    },
    {
      "epoch": 3.456692913385827,
      "grad_norm": 2.518871307373047,
      "learning_rate": 8.571428571428571e-05,
      "loss": 0.1554,
      "step": 221
    },
    {
      "epoch": 3.47244094488189,
      "grad_norm": 2.448605537414551,
      "learning_rate": 8.518518518518518e-05,
      "loss": 0.1457,
      "step": 222
    },
    {
      "epoch": 3.4881889763779528,
      "grad_norm": 2.281068801879883,
      "learning_rate": 8.465608465608466e-05,
      "loss": 0.1646,
      "step": 223
    },
    {
      "epoch": 3.5039370078740157,
      "grad_norm": 2.9600934982299805,
      "learning_rate": 8.412698412698413e-05,
      "loss": 0.1828,
      "step": 224
    },
    {
      "epoch": 3.5196850393700787,
      "grad_norm": 2.4201560020446777,
      "learning_rate": 8.35978835978836e-05,
      "loss": 0.1602,
      "step": 225
    },
    {
      "epoch": 3.5354330708661417,
      "grad_norm": 2.9241604804992676,
      "learning_rate": 8.306878306878307e-05,
      "loss": 0.1559,
      "step": 226
    },
    {
      "epoch": 3.5511811023622046,
      "grad_norm": 2.7662606239318848,
      "learning_rate": 8.253968253968255e-05,
      "loss": 0.1888,
      "step": 227
    },
    {
      "epoch": 3.5669291338582676,
      "grad_norm": 2.223564863204956,
      "learning_rate": 8.201058201058202e-05,
      "loss": 0.1585,
      "step": 228
    },
    {
      "epoch": 3.5826771653543306,
      "grad_norm": 2.081737756729126,
      "learning_rate": 8.148148148148148e-05,
      "loss": 0.0882,
      "step": 229
    },
    {
      "epoch": 3.5984251968503935,
      "grad_norm": 3.624234914779663,
      "learning_rate": 8.095238095238096e-05,
      "loss": 0.1765,
      "step": 230
    },
    {
      "epoch": 3.6141732283464565,
      "grad_norm": 2.381027936935425,
      "learning_rate": 8.042328042328042e-05,
      "loss": 0.0891,
      "step": 231
    },
    {
      "epoch": 3.6299212598425195,
      "grad_norm": 2.1471757888793945,
      "learning_rate": 7.989417989417989e-05,
      "loss": 0.1347,
      "step": 232
    },
    {
      "epoch": 3.6456692913385824,
      "grad_norm": 2.760270118713379,
      "learning_rate": 7.936507936507937e-05,
      "loss": 0.1879,
      "step": 233
    },
    {
      "epoch": 3.661417322834646,
      "grad_norm": 3.6311028003692627,
      "learning_rate": 7.883597883597884e-05,
      "loss": 0.2101,
      "step": 234
    },
    {
      "epoch": 3.677165354330709,
      "grad_norm": 3.5848238468170166,
      "learning_rate": 7.830687830687831e-05,
      "loss": 0.2126,
      "step": 235
    },
    {
      "epoch": 3.6929133858267718,
      "grad_norm": 2.9069528579711914,
      "learning_rate": 7.777777777777778e-05,
      "loss": 0.1766,
      "step": 236
    },
    {
      "epoch": 3.7086614173228347,
      "grad_norm": 2.5179522037506104,
      "learning_rate": 7.724867724867725e-05,
      "loss": 0.1849,
      "step": 237
    },
    {
      "epoch": 3.7244094488188977,
      "grad_norm": 2.3298404216766357,
      "learning_rate": 7.671957671957673e-05,
      "loss": 0.096,
      "step": 238
    },
    {
      "epoch": 3.7401574803149606,
      "grad_norm": 2.6569457054138184,
      "learning_rate": 7.619047619047618e-05,
      "loss": 0.1655,
      "step": 239
    },
    {
      "epoch": 3.7559055118110236,
      "grad_norm": 2.2531821727752686,
      "learning_rate": 7.566137566137567e-05,
      "loss": 0.1291,
      "step": 240
    },
    {
      "epoch": 3.7716535433070866,
      "grad_norm": 2.974520683288574,
      "learning_rate": 7.513227513227513e-05,
      "loss": 0.171,
      "step": 241
    },
    {
      "epoch": 3.7874015748031495,
      "grad_norm": 2.3950557708740234,
      "learning_rate": 7.460317460317461e-05,
      "loss": 0.1583,
      "step": 242
    },
    {
      "epoch": 3.8031496062992125,
      "grad_norm": 2.133030652999878,
      "learning_rate": 7.407407407407407e-05,
      "loss": 0.1823,
      "step": 243
    },
    {
      "epoch": 3.8188976377952755,
      "grad_norm": 3.0104713439941406,
      "learning_rate": 7.354497354497355e-05,
      "loss": 0.2604,
      "step": 244
    },
    {
      "epoch": 3.8346456692913384,
      "grad_norm": 2.56745982170105,
      "learning_rate": 7.301587301587302e-05,
      "loss": 0.2224,
      "step": 245
    },
    {
      "epoch": 3.850393700787402,
      "grad_norm": 2.9655025005340576,
      "learning_rate": 7.248677248677249e-05,
      "loss": 0.1771,
      "step": 246
    },
    {
      "epoch": 3.866141732283465,
      "grad_norm": 2.203533887863159,
      "learning_rate": 7.195767195767196e-05,
      "loss": 0.1282,
      "step": 247
    },
    {
      "epoch": 3.8818897637795278,
      "grad_norm": 3.6511521339416504,
      "learning_rate": 7.142857142857143e-05,
      "loss": 0.2681,
      "step": 248
    },
    {
      "epoch": 3.8976377952755907,
      "grad_norm": 2.553262710571289,
      "learning_rate": 7.089947089947089e-05,
      "loss": 0.1389,
      "step": 249
    },
    {
      "epoch": 3.9133858267716537,
      "grad_norm": 2.323084831237793,
      "learning_rate": 7.037037037037038e-05,
      "loss": 0.1215,
      "step": 250
    },
    {
      "epoch": 3.9291338582677167,
      "grad_norm": 3.2175681591033936,
      "learning_rate": 6.984126984126984e-05,
      "loss": 0.1827,
      "step": 251
    },
    {
      "epoch": 3.9448818897637796,
      "grad_norm": 2.4799718856811523,
      "learning_rate": 6.931216931216932e-05,
      "loss": 0.1844,
      "step": 252
    },
    {
      "epoch": 3.9606299212598426,
      "grad_norm": 1.642297387123108,
      "learning_rate": 6.878306878306878e-05,
      "loss": 0.0651,
      "step": 253
    },
    {
      "epoch": 3.9763779527559056,
      "grad_norm": 3.0348832607269287,
      "learning_rate": 6.825396825396825e-05,
      "loss": 0.2104,
      "step": 254
    },
    {
      "epoch": 3.9921259842519685,
      "grad_norm": 2.8224358558654785,
      "learning_rate": 6.772486772486773e-05,
      "loss": 0.1496,
      "step": 255
    },
    {
      "epoch": 4.0,
      "grad_norm": 1.6469943523406982,
      "learning_rate": 6.71957671957672e-05,
      "loss": 0.0511,
      "step": 256
    },
    {
      "epoch": 4.0,
      "eval_loss": 0.23694242537021637,
      "eval_runtime": 26.9431,
      "eval_samples_per_second": 2.375,
      "eval_steps_per_second": 0.297,
      "step": 256
    },
    {
      "epoch": 4.015748031496063,
      "grad_norm": 1.8855518102645874,
      "learning_rate": 6.666666666666667e-05,
      "loss": 0.1022,
      "step": 257
    },
    {
      "epoch": 4.031496062992126,
      "grad_norm": 2.416445732116699,
      "learning_rate": 6.613756613756614e-05,
      "loss": 0.1395,
      "step": 258
    },
    {
      "epoch": 4.047244094488189,
      "grad_norm": 1.8975110054016113,
      "learning_rate": 6.560846560846561e-05,
      "loss": 0.0765,
      "step": 259
    },
    {
      "epoch": 4.062992125984252,
      "grad_norm": 2.581369400024414,
      "learning_rate": 6.507936507936509e-05,
      "loss": 0.1167,
      "step": 260
    },
    {
      "epoch": 4.078740157480315,
      "grad_norm": 4.081516265869141,
      "learning_rate": 6.455026455026454e-05,
      "loss": 0.1296,
      "step": 261
    },
    {
      "epoch": 4.094488188976378,
      "grad_norm": 3.2160041332244873,
      "learning_rate": 6.402116402116403e-05,
      "loss": 0.1908,
      "step": 262
    },
    {
      "epoch": 4.110236220472441,
      "grad_norm": 2.693803548812866,
      "learning_rate": 6.349206349206349e-05,
      "loss": 0.1249,
      "step": 263
    },
    {
      "epoch": 4.125984251968504,
      "grad_norm": 1.5602506399154663,
      "learning_rate": 6.296296296296296e-05,
      "loss": 0.0635,
      "step": 264
    },
    {
      "epoch": 4.141732283464567,
      "grad_norm": 2.6416220664978027,
      "learning_rate": 6.243386243386243e-05,
      "loss": 0.1449,
      "step": 265
    },
    {
      "epoch": 4.15748031496063,
      "grad_norm": 1.9306683540344238,
      "learning_rate": 6.19047619047619e-05,
      "loss": 0.0689,
      "step": 266
    },
    {
      "epoch": 4.173228346456693,
      "grad_norm": 1.8357492685317993,
      "learning_rate": 6.137566137566138e-05,
      "loss": 0.1028,
      "step": 267
    },
    {
      "epoch": 4.188976377952756,
      "grad_norm": 2.442382574081421,
      "learning_rate": 6.084656084656085e-05,
      "loss": 0.089,
      "step": 268
    },
    {
      "epoch": 4.2047244094488185,
      "grad_norm": 3.147902488708496,
      "learning_rate": 6.0317460317460316e-05,
      "loss": 0.1446,
      "step": 269
    },
    {
      "epoch": 4.2204724409448815,
      "grad_norm": 4.012113094329834,
      "learning_rate": 5.9788359788359794e-05,
      "loss": 0.1726,
      "step": 270
    },
    {
      "epoch": 4.2362204724409445,
      "grad_norm": 2.9812517166137695,
      "learning_rate": 5.925925925925926e-05,
      "loss": 0.1169,
      "step": 271
    },
    {
      "epoch": 4.251968503937007,
      "grad_norm": 2.360576629638672,
      "learning_rate": 5.873015873015873e-05,
      "loss": 0.1255,
      "step": 272
    },
    {
      "epoch": 4.267716535433071,
      "grad_norm": 3.0435686111450195,
      "learning_rate": 5.82010582010582e-05,
      "loss": 0.0655,
      "step": 273
    },
    {
      "epoch": 4.283464566929134,
      "grad_norm": 2.8004462718963623,
      "learning_rate": 5.7671957671957676e-05,
      "loss": 0.0816,
      "step": 274
    },
    {
      "epoch": 4.299212598425197,
      "grad_norm": 2.6362526416778564,
      "learning_rate": 5.714285714285714e-05,
      "loss": 0.1272,
      "step": 275
    },
    {
      "epoch": 4.31496062992126,
      "grad_norm": 3.1129889488220215,
      "learning_rate": 5.661375661375662e-05,
      "loss": 0.0651,
      "step": 276
    },
    {
      "epoch": 4.330708661417323,
      "grad_norm": 2.9428136348724365,
      "learning_rate": 5.6084656084656086e-05,
      "loss": 0.1774,
      "step": 277
    },
    {
      "epoch": 4.346456692913386,
      "grad_norm": 2.4719181060791016,
      "learning_rate": 5.555555555555556e-05,
      "loss": 0.0978,
      "step": 278
    },
    {
      "epoch": 4.362204724409449,
      "grad_norm": 2.600954294204712,
      "learning_rate": 5.5026455026455024e-05,
      "loss": 0.0966,
      "step": 279
    },
    {
      "epoch": 4.377952755905512,
      "grad_norm": 3.7654852867126465,
      "learning_rate": 5.44973544973545e-05,
      "loss": 0.149,
      "step": 280
    },
    {
      "epoch": 4.393700787401575,
      "grad_norm": 3.266610860824585,
      "learning_rate": 5.396825396825397e-05,
      "loss": 0.1777,
      "step": 281
    },
    {
      "epoch": 4.409448818897638,
      "grad_norm": 1.8286641836166382,
      "learning_rate": 5.343915343915345e-05,
      "loss": 0.0672,
      "step": 282
    },
    {
      "epoch": 4.425196850393701,
      "grad_norm": 2.853868246078491,
      "learning_rate": 5.291005291005291e-05,
      "loss": 0.132,
      "step": 283
    },
    {
      "epoch": 4.440944881889764,
      "grad_norm": 3.4949874877929688,
      "learning_rate": 5.2380952380952384e-05,
      "loss": 0.1326,
      "step": 284
    },
    {
      "epoch": 4.456692913385827,
      "grad_norm": 3.0846071243286133,
      "learning_rate": 5.185185185185185e-05,
      "loss": 0.0786,
      "step": 285
    },
    {
      "epoch": 4.47244094488189,
      "grad_norm": 3.052459716796875,
      "learning_rate": 5.132275132275133e-05,
      "loss": 0.1475,
      "step": 286
    },
    {
      "epoch": 4.488188976377953,
      "grad_norm": 1.9307506084442139,
      "learning_rate": 5.0793650793650794e-05,
      "loss": 0.0672,
      "step": 287
    },
    {
      "epoch": 4.503937007874016,
      "grad_norm": 2.995371103286743,
      "learning_rate": 5.026455026455027e-05,
      "loss": 0.1157,
      "step": 288
    },
    {
      "epoch": 4.519685039370079,
      "grad_norm": 1.7526848316192627,
      "learning_rate": 4.973544973544973e-05,
      "loss": 0.0717,
      "step": 289
    },
    {
      "epoch": 4.535433070866142,
      "grad_norm": 1.5248143672943115,
      "learning_rate": 4.9206349206349204e-05,
      "loss": 0.0642,
      "step": 290
    },
    {
      "epoch": 4.551181102362205,
      "grad_norm": 2.7267649173736572,
      "learning_rate": 4.8677248677248676e-05,
      "loss": 0.0946,
      "step": 291
    },
    {
      "epoch": 4.566929133858268,
      "grad_norm": 2.7060441970825195,
      "learning_rate": 4.814814814814815e-05,
      "loss": 0.1362,
      "step": 292
    },
    {
      "epoch": 4.582677165354331,
      "grad_norm": 2.7383832931518555,
      "learning_rate": 4.761904761904762e-05,
      "loss": 0.1097,
      "step": 293
    },
    {
      "epoch": 4.5984251968503935,
      "grad_norm": 2.336318016052246,
      "learning_rate": 4.708994708994709e-05,
      "loss": 0.0914,
      "step": 294
    },
    {
      "epoch": 4.6141732283464565,
      "grad_norm": 2.3426754474639893,
      "learning_rate": 4.656084656084656e-05,
      "loss": 0.0892,
      "step": 295
    },
    {
      "epoch": 4.6299212598425195,
      "grad_norm": 3.021507501602173,
      "learning_rate": 4.603174603174603e-05,
      "loss": 0.1152,
      "step": 296
    },
    {
      "epoch": 4.645669291338582,
      "grad_norm": 3.2895145416259766,
      "learning_rate": 4.55026455026455e-05,
      "loss": 0.1537,
      "step": 297
    },
    {
      "epoch": 4.661417322834645,
      "grad_norm": 3.184382438659668,
      "learning_rate": 4.4973544973544974e-05,
      "loss": 0.183,
      "step": 298
    },
    {
      "epoch": 4.677165354330708,
      "grad_norm": 3.0715630054473877,
      "learning_rate": 4.4444444444444447e-05,
      "loss": 0.1316,
      "step": 299
    },
    {
      "epoch": 4.692913385826771,
      "grad_norm": 3.0267982482910156,
      "learning_rate": 4.391534391534391e-05,
      "loss": 0.1455,
      "step": 300
    },
    {
      "epoch": 4.708661417322834,
      "grad_norm": 2.356571674346924,
      "learning_rate": 4.3386243386243384e-05,
      "loss": 0.1043,
      "step": 301
    },
    {
      "epoch": 4.724409448818898,
      "grad_norm": 1.9336564540863037,
      "learning_rate": 4.2857142857142856e-05,
      "loss": 0.074,
      "step": 302
    },
    {
      "epoch": 4.740157480314961,
      "grad_norm": 2.724562406539917,
      "learning_rate": 4.232804232804233e-05,
      "loss": 0.1001,
      "step": 303
    },
    {
      "epoch": 4.755905511811024,
      "grad_norm": 4.442675590515137,
      "learning_rate": 4.17989417989418e-05,
      "loss": 0.2177,
      "step": 304
    },
    {
      "epoch": 4.771653543307087,
      "grad_norm": 3.026381015777588,
      "learning_rate": 4.126984126984127e-05,
      "loss": 0.1498,
      "step": 305
    },
    {
      "epoch": 4.78740157480315,
      "grad_norm": 3.285536766052246,
      "learning_rate": 4.074074074074074e-05,
      "loss": 0.169,
      "step": 306
    },
    {
      "epoch": 4.803149606299213,
      "grad_norm": 2.5938820838928223,
      "learning_rate": 4.021164021164021e-05,
      "loss": 0.1259,
      "step": 307
    },
    {
      "epoch": 4.818897637795276,
      "grad_norm": 3.465143918991089,
      "learning_rate": 3.968253968253968e-05,
      "loss": 0.1326,
      "step": 308
    },
    {
      "epoch": 4.834645669291339,
      "grad_norm": 2.12992525100708,
      "learning_rate": 3.9153439153439155e-05,
      "loss": 0.0898,
      "step": 309
    },
    {
      "epoch": 4.850393700787402,
      "grad_norm": 1.6682718992233276,
      "learning_rate": 3.862433862433863e-05,
      "loss": 0.0558,
      "step": 310
    },
    {
      "epoch": 4.866141732283465,
      "grad_norm": 2.515169382095337,
      "learning_rate": 3.809523809523809e-05,
      "loss": 0.1515,
      "step": 311
    },
    {
      "epoch": 4.881889763779528,
      "grad_norm": 3.9461846351623535,
      "learning_rate": 3.7566137566137564e-05,
      "loss": 0.1336,
      "step": 312
    },
    {
      "epoch": 4.897637795275591,
      "grad_norm": 3.440113067626953,
      "learning_rate": 3.7037037037037037e-05,
      "loss": 0.1898,
      "step": 313
    },
    {
      "epoch": 4.913385826771654,
      "grad_norm": 1.9951832294464111,
      "learning_rate": 3.650793650793651e-05,
      "loss": 0.0526,
      "step": 314
    },
    {
      "epoch": 4.929133858267717,
      "grad_norm": 2.644343852996826,
      "learning_rate": 3.597883597883598e-05,
      "loss": 0.1222,
      "step": 315
    },
    {
      "epoch": 4.94488188976378,
      "grad_norm": 2.637573480606079,
      "learning_rate": 3.5449735449735446e-05,
      "loss": 0.1564,
      "step": 316
    },
    {
      "epoch": 4.960629921259843,
      "grad_norm": 2.8677544593811035,
      "learning_rate": 3.492063492063492e-05,
      "loss": 0.1102,
      "step": 317
    },
    {
      "epoch": 4.9763779527559056,
      "grad_norm": 2.829080581665039,
      "learning_rate": 3.439153439153439e-05,
      "loss": 0.1094,
      "step": 318
    },
    {
      "epoch": 4.9921259842519685,
      "grad_norm": 3.5179617404937744,
      "learning_rate": 3.386243386243386e-05,
      "loss": 0.1694,
      "step": 319
    },
    {
      "epoch": 5.0,
      "grad_norm": 1.6180481910705566,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 0.038,
      "step": 320
    },
    {
      "epoch": 5.0,
      "eval_loss": 0.22777973115444183,
      "eval_runtime": 26.3957,
      "eval_samples_per_second": 2.425,
      "eval_steps_per_second": 0.303,
      "step": 320
    },
    {
      "epoch": 5.015748031496063,
      "grad_norm": 1.9994763135910034,
      "learning_rate": 3.280423280423281e-05,
      "loss": 0.0662,
      "step": 321
    },
    {
      "epoch": 5.031496062992126,
      "grad_norm": 2.726971387863159,
      "learning_rate": 3.227513227513227e-05,
      "loss": 0.099,
      "step": 322
    },
    {
      "epoch": 5.047244094488189,
      "grad_norm": 2.6560659408569336,
      "learning_rate": 3.1746031746031745e-05,
      "loss": 0.1297,
      "step": 323
    },
    {
      "epoch": 5.062992125984252,
      "grad_norm": 2.6483688354492188,
      "learning_rate": 3.121693121693122e-05,
      "loss": 0.1371,
      "step": 324
    },
    {
      "epoch": 5.078740157480315,
      "grad_norm": 1.8274121284484863,
      "learning_rate": 3.068783068783069e-05,
      "loss": 0.0745,
      "step": 325
    },
    {
      "epoch": 5.094488188976378,
      "grad_norm": 3.4843523502349854,
      "learning_rate": 3.0158730158730158e-05,
      "loss": 0.1189,
      "step": 326
    },
    {
      "epoch": 5.110236220472441,
      "grad_norm": 1.9978708028793335,
      "learning_rate": 2.962962962962963e-05,
      "loss": 0.1061,
      "step": 327
    },
    {
      "epoch": 5.125984251968504,
      "grad_norm": 2.1945767402648926,
      "learning_rate": 2.91005291005291e-05,
      "loss": 0.0653,
      "step": 328
    },
    {
      "epoch": 5.141732283464567,
      "grad_norm": 2.294682264328003,
      "learning_rate": 2.857142857142857e-05,
      "loss": 0.0864,
      "step": 329
    },
    {
      "epoch": 5.15748031496063,
      "grad_norm": 2.270611047744751,
      "learning_rate": 2.8042328042328043e-05,
      "loss": 0.0833,
      "step": 330
    },
    {
      "epoch": 5.173228346456693,
      "grad_norm": 3.1255877017974854,
      "learning_rate": 2.7513227513227512e-05,
      "loss": 0.1229,
      "step": 331
    },
    {
      "epoch": 5.188976377952756,
      "grad_norm": 2.91329026222229,
      "learning_rate": 2.6984126984126984e-05,
      "loss": 0.112,
      "step": 332
    },
    {
      "epoch": 5.2047244094488185,
      "grad_norm": 3.3271281719207764,
      "learning_rate": 2.6455026455026456e-05,
      "loss": 0.1176,
      "step": 333
    },
    {
      "epoch": 5.2204724409448815,
      "grad_norm": 1.9838052988052368,
      "learning_rate": 2.5925925925925925e-05,
      "loss": 0.0753,
      "step": 334
    },
    {
      "epoch": 5.2362204724409445,
      "grad_norm": 3.5805039405822754,
      "learning_rate": 2.5396825396825397e-05,
      "loss": 0.1122,
      "step": 335
    },
    {
      "epoch": 5.251968503937007,
      "grad_norm": 2.0203680992126465,
      "learning_rate": 2.4867724867724866e-05,
      "loss": 0.0679,
      "step": 336
    },
    {
      "epoch": 5.267716535433071,
      "grad_norm": 2.154576301574707,
      "learning_rate": 2.4338624338624338e-05,
      "loss": 0.0923,
      "step": 337
    },
    {
      "epoch": 5.283464566929134,
      "grad_norm": 3.23982572555542,
      "learning_rate": 2.380952380952381e-05,
      "loss": 0.1172,
      "step": 338
    },
    {
      "epoch": 5.299212598425197,
      "grad_norm": 2.3677990436553955,
      "learning_rate": 2.328042328042328e-05,
      "loss": 0.1075,
      "step": 339
    },
    {
      "epoch": 5.31496062992126,
      "grad_norm": 2.015462636947632,
      "learning_rate": 2.275132275132275e-05,
      "loss": 0.0935,
      "step": 340
    },
    {
      "epoch": 5.330708661417323,
      "grad_norm": 2.735832691192627,
      "learning_rate": 2.2222222222222223e-05,
      "loss": 0.098,
      "step": 341
    },
    {
      "epoch": 5.346456692913386,
      "grad_norm": 2.104445695877075,
      "learning_rate": 2.1693121693121692e-05,
      "loss": 0.0758,
      "step": 342
    },
    {
      "epoch": 5.362204724409449,
      "grad_norm": 2.748269557952881,
      "learning_rate": 2.1164021164021164e-05,
      "loss": 0.0973,
      "step": 343
    },
    {
      "epoch": 5.377952755905512,
      "grad_norm": 2.6600911617279053,
      "learning_rate": 2.0634920634920636e-05,
      "loss": 0.0806,
      "step": 344
    },
    {
      "epoch": 5.393700787401575,
      "grad_norm": 2.926095485687256,
      "learning_rate": 2.0105820105820105e-05,
      "loss": 0.0749,
      "step": 345
    },
    {
      "epoch": 5.409448818897638,
      "grad_norm": 2.8450281620025635,
      "learning_rate": 1.9576719576719577e-05,
      "loss": 0.0783,
      "step": 346
    },
    {
      "epoch": 5.425196850393701,
      "grad_norm": 3.1997084617614746,
      "learning_rate": 1.9047619047619046e-05,
      "loss": 0.0892,
      "step": 347
    },
    {
      "epoch": 5.440944881889764,
      "grad_norm": 2.175342559814453,
      "learning_rate": 1.8518518518518518e-05,
      "loss": 0.0937,
      "step": 348
    },
    {
      "epoch": 5.456692913385827,
      "grad_norm": 3.2938361167907715,
      "learning_rate": 1.798941798941799e-05,
      "loss": 0.1033,
      "step": 349
    },
    {
      "epoch": 5.47244094488189,
      "grad_norm": 2.4631547927856445,
      "learning_rate": 1.746031746031746e-05,
      "loss": 0.1075,
      "step": 350
    },
    {
      "epoch": 5.488188976377953,
      "grad_norm": 2.1883955001831055,
      "learning_rate": 1.693121693121693e-05,
      "loss": 0.0585,
      "step": 351
    },
    {
      "epoch": 5.503937007874016,
      "grad_norm": 3.397543430328369,
      "learning_rate": 1.6402116402116404e-05,
      "loss": 0.1652,
      "step": 352
    },
    {
      "epoch": 5.519685039370079,
      "grad_norm": 2.188857078552246,
      "learning_rate": 1.5873015873015872e-05,
      "loss": 0.074,
      "step": 353
    },
    {
      "epoch": 5.535433070866142,
      "grad_norm": 2.339099884033203,
      "learning_rate": 1.5343915343915344e-05,
      "loss": 0.084,
      "step": 354
    },
    {
      "epoch": 5.551181102362205,
      "grad_norm": 2.2820396423339844,
      "learning_rate": 1.4814814814814815e-05,
      "loss": 0.0727,
      "step": 355
    },
    {
      "epoch": 5.566929133858268,
      "grad_norm": 2.5689210891723633,
      "learning_rate": 1.4285714285714285e-05,
      "loss": 0.102,
      "step": 356
    },
    {
      "epoch": 5.582677165354331,
      "grad_norm": 2.846947431564331,
      "learning_rate": 1.3756613756613756e-05,
      "loss": 0.1081,
      "step": 357
    },
    {
      "epoch": 5.5984251968503935,
      "grad_norm": 2.5473179817199707,
      "learning_rate": 1.3227513227513228e-05,
      "loss": 0.0852,
      "step": 358
    },
    {
      "epoch": 5.6141732283464565,
      "grad_norm": 2.7979416847229004,
      "learning_rate": 1.2698412698412699e-05,
      "loss": 0.0914,
      "step": 359
    },
    {
      "epoch": 5.6299212598425195,
      "grad_norm": 2.5420584678649902,
      "learning_rate": 1.2169312169312169e-05,
      "loss": 0.1208,
      "step": 360
    },
    {
      "epoch": 5.645669291338582,
      "grad_norm": 4.605323314666748,
      "learning_rate": 1.164021164021164e-05,
      "loss": 0.1219,
      "step": 361
    },
    {
      "epoch": 5.661417322834645,
      "grad_norm": 2.3841617107391357,
      "learning_rate": 1.1111111111111112e-05,
      "loss": 0.0863,
      "step": 362
    },
    {
      "epoch": 5.677165354330708,
      "grad_norm": 2.21199631690979,
      "learning_rate": 1.0582010582010582e-05,
      "loss": 0.0719,
      "step": 363
    },
    {
      "epoch": 5.692913385826771,
      "grad_norm": 3.274789571762085,
      "learning_rate": 1.0052910052910053e-05,
      "loss": 0.1555,
      "step": 364
    },
    {
      "epoch": 5.708661417322834,
      "grad_norm": 1.8938199281692505,
      "learning_rate": 9.523809523809523e-06,
      "loss": 0.0541,
      "step": 365
    },
    {
      "epoch": 5.724409448818898,
      "grad_norm": 3.435129404067993,
      "learning_rate": 8.994708994708995e-06,
      "loss": 0.1296,
      "step": 366
    },
    {
      "epoch": 5.740157480314961,
      "grad_norm": 2.697870969772339,
      "learning_rate": 8.465608465608466e-06,
      "loss": 0.0812,
      "step": 367
    },
    {
      "epoch": 5.755905511811024,
      "grad_norm": 2.808487892150879,
      "learning_rate": 7.936507936507936e-06,
      "loss": 0.0874,
      "step": 368
    },
    {
      "epoch": 5.771653543307087,
      "grad_norm": 1.5478595495224,
      "learning_rate": 7.4074074074074075e-06,
      "loss": 0.0514,
      "step": 369
    },
    {
      "epoch": 5.78740157480315,
      "grad_norm": 2.73984432220459,
      "learning_rate": 6.878306878306878e-06,
      "loss": 0.1061,
      "step": 370
    },
    {
      "epoch": 5.803149606299213,
      "grad_norm": 2.1385202407836914,
      "learning_rate": 6.349206349206349e-06,
      "loss": 0.085,
      "step": 371
    },
    {
      "epoch": 5.818897637795276,
      "grad_norm": 2.9035861492156982,
      "learning_rate": 5.82010582010582e-06,
      "loss": 0.0737,
      "step": 372
    },
    {
      "epoch": 5.834645669291339,
      "grad_norm": 1.557504653930664,
      "learning_rate": 5.291005291005291e-06,
      "loss": 0.0361,
      "step": 373
    },
    {
      "epoch": 5.850393700787402,
      "grad_norm": 2.2422542572021484,
      "learning_rate": 4.7619047619047615e-06,
      "loss": 0.0725,
      "step": 374
    },
    {
      "epoch": 5.866141732283465,
      "grad_norm": 2.729954957962036,
      "learning_rate": 4.232804232804233e-06,
      "loss": 0.1442,
      "step": 375
    },
    {
      "epoch": 5.881889763779528,
      "grad_norm": 5.773847579956055,
      "learning_rate": 3.7037037037037037e-06,
      "loss": 0.1224,
      "step": 376
    },
    {
      "epoch": 5.897637795275591,
      "grad_norm": 2.9237236976623535,
      "learning_rate": 3.1746031746031746e-06,
      "loss": 0.0894,
      "step": 377
    },
    {
      "epoch": 5.913385826771654,
      "grad_norm": 3.0571537017822266,
      "learning_rate": 2.6455026455026455e-06,
      "loss": 0.1026,
      "step": 378
    },
    {
      "epoch": 5.913385826771654,
      "eval_loss": 0.2329818308353424,
      "eval_runtime": 27.7648,
      "eval_samples_per_second": 2.305,
      "eval_steps_per_second": 0.288,
      "step": 378
    }
  ],
  "logging_steps": 1,
  "max_steps": 378,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 6,
  "save_steps": 500,
  "stateful_callbacks": {
    "EarlyStoppingCallback": {
      "args": {
        "early_stopping_patience": 5,
        "early_stopping_threshold": 0.0
      },
      "attributes": {
        "early_stopping_patience_counter": 1
      }
    },
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.6412667015069696e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
