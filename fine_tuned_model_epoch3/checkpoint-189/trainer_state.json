{
  "best_metric": 0.3145443797111511,
  "best_model_checkpoint": "./fine_tuned_model_epoch3\\checkpoint-189",
  "epoch": 2.9606299212598426,
  "eval_steps": 500,
  "global_step": 189,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.015748031496062992,
      "grad_norm": NaN,
      "learning_rate": 0.0002,
      "loss": 15.3215,
      "step": 1
    },
    {
      "epoch": 0.031496062992125984,
      "grad_norm": NaN,
      "learning_rate": 0.0002,
      "loss": 14.7205,
      "step": 2
    },
    {
      "epoch": 0.047244094488188976,
      "grad_norm": NaN,
      "learning_rate": 0.0002,
      "loss": 15.6231,
      "step": 3
    },
    {
      "epoch": 0.06299212598425197,
      "grad_norm": NaN,
      "learning_rate": 0.0002,
      "loss": 14.7818,
      "step": 4
    },
    {
      "epoch": 0.07874015748031496,
      "grad_norm": NaN,
      "learning_rate": 0.0002,
      "loss": 15.0946,
      "step": 5
    },
    {
      "epoch": 0.09448818897637795,
      "grad_norm": 521.830810546875,
      "learning_rate": 0.00019894179894179893,
      "loss": 14.6601,
      "step": 6
    },
    {
      "epoch": 0.11023622047244094,
      "grad_norm": 295.88714599609375,
      "learning_rate": 0.0001978835978835979,
      "loss": 9.4869,
      "step": 7
    },
    {
      "epoch": 0.12598425196850394,
      "grad_norm": 196.4195098876953,
      "learning_rate": 0.00019682539682539682,
      "loss": 4.7576,
      "step": 8
    },
    {
      "epoch": 0.14173228346456693,
      "grad_norm": 81.63465881347656,
      "learning_rate": 0.0001957671957671958,
      "loss": 1.8001,
      "step": 9
    },
    {
      "epoch": 0.15748031496062992,
      "grad_norm": 14.854987144470215,
      "learning_rate": 0.0001947089947089947,
      "loss": 1.2071,
      "step": 10
    },
    {
      "epoch": 0.1732283464566929,
      "grad_norm": 36.6353759765625,
      "learning_rate": 0.00019365079365079365,
      "loss": 1.188,
      "step": 11
    },
    {
      "epoch": 0.1889763779527559,
      "grad_norm": 4.6004462242126465,
      "learning_rate": 0.0001925925925925926,
      "loss": 1.175,
      "step": 12
    },
    {
      "epoch": 0.2047244094488189,
      "grad_norm": 4.323287010192871,
      "learning_rate": 0.00019153439153439154,
      "loss": 1.0805,
      "step": 13
    },
    {
      "epoch": 0.2204724409448819,
      "grad_norm": 6.317239761352539,
      "learning_rate": 0.00019047619047619048,
      "loss": 1.1163,
      "step": 14
    },
    {
      "epoch": 0.23622047244094488,
      "grad_norm": 7.027277946472168,
      "learning_rate": 0.00018941798941798943,
      "loss": 0.8898,
      "step": 15
    },
    {
      "epoch": 0.25196850393700787,
      "grad_norm": 3.291553258895874,
      "learning_rate": 0.00018835978835978837,
      "loss": 0.9907,
      "step": 16
    },
    {
      "epoch": 0.2677165354330709,
      "grad_norm": 3.1255714893341064,
      "learning_rate": 0.00018730158730158731,
      "loss": 0.9609,
      "step": 17
    },
    {
      "epoch": 0.28346456692913385,
      "grad_norm": 2.977768659591675,
      "learning_rate": 0.00018624338624338623,
      "loss": 0.9752,
      "step": 18
    },
    {
      "epoch": 0.2992125984251969,
      "grad_norm": 7.5995683670043945,
      "learning_rate": 0.0001851851851851852,
      "loss": 0.742,
      "step": 19
    },
    {
      "epoch": 0.31496062992125984,
      "grad_norm": 3.4409103393554688,
      "learning_rate": 0.00018412698412698412,
      "loss": 0.988,
      "step": 20
    },
    {
      "epoch": 0.33070866141732286,
      "grad_norm": 3.016455888748169,
      "learning_rate": 0.0001830687830687831,
      "loss": 0.7984,
      "step": 21
    },
    {
      "epoch": 0.3464566929133858,
      "grad_norm": 2.438716173171997,
      "learning_rate": 0.000182010582010582,
      "loss": 0.6347,
      "step": 22
    },
    {
      "epoch": 0.36220472440944884,
      "grad_norm": 23.7377872467041,
      "learning_rate": 0.00018095238095238095,
      "loss": 0.7603,
      "step": 23
    },
    {
      "epoch": 0.3779527559055118,
      "grad_norm": 8.658821105957031,
      "learning_rate": 0.0001798941798941799,
      "loss": 0.8231,
      "step": 24
    },
    {
      "epoch": 0.3937007874015748,
      "grad_norm": 3.0620429515838623,
      "learning_rate": 0.00017883597883597884,
      "loss": 0.6781,
      "step": 25
    },
    {
      "epoch": 0.4094488188976378,
      "grad_norm": 2.788254976272583,
      "learning_rate": 0.00017777777777777779,
      "loss": 0.7704,
      "step": 26
    },
    {
      "epoch": 0.4251968503937008,
      "grad_norm": 2.772552728652954,
      "learning_rate": 0.00017671957671957673,
      "loss": 0.7114,
      "step": 27
    },
    {
      "epoch": 0.4409448818897638,
      "grad_norm": 2.3100695610046387,
      "learning_rate": 0.00017566137566137565,
      "loss": 0.5101,
      "step": 28
    },
    {
      "epoch": 0.4566929133858268,
      "grad_norm": 2.881009101867676,
      "learning_rate": 0.00017460317460317462,
      "loss": 0.589,
      "step": 29
    },
    {
      "epoch": 0.47244094488188976,
      "grad_norm": 1.8559956550598145,
      "learning_rate": 0.00017354497354497354,
      "loss": 0.5629,
      "step": 30
    },
    {
      "epoch": 0.4881889763779528,
      "grad_norm": 2.0417184829711914,
      "learning_rate": 0.0001724867724867725,
      "loss": 0.7277,
      "step": 31
    },
    {
      "epoch": 0.5039370078740157,
      "grad_norm": 1.5585144758224487,
      "learning_rate": 0.00017142857142857143,
      "loss": 0.6377,
      "step": 32
    },
    {
      "epoch": 0.5196850393700787,
      "grad_norm": 1.299907922744751,
      "learning_rate": 0.00017037037037037037,
      "loss": 0.5039,
      "step": 33
    },
    {
      "epoch": 0.5354330708661418,
      "grad_norm": 1.3178186416625977,
      "learning_rate": 0.00016931216931216931,
      "loss": 0.4877,
      "step": 34
    },
    {
      "epoch": 0.5511811023622047,
      "grad_norm": 1.8022898435592651,
      "learning_rate": 0.00016825396825396826,
      "loss": 0.6532,
      "step": 35
    },
    {
      "epoch": 0.5669291338582677,
      "grad_norm": 1.5257614850997925,
      "learning_rate": 0.0001671957671957672,
      "loss": 0.5973,
      "step": 36
    },
    {
      "epoch": 0.5826771653543307,
      "grad_norm": 1.252596378326416,
      "learning_rate": 0.00016613756613756615,
      "loss": 0.4768,
      "step": 37
    },
    {
      "epoch": 0.5984251968503937,
      "grad_norm": 1.327765703201294,
      "learning_rate": 0.0001650793650793651,
      "loss": 0.5118,
      "step": 38
    },
    {
      "epoch": 0.6141732283464567,
      "grad_norm": 1.1705036163330078,
      "learning_rate": 0.00016402116402116404,
      "loss": 0.5304,
      "step": 39
    },
    {
      "epoch": 0.6299212598425197,
      "grad_norm": 1.6234254837036133,
      "learning_rate": 0.00016296296296296295,
      "loss": 0.633,
      "step": 40
    },
    {
      "epoch": 0.6456692913385826,
      "grad_norm": 1.615294337272644,
      "learning_rate": 0.00016190476190476192,
      "loss": 0.4757,
      "step": 41
    },
    {
      "epoch": 0.6614173228346457,
      "grad_norm": 1.4649242162704468,
      "learning_rate": 0.00016084656084656084,
      "loss": 0.4716,
      "step": 42
    },
    {
      "epoch": 0.6771653543307087,
      "grad_norm": 1.0861765146255493,
      "learning_rate": 0.00015978835978835979,
      "loss": 0.311,
      "step": 43
    },
    {
      "epoch": 0.6929133858267716,
      "grad_norm": 1.7320266962051392,
      "learning_rate": 0.00015873015873015873,
      "loss": 0.5105,
      "step": 44
    },
    {
      "epoch": 0.7086614173228346,
      "grad_norm": 1.6445720195770264,
      "learning_rate": 0.00015767195767195767,
      "loss": 0.4755,
      "step": 45
    },
    {
      "epoch": 0.7244094488188977,
      "grad_norm": 1.4835795164108276,
      "learning_rate": 0.00015661375661375662,
      "loss": 0.3848,
      "step": 46
    },
    {
      "epoch": 0.7401574803149606,
      "grad_norm": 1.7306536436080933,
      "learning_rate": 0.00015555555555555556,
      "loss": 0.4467,
      "step": 47
    },
    {
      "epoch": 0.7559055118110236,
      "grad_norm": 1.1760823726654053,
      "learning_rate": 0.0001544973544973545,
      "loss": 0.3749,
      "step": 48
    },
    {
      "epoch": 0.7716535433070866,
      "grad_norm": 1.5630214214324951,
      "learning_rate": 0.00015343915343915345,
      "loss": 0.3895,
      "step": 49
    },
    {
      "epoch": 0.7874015748031497,
      "grad_norm": 1.6907292604446411,
      "learning_rate": 0.00015238095238095237,
      "loss": 0.5259,
      "step": 50
    },
    {
      "epoch": 0.8031496062992126,
      "grad_norm": 1.5841952562332153,
      "learning_rate": 0.00015132275132275134,
      "loss": 0.3727,
      "step": 51
    },
    {
      "epoch": 0.8188976377952756,
      "grad_norm": 1.4611706733703613,
      "learning_rate": 0.00015026455026455026,
      "loss": 0.39,
      "step": 52
    },
    {
      "epoch": 0.8346456692913385,
      "grad_norm": 1.765061378479004,
      "learning_rate": 0.00014920634920634923,
      "loss": 0.5218,
      "step": 53
    },
    {
      "epoch": 0.8503937007874016,
      "grad_norm": 1.5611538887023926,
      "learning_rate": 0.00014814814814814815,
      "loss": 0.4044,
      "step": 54
    },
    {
      "epoch": 0.8661417322834646,
      "grad_norm": 1.4266002178192139,
      "learning_rate": 0.0001470899470899471,
      "loss": 0.3307,
      "step": 55
    },
    {
      "epoch": 0.8818897637795275,
      "grad_norm": 1.6585214138031006,
      "learning_rate": 0.00014603174603174603,
      "loss": 0.3254,
      "step": 56
    },
    {
      "epoch": 0.8976377952755905,
      "grad_norm": 1.6138404607772827,
      "learning_rate": 0.00014497354497354498,
      "loss": 0.34,
      "step": 57
    },
    {
      "epoch": 0.9133858267716536,
      "grad_norm": 1.539273977279663,
      "learning_rate": 0.00014391534391534392,
      "loss": 0.3436,
      "step": 58
    },
    {
      "epoch": 0.9291338582677166,
      "grad_norm": 1.2497774362564087,
      "learning_rate": 0.00014285714285714287,
      "loss": 0.2531,
      "step": 59
    },
    {
      "epoch": 0.9448818897637795,
      "grad_norm": 2.6391592025756836,
      "learning_rate": 0.00014179894179894179,
      "loss": 0.4537,
      "step": 60
    },
    {
      "epoch": 0.9606299212598425,
      "grad_norm": 9.42066478729248,
      "learning_rate": 0.00014074074074074076,
      "loss": 0.3958,
      "step": 61
    },
    {
      "epoch": 0.9763779527559056,
      "grad_norm": 2.1686317920684814,
      "learning_rate": 0.00013968253968253967,
      "loss": 0.4579,
      "step": 62
    },
    {
      "epoch": 0.9921259842519685,
      "grad_norm": 11.829385757446289,
      "learning_rate": 0.00013862433862433865,
      "loss": 0.5311,
      "step": 63
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.9181143641471863,
      "learning_rate": 0.00013756613756613756,
      "loss": 0.1219,
      "step": 64
    },
    {
      "epoch": 1.0,
      "eval_loss": 0.38516026735305786,
      "eval_runtime": 21.4423,
      "eval_samples_per_second": 2.985,
      "eval_steps_per_second": 0.373,
      "step": 64
    },
    {
      "epoch": 1.015748031496063,
      "grad_norm": 3.8337910175323486,
      "learning_rate": 0.0001365079365079365,
      "loss": 0.4805,
      "step": 65
    },
    {
      "epoch": 1.031496062992126,
      "grad_norm": 1.681383490562439,
      "learning_rate": 0.00013544973544973545,
      "loss": 0.4113,
      "step": 66
    },
    {
      "epoch": 1.047244094488189,
      "grad_norm": 1.47898530960083,
      "learning_rate": 0.0001343915343915344,
      "loss": 0.3355,
      "step": 67
    },
    {
      "epoch": 1.0629921259842519,
      "grad_norm": 1.3520879745483398,
      "learning_rate": 0.00013333333333333334,
      "loss": 0.3641,
      "step": 68
    },
    {
      "epoch": 1.078740157480315,
      "grad_norm": 1.5481432676315308,
      "learning_rate": 0.00013227513227513228,
      "loss": 0.3363,
      "step": 69
    },
    {
      "epoch": 1.094488188976378,
      "grad_norm": 1.5076496601104736,
      "learning_rate": 0.00013121693121693123,
      "loss": 0.3246,
      "step": 70
    },
    {
      "epoch": 1.110236220472441,
      "grad_norm": 1.3278840780258179,
      "learning_rate": 0.00013015873015873017,
      "loss": 0.3015,
      "step": 71
    },
    {
      "epoch": 1.125984251968504,
      "grad_norm": 1.7984853982925415,
      "learning_rate": 0.0001291005291005291,
      "loss": 0.3724,
      "step": 72
    },
    {
      "epoch": 1.141732283464567,
      "grad_norm": 1.3459147214889526,
      "learning_rate": 0.00012804232804232806,
      "loss": 0.2833,
      "step": 73
    },
    {
      "epoch": 1.1574803149606299,
      "grad_norm": 1.3922438621520996,
      "learning_rate": 0.00012698412698412698,
      "loss": 0.3958,
      "step": 74
    },
    {
      "epoch": 1.1732283464566928,
      "grad_norm": 1.6595858335494995,
      "learning_rate": 0.00012592592592592592,
      "loss": 0.3339,
      "step": 75
    },
    {
      "epoch": 1.188976377952756,
      "grad_norm": 2.378765821456909,
      "learning_rate": 0.00012486772486772487,
      "loss": 0.4259,
      "step": 76
    },
    {
      "epoch": 1.204724409448819,
      "grad_norm": 2.4765655994415283,
      "learning_rate": 0.0001238095238095238,
      "loss": 0.4517,
      "step": 77
    },
    {
      "epoch": 1.220472440944882,
      "grad_norm": 1.6138514280319214,
      "learning_rate": 0.00012275132275132276,
      "loss": 0.3132,
      "step": 78
    },
    {
      "epoch": 1.236220472440945,
      "grad_norm": 1.7591887712478638,
      "learning_rate": 0.0001216931216931217,
      "loss": 0.3345,
      "step": 79
    },
    {
      "epoch": 1.2519685039370079,
      "grad_norm": 1.4427027702331543,
      "learning_rate": 0.00012063492063492063,
      "loss": 0.2509,
      "step": 80
    },
    {
      "epoch": 1.2677165354330708,
      "grad_norm": 1.9053133726119995,
      "learning_rate": 0.00011957671957671959,
      "loss": 0.3092,
      "step": 81
    },
    {
      "epoch": 1.2834645669291338,
      "grad_norm": 2.3294332027435303,
      "learning_rate": 0.00011851851851851852,
      "loss": 0.4256,
      "step": 82
    },
    {
      "epoch": 1.2992125984251968,
      "grad_norm": 2.4260263442993164,
      "learning_rate": 0.00011746031746031746,
      "loss": 0.4094,
      "step": 83
    },
    {
      "epoch": 1.3149606299212597,
      "grad_norm": 1.820695400238037,
      "learning_rate": 0.0001164021164021164,
      "loss": 0.4277,
      "step": 84
    },
    {
      "epoch": 1.330708661417323,
      "grad_norm": 1.9267514944076538,
      "learning_rate": 0.00011534391534391535,
      "loss": 0.3479,
      "step": 85
    },
    {
      "epoch": 1.3464566929133859,
      "grad_norm": 2.6787021160125732,
      "learning_rate": 0.00011428571428571428,
      "loss": 0.2855,
      "step": 86
    },
    {
      "epoch": 1.3622047244094488,
      "grad_norm": 1.8436130285263062,
      "learning_rate": 0.00011322751322751324,
      "loss": 0.388,
      "step": 87
    },
    {
      "epoch": 1.3779527559055118,
      "grad_norm": 1.996294379234314,
      "learning_rate": 0.00011216931216931217,
      "loss": 0.3881,
      "step": 88
    },
    {
      "epoch": 1.3937007874015748,
      "grad_norm": 1.625036358833313,
      "learning_rate": 0.00011111111111111112,
      "loss": 0.2719,
      "step": 89
    },
    {
      "epoch": 1.4094488188976377,
      "grad_norm": 2.1395723819732666,
      "learning_rate": 0.00011005291005291005,
      "loss": 0.3579,
      "step": 90
    },
    {
      "epoch": 1.425196850393701,
      "grad_norm": 1.5052261352539062,
      "learning_rate": 0.000108994708994709,
      "loss": 0.2147,
      "step": 91
    },
    {
      "epoch": 1.4409448818897639,
      "grad_norm": 1.7203080654144287,
      "learning_rate": 0.00010793650793650794,
      "loss": 0.2867,
      "step": 92
    },
    {
      "epoch": 1.4566929133858268,
      "grad_norm": 2.028801679611206,
      "learning_rate": 0.0001068783068783069,
      "loss": 0.3453,
      "step": 93
    },
    {
      "epoch": 1.4724409448818898,
      "grad_norm": 1.5206133127212524,
      "learning_rate": 0.00010582010582010582,
      "loss": 0.2483,
      "step": 94
    },
    {
      "epoch": 1.4881889763779528,
      "grad_norm": 1.6643975973129272,
      "learning_rate": 0.00010476190476190477,
      "loss": 0.2426,
      "step": 95
    },
    {
      "epoch": 1.5039370078740157,
      "grad_norm": 1.6946851015090942,
      "learning_rate": 0.0001037037037037037,
      "loss": 0.2681,
      "step": 96
    },
    {
      "epoch": 1.5196850393700787,
      "grad_norm": 1.7946363687515259,
      "learning_rate": 0.00010264550264550266,
      "loss": 0.2975,
      "step": 97
    },
    {
      "epoch": 1.5354330708661417,
      "grad_norm": 1.8522924184799194,
      "learning_rate": 0.00010158730158730159,
      "loss": 0.3029,
      "step": 98
    },
    {
      "epoch": 1.5511811023622046,
      "grad_norm": 2.582213878631592,
      "learning_rate": 0.00010052910052910055,
      "loss": 0.3497,
      "step": 99
    },
    {
      "epoch": 1.5669291338582676,
      "grad_norm": 1.964684247970581,
      "learning_rate": 9.947089947089946e-05,
      "loss": 0.4226,
      "step": 100
    },
    {
      "epoch": 1.5826771653543306,
      "grad_norm": 2.287158250808716,
      "learning_rate": 9.841269841269841e-05,
      "loss": 0.3646,
      "step": 101
    },
    {
      "epoch": 1.5984251968503937,
      "grad_norm": 1.9622077941894531,
      "learning_rate": 9.735449735449735e-05,
      "loss": 0.3254,
      "step": 102
    },
    {
      "epoch": 1.6141732283464567,
      "grad_norm": 2.2544617652893066,
      "learning_rate": 9.62962962962963e-05,
      "loss": 0.3586,
      "step": 103
    },
    {
      "epoch": 1.6299212598425197,
      "grad_norm": 2.8069920539855957,
      "learning_rate": 9.523809523809524e-05,
      "loss": 0.4333,
      "step": 104
    },
    {
      "epoch": 1.6456692913385826,
      "grad_norm": 2.01958966255188,
      "learning_rate": 9.417989417989419e-05,
      "loss": 0.323,
      "step": 105
    },
    {
      "epoch": 1.6614173228346458,
      "grad_norm": 1.9339945316314697,
      "learning_rate": 9.312169312169312e-05,
      "loss": 0.2568,
      "step": 106
    },
    {
      "epoch": 1.6771653543307088,
      "grad_norm": 2.0816972255706787,
      "learning_rate": 9.206349206349206e-05,
      "loss": 0.366,
      "step": 107
    },
    {
      "epoch": 1.6929133858267718,
      "grad_norm": 1.9964051246643066,
      "learning_rate": 9.1005291005291e-05,
      "loss": 0.3363,
      "step": 108
    },
    {
      "epoch": 1.7086614173228347,
      "grad_norm": 2.3179569244384766,
      "learning_rate": 8.994708994708995e-05,
      "loss": 0.372,
      "step": 109
    },
    {
      "epoch": 1.7244094488188977,
      "grad_norm": 1.6388657093048096,
      "learning_rate": 8.888888888888889e-05,
      "loss": 0.2406,
      "step": 110
    },
    {
      "epoch": 1.7401574803149606,
      "grad_norm": 1.7996677160263062,
      "learning_rate": 8.783068783068782e-05,
      "loss": 0.291,
      "step": 111
    },
    {
      "epoch": 1.7559055118110236,
      "grad_norm": 2.906852960586548,
      "learning_rate": 8.677248677248677e-05,
      "loss": 0.4328,
      "step": 112
    },
    {
      "epoch": 1.7716535433070866,
      "grad_norm": 2.0851962566375732,
      "learning_rate": 8.571428571428571e-05,
      "loss": 0.3461,
      "step": 113
    },
    {
      "epoch": 1.7874015748031495,
      "grad_norm": 2.0622050762176514,
      "learning_rate": 8.465608465608466e-05,
      "loss": 0.3396,
      "step": 114
    },
    {
      "epoch": 1.8031496062992125,
      "grad_norm": 1.9183976650238037,
      "learning_rate": 8.35978835978836e-05,
      "loss": 0.2331,
      "step": 115
    },
    {
      "epoch": 1.8188976377952755,
      "grad_norm": 2.82682204246521,
      "learning_rate": 8.253968253968255e-05,
      "loss": 0.3708,
      "step": 116
    },
    {
      "epoch": 1.8346456692913384,
      "grad_norm": 1.838142991065979,
      "learning_rate": 8.148148148148148e-05,
      "loss": 0.2852,
      "step": 117
    },
    {
      "epoch": 1.8503937007874016,
      "grad_norm": 2.6056277751922607,
      "learning_rate": 8.042328042328042e-05,
      "loss": 0.3548,
      "step": 118
    },
    {
      "epoch": 1.8661417322834646,
      "grad_norm": 2.262723684310913,
      "learning_rate": 7.936507936507937e-05,
      "loss": 0.3355,
      "step": 119
    },
    {
      "epoch": 1.8818897637795275,
      "grad_norm": 1.6915394067764282,
      "learning_rate": 7.830687830687831e-05,
      "loss": 0.2374,
      "step": 120
    },
    {
      "epoch": 1.8976377952755905,
      "grad_norm": 1.645240306854248,
      "learning_rate": 7.724867724867725e-05,
      "loss": 0.2349,
      "step": 121
    },
    {
      "epoch": 1.9133858267716537,
      "grad_norm": 1.9150551557540894,
      "learning_rate": 7.619047619047618e-05,
      "loss": 0.2645,
      "step": 122
    },
    {
      "epoch": 1.9291338582677167,
      "grad_norm": 2.6754958629608154,
      "learning_rate": 7.513227513227513e-05,
      "loss": 0.3263,
      "step": 123
    },
    {
      "epoch": 1.9448818897637796,
      "grad_norm": 1.9488743543624878,
      "learning_rate": 7.407407407407407e-05,
      "loss": 0.3181,
      "step": 124
    },
    {
      "epoch": 1.9606299212598426,
      "grad_norm": 2.1409010887145996,
      "learning_rate": 7.301587301587302e-05,
      "loss": 0.3061,
      "step": 125
    },
    {
      "epoch": 1.9763779527559056,
      "grad_norm": 2.1201581954956055,
      "learning_rate": 7.195767195767196e-05,
      "loss": 0.2906,
      "step": 126
    },
    {
      "epoch": 1.9921259842519685,
      "grad_norm": 1.802525281906128,
      "learning_rate": 7.089947089947089e-05,
      "loss": 0.248,
      "step": 127
    },
    {
      "epoch": 2.0,
      "grad_norm": 2.1721079349517822,
      "learning_rate": 6.984126984126984e-05,
      "loss": 0.232,
      "step": 128
    },
    {
      "epoch": 2.0,
      "eval_loss": 0.3264066278934479,
      "eval_runtime": 21.5933,
      "eval_samples_per_second": 2.964,
      "eval_steps_per_second": 0.37,
      "step": 128
    },
    {
      "epoch": 2.015748031496063,
      "grad_norm": 2.194037437438965,
      "learning_rate": 6.878306878306878e-05,
      "loss": 0.356,
      "step": 129
    },
    {
      "epoch": 2.031496062992126,
      "grad_norm": 1.4078272581100464,
      "learning_rate": 6.772486772486773e-05,
      "loss": 0.1665,
      "step": 130
    },
    {
      "epoch": 2.047244094488189,
      "grad_norm": 1.571259617805481,
      "learning_rate": 6.666666666666667e-05,
      "loss": 0.2351,
      "step": 131
    },
    {
      "epoch": 2.062992125984252,
      "grad_norm": 2.329864501953125,
      "learning_rate": 6.560846560846561e-05,
      "loss": 0.2961,
      "step": 132
    },
    {
      "epoch": 2.078740157480315,
      "grad_norm": 2.2667887210845947,
      "learning_rate": 6.455026455026454e-05,
      "loss": 0.3644,
      "step": 133
    },
    {
      "epoch": 2.094488188976378,
      "grad_norm": 2.394069194793701,
      "learning_rate": 6.349206349206349e-05,
      "loss": 0.3491,
      "step": 134
    },
    {
      "epoch": 2.1102362204724407,
      "grad_norm": 2.3943631649017334,
      "learning_rate": 6.243386243386243e-05,
      "loss": 0.3551,
      "step": 135
    },
    {
      "epoch": 2.1259842519685037,
      "grad_norm": 1.6901034116744995,
      "learning_rate": 6.137566137566138e-05,
      "loss": 0.2161,
      "step": 136
    },
    {
      "epoch": 2.141732283464567,
      "grad_norm": 2.0470616817474365,
      "learning_rate": 6.0317460317460316e-05,
      "loss": 0.2592,
      "step": 137
    },
    {
      "epoch": 2.15748031496063,
      "grad_norm": 1.7342020273208618,
      "learning_rate": 5.925925925925926e-05,
      "loss": 0.2169,
      "step": 138
    },
    {
      "epoch": 2.173228346456693,
      "grad_norm": 2.5044760704040527,
      "learning_rate": 5.82010582010582e-05,
      "loss": 0.281,
      "step": 139
    },
    {
      "epoch": 2.188976377952756,
      "grad_norm": 1.6008721590042114,
      "learning_rate": 5.714285714285714e-05,
      "loss": 0.212,
      "step": 140
    },
    {
      "epoch": 2.204724409448819,
      "grad_norm": 2.6787924766540527,
      "learning_rate": 5.6084656084656086e-05,
      "loss": 0.2959,
      "step": 141
    },
    {
      "epoch": 2.220472440944882,
      "grad_norm": 2.752734661102295,
      "learning_rate": 5.5026455026455024e-05,
      "loss": 0.3242,
      "step": 142
    },
    {
      "epoch": 2.236220472440945,
      "grad_norm": 1.8710492849349976,
      "learning_rate": 5.396825396825397e-05,
      "loss": 0.1962,
      "step": 143
    },
    {
      "epoch": 2.251968503937008,
      "grad_norm": 1.8881371021270752,
      "learning_rate": 5.291005291005291e-05,
      "loss": 0.1982,
      "step": 144
    },
    {
      "epoch": 2.267716535433071,
      "grad_norm": 2.817103385925293,
      "learning_rate": 5.185185185185185e-05,
      "loss": 0.3606,
      "step": 145
    },
    {
      "epoch": 2.283464566929134,
      "grad_norm": 2.1529667377471924,
      "learning_rate": 5.0793650793650794e-05,
      "loss": 0.2443,
      "step": 146
    },
    {
      "epoch": 2.2992125984251968,
      "grad_norm": 2.139225721359253,
      "learning_rate": 4.973544973544973e-05,
      "loss": 0.2687,
      "step": 147
    },
    {
      "epoch": 2.3149606299212597,
      "grad_norm": 2.524662494659424,
      "learning_rate": 4.8677248677248676e-05,
      "loss": 0.2671,
      "step": 148
    },
    {
      "epoch": 2.3307086614173227,
      "grad_norm": 2.529973030090332,
      "learning_rate": 4.761904761904762e-05,
      "loss": 0.2922,
      "step": 149
    },
    {
      "epoch": 2.3464566929133857,
      "grad_norm": 1.9670342206954956,
      "learning_rate": 4.656084656084656e-05,
      "loss": 0.2544,
      "step": 150
    },
    {
      "epoch": 2.362204724409449,
      "grad_norm": 2.106318712234497,
      "learning_rate": 4.55026455026455e-05,
      "loss": 0.2661,
      "step": 151
    },
    {
      "epoch": 2.377952755905512,
      "grad_norm": 2.075347661972046,
      "learning_rate": 4.4444444444444447e-05,
      "loss": 0.2339,
      "step": 152
    },
    {
      "epoch": 2.393700787401575,
      "grad_norm": 1.7906602621078491,
      "learning_rate": 4.3386243386243384e-05,
      "loss": 0.1814,
      "step": 153
    },
    {
      "epoch": 2.409448818897638,
      "grad_norm": 2.547316312789917,
      "learning_rate": 4.232804232804233e-05,
      "loss": 0.3188,
      "step": 154
    },
    {
      "epoch": 2.425196850393701,
      "grad_norm": 2.305150032043457,
      "learning_rate": 4.126984126984127e-05,
      "loss": 0.2484,
      "step": 155
    },
    {
      "epoch": 2.440944881889764,
      "grad_norm": 2.1817429065704346,
      "learning_rate": 4.021164021164021e-05,
      "loss": 0.2658,
      "step": 156
    },
    {
      "epoch": 2.456692913385827,
      "grad_norm": 2.0258102416992188,
      "learning_rate": 3.9153439153439155e-05,
      "loss": 0.1939,
      "step": 157
    },
    {
      "epoch": 2.47244094488189,
      "grad_norm": 2.1457173824310303,
      "learning_rate": 3.809523809523809e-05,
      "loss": 0.3209,
      "step": 158
    },
    {
      "epoch": 2.4881889763779528,
      "grad_norm": 3.269383668899536,
      "learning_rate": 3.7037037037037037e-05,
      "loss": 0.4227,
      "step": 159
    },
    {
      "epoch": 2.5039370078740157,
      "grad_norm": 2.187407970428467,
      "learning_rate": 3.597883597883598e-05,
      "loss": 0.2998,
      "step": 160
    },
    {
      "epoch": 2.5196850393700787,
      "grad_norm": 2.1418399810791016,
      "learning_rate": 3.492063492063492e-05,
      "loss": 0.2575,
      "step": 161
    },
    {
      "epoch": 2.5354330708661417,
      "grad_norm": 2.0921170711517334,
      "learning_rate": 3.386243386243386e-05,
      "loss": 0.1716,
      "step": 162
    },
    {
      "epoch": 2.5511811023622046,
      "grad_norm": 2.214930534362793,
      "learning_rate": 3.280423280423281e-05,
      "loss": 0.3104,
      "step": 163
    },
    {
      "epoch": 2.5669291338582676,
      "grad_norm": 1.8452808856964111,
      "learning_rate": 3.1746031746031745e-05,
      "loss": 0.1936,
      "step": 164
    },
    {
      "epoch": 2.5826771653543306,
      "grad_norm": 2.504385471343994,
      "learning_rate": 3.068783068783069e-05,
      "loss": 0.2739,
      "step": 165
    },
    {
      "epoch": 2.5984251968503935,
      "grad_norm": 2.5358471870422363,
      "learning_rate": 2.962962962962963e-05,
      "loss": 0.2345,
      "step": 166
    },
    {
      "epoch": 2.6141732283464565,
      "grad_norm": 2.3843979835510254,
      "learning_rate": 2.857142857142857e-05,
      "loss": 0.2704,
      "step": 167
    },
    {
      "epoch": 2.6299212598425195,
      "grad_norm": 2.4227468967437744,
      "learning_rate": 2.7513227513227512e-05,
      "loss": 0.2792,
      "step": 168
    },
    {
      "epoch": 2.6456692913385824,
      "grad_norm": 3.7230448722839355,
      "learning_rate": 2.6455026455026456e-05,
      "loss": 0.2594,
      "step": 169
    },
    {
      "epoch": 2.661417322834646,
      "grad_norm": 2.556044578552246,
      "learning_rate": 2.5396825396825397e-05,
      "loss": 0.3457,
      "step": 170
    },
    {
      "epoch": 2.677165354330709,
      "grad_norm": 2.7415707111358643,
      "learning_rate": 2.4338624338624338e-05,
      "loss": 0.3333,
      "step": 171
    },
    {
      "epoch": 2.6929133858267718,
      "grad_norm": 1.635648488998413,
      "learning_rate": 2.328042328042328e-05,
      "loss": 0.2118,
      "step": 172
    },
    {
      "epoch": 2.7086614173228347,
      "grad_norm": 2.48787784576416,
      "learning_rate": 2.2222222222222223e-05,
      "loss": 0.2629,
      "step": 173
    },
    {
      "epoch": 2.7244094488188977,
      "grad_norm": 2.500706434249878,
      "learning_rate": 2.1164021164021164e-05,
      "loss": 0.2527,
      "step": 174
    },
    {
      "epoch": 2.7401574803149606,
      "grad_norm": 2.2155301570892334,
      "learning_rate": 2.0105820105820105e-05,
      "loss": 0.2134,
      "step": 175
    },
    {
      "epoch": 2.7559055118110236,
      "grad_norm": 2.2884111404418945,
      "learning_rate": 1.9047619047619046e-05,
      "loss": 0.288,
      "step": 176
    },
    {
      "epoch": 2.7716535433070866,
      "grad_norm": 2.7226176261901855,
      "learning_rate": 1.798941798941799e-05,
      "loss": 0.3473,
      "step": 177
    },
    {
      "epoch": 2.7874015748031495,
      "grad_norm": 2.7840006351470947,
      "learning_rate": 1.693121693121693e-05,
      "loss": 0.3812,
      "step": 178
    },
    {
      "epoch": 2.8031496062992125,
      "grad_norm": 2.702519416809082,
      "learning_rate": 1.5873015873015872e-05,
      "loss": 0.2917,
      "step": 179
    },
    {
      "epoch": 2.8188976377952755,
      "grad_norm": 1.945273756980896,
      "learning_rate": 1.4814814814814815e-05,
      "loss": 0.196,
      "step": 180
    },
    {
      "epoch": 2.8346456692913384,
      "grad_norm": 2.054056167602539,
      "learning_rate": 1.3756613756613756e-05,
      "loss": 0.1748,
      "step": 181
    },
    {
      "epoch": 2.850393700787402,
      "grad_norm": 2.2657411098480225,
      "learning_rate": 1.2698412698412699e-05,
      "loss": 0.2222,
      "step": 182
    },
    {
      "epoch": 2.866141732283465,
      "grad_norm": 2.23899507522583,
      "learning_rate": 1.164021164021164e-05,
      "loss": 0.3041,
      "step": 183
    },
    {
      "epoch": 2.8818897637795278,
      "grad_norm": 2.8115084171295166,
      "learning_rate": 1.0582010582010582e-05,
      "loss": 0.3403,
      "step": 184
    },
    {
      "epoch": 2.8976377952755907,
      "grad_norm": 2.0621676445007324,
      "learning_rate": 9.523809523809523e-06,
      "loss": 0.1526,
      "step": 185
    },
    {
      "epoch": 2.9133858267716537,
      "grad_norm": 1.8872700929641724,
      "learning_rate": 8.465608465608466e-06,
      "loss": 0.1828,
      "step": 186
    },
    {
      "epoch": 2.9291338582677167,
      "grad_norm": 2.4312593936920166,
      "learning_rate": 7.4074074074074075e-06,
      "loss": 0.2515,
      "step": 187
    },
    {
      "epoch": 2.9448818897637796,
      "grad_norm": 2.198436975479126,
      "learning_rate": 6.349206349206349e-06,
      "loss": 0.2784,
      "step": 188
    },
    {
      "epoch": 2.9606299212598426,
      "grad_norm": 2.1525535583496094,
      "learning_rate": 5.291005291005291e-06,
      "loss": 0.2632,
      "step": 189
    },
    {
      "epoch": 2.9606299212598426,
      "eval_loss": 0.3145443797111511,
      "eval_runtime": 22.9439,
      "eval_samples_per_second": 2.789,
      "eval_steps_per_second": 0.349,
      "step": 189
    }
  ],
  "logging_steps": 1,
  "max_steps": 189,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "EarlyStoppingCallback": {
      "args": {
        "early_stopping_patience": 5,
        "early_stopping_threshold": 0.0
      },
      "attributes": {
        "early_stopping_patience_counter": 0
      }
    },
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 8217260715933696.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
