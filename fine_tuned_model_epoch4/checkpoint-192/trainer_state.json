{
  "best_metric": 0.29737889766693115,
  "best_model_checkpoint": "./fine_tuned_model_epoch4\\checkpoint-192",
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 192,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.015748031496062992,
      "grad_norm": NaN,
      "learning_rate": 0.0002,
      "loss": 15.4227,
      "step": 1
    },
    {
      "epoch": 0.031496062992125984,
      "grad_norm": NaN,
      "learning_rate": 0.0002,
      "loss": 14.6664,
      "step": 2
    },
    {
      "epoch": 0.047244094488188976,
      "grad_norm": NaN,
      "learning_rate": 0.0002,
      "loss": 15.6397,
      "step": 3
    },
    {
      "epoch": 0.06299212598425197,
      "grad_norm": NaN,
      "learning_rate": 0.0002,
      "loss": 14.8044,
      "step": 4
    },
    {
      "epoch": 0.07874015748031496,
      "grad_norm": NaN,
      "learning_rate": 0.0002,
      "loss": 14.9694,
      "step": 5
    },
    {
      "epoch": 0.09448818897637795,
      "grad_norm": 499.845458984375,
      "learning_rate": 0.00019920634920634922,
      "loss": 14.6601,
      "step": 6
    },
    {
      "epoch": 0.11023622047244094,
      "grad_norm": 266.7703857421875,
      "learning_rate": 0.00019841269841269844,
      "loss": 9.2386,
      "step": 7
    },
    {
      "epoch": 0.12598425196850394,
      "grad_norm": 178.03607177734375,
      "learning_rate": 0.00019761904761904763,
      "loss": 4.3017,
      "step": 8
    },
    {
      "epoch": 0.14173228346456693,
      "grad_norm": 59.45682907104492,
      "learning_rate": 0.00019682539682539682,
      "loss": 1.5669,
      "step": 9
    },
    {
      "epoch": 0.15748031496062992,
      "grad_norm": 13.059600830078125,
      "learning_rate": 0.00019603174603174603,
      "loss": 1.1906,
      "step": 10
    },
    {
      "epoch": 0.1732283464566929,
      "grad_norm": 95.64628601074219,
      "learning_rate": 0.00019523809523809525,
      "loss": 1.2304,
      "step": 11
    },
    {
      "epoch": 0.1889763779527559,
      "grad_norm": 4.774786472320557,
      "learning_rate": 0.00019444444444444446,
      "loss": 1.1475,
      "step": 12
    },
    {
      "epoch": 0.2047244094488189,
      "grad_norm": 3.627094268798828,
      "learning_rate": 0.00019365079365079365,
      "loss": 1.0512,
      "step": 13
    },
    {
      "epoch": 0.2204724409448819,
      "grad_norm": 5.880895137786865,
      "learning_rate": 0.00019285714285714286,
      "loss": 1.096,
      "step": 14
    },
    {
      "epoch": 0.23622047244094488,
      "grad_norm": 3.3823163509368896,
      "learning_rate": 0.00019206349206349208,
      "loss": 0.8857,
      "step": 15
    },
    {
      "epoch": 0.25196850393700787,
      "grad_norm": 3.5293431282043457,
      "learning_rate": 0.0001912698412698413,
      "loss": 0.9472,
      "step": 16
    },
    {
      "epoch": 0.2677165354330709,
      "grad_norm": 3.2343223094940186,
      "learning_rate": 0.00019047619047619048,
      "loss": 0.9377,
      "step": 17
    },
    {
      "epoch": 0.28346456692913385,
      "grad_norm": 2.8431527614593506,
      "learning_rate": 0.0001896825396825397,
      "loss": 0.9351,
      "step": 18
    },
    {
      "epoch": 0.2992125984251969,
      "grad_norm": 2.470867872238159,
      "learning_rate": 0.00018888888888888888,
      "loss": 0.7316,
      "step": 19
    },
    {
      "epoch": 0.31496062992125984,
      "grad_norm": 4.637535572052002,
      "learning_rate": 0.0001880952380952381,
      "loss": 0.9899,
      "step": 20
    },
    {
      "epoch": 0.33070866141732286,
      "grad_norm": 3.765122652053833,
      "learning_rate": 0.00018730158730158731,
      "loss": 0.7329,
      "step": 21
    },
    {
      "epoch": 0.3464566929133858,
      "grad_norm": 2.2409863471984863,
      "learning_rate": 0.00018650793650793653,
      "loss": 0.6065,
      "step": 22
    },
    {
      "epoch": 0.36220472440944884,
      "grad_norm": 64.65618896484375,
      "learning_rate": 0.00018571428571428572,
      "loss": 0.7996,
      "step": 23
    },
    {
      "epoch": 0.3779527559055118,
      "grad_norm": 23.770706176757812,
      "learning_rate": 0.00018492063492063493,
      "loss": 0.8842,
      "step": 24
    },
    {
      "epoch": 0.3937007874015748,
      "grad_norm": 4.2477827072143555,
      "learning_rate": 0.00018412698412698412,
      "loss": 0.6122,
      "step": 25
    },
    {
      "epoch": 0.4094488188976378,
      "grad_norm": 4.086635112762451,
      "learning_rate": 0.00018333333333333334,
      "loss": 0.8048,
      "step": 26
    },
    {
      "epoch": 0.4251968503937008,
      "grad_norm": 2.6676580905914307,
      "learning_rate": 0.00018253968253968255,
      "loss": 0.7801,
      "step": 27
    },
    {
      "epoch": 0.4409448818897638,
      "grad_norm": 23.283315658569336,
      "learning_rate": 0.00018174603174603177,
      "loss": 0.5557,
      "step": 28
    },
    {
      "epoch": 0.4566929133858268,
      "grad_norm": 19.271642684936523,
      "learning_rate": 0.00018095238095238095,
      "loss": 0.641,
      "step": 29
    },
    {
      "epoch": 0.47244094488188976,
      "grad_norm": 1.9418091773986816,
      "learning_rate": 0.00018015873015873017,
      "loss": 0.6093,
      "step": 30
    },
    {
      "epoch": 0.4881889763779528,
      "grad_norm": 3.199025869369507,
      "learning_rate": 0.00017936507936507938,
      "loss": 0.7414,
      "step": 31
    },
    {
      "epoch": 0.5039370078740157,
      "grad_norm": 2.8052518367767334,
      "learning_rate": 0.0001785714285714286,
      "loss": 0.6426,
      "step": 32
    },
    {
      "epoch": 0.5196850393700787,
      "grad_norm": 2.4842007160186768,
      "learning_rate": 0.00017777777777777779,
      "loss": 0.503,
      "step": 33
    },
    {
      "epoch": 0.5354330708661418,
      "grad_norm": 1.2727268934249878,
      "learning_rate": 0.00017698412698412697,
      "loss": 0.5043,
      "step": 34
    },
    {
      "epoch": 0.5511811023622047,
      "grad_norm": 2.8366410732269287,
      "learning_rate": 0.0001761904761904762,
      "loss": 0.6354,
      "step": 35
    },
    {
      "epoch": 0.5669291338582677,
      "grad_norm": 2.0861079692840576,
      "learning_rate": 0.0001753968253968254,
      "loss": 0.5995,
      "step": 36
    },
    {
      "epoch": 0.5826771653543307,
      "grad_norm": 1.2307648658752441,
      "learning_rate": 0.00017460317460317462,
      "loss": 0.4907,
      "step": 37
    },
    {
      "epoch": 0.5984251968503937,
      "grad_norm": 1.7936501502990723,
      "learning_rate": 0.00017380952380952383,
      "loss": 0.5708,
      "step": 38
    },
    {
      "epoch": 0.6141732283464567,
      "grad_norm": 2.1429905891418457,
      "learning_rate": 0.00017301587301587302,
      "loss": 0.5241,
      "step": 39
    },
    {
      "epoch": 0.6299212598425197,
      "grad_norm": 1.6337093114852905,
      "learning_rate": 0.00017222222222222224,
      "loss": 0.6412,
      "step": 40
    },
    {
      "epoch": 0.6456692913385826,
      "grad_norm": 1.6476855278015137,
      "learning_rate": 0.00017142857142857143,
      "loss": 0.4878,
      "step": 41
    },
    {
      "epoch": 0.6614173228346457,
      "grad_norm": 1.7862098217010498,
      "learning_rate": 0.00017063492063492064,
      "loss": 0.5093,
      "step": 42
    },
    {
      "epoch": 0.6771653543307087,
      "grad_norm": 1.0115402936935425,
      "learning_rate": 0.00016984126984126986,
      "loss": 0.3114,
      "step": 43
    },
    {
      "epoch": 0.6929133858267716,
      "grad_norm": 1.3242247104644775,
      "learning_rate": 0.00016904761904761904,
      "loss": 0.4772,
      "step": 44
    },
    {
      "epoch": 0.7086614173228346,
      "grad_norm": 1.486046552658081,
      "learning_rate": 0.00016825396825396826,
      "loss": 0.4705,
      "step": 45
    },
    {
      "epoch": 0.7244094488188977,
      "grad_norm": 1.4594428539276123,
      "learning_rate": 0.00016746031746031747,
      "loss": 0.4111,
      "step": 46
    },
    {
      "epoch": 0.7401574803149606,
      "grad_norm": 1.4396812915802002,
      "learning_rate": 0.0001666666666666667,
      "loss": 0.4073,
      "step": 47
    },
    {
      "epoch": 0.7559055118110236,
      "grad_norm": 1.246954083442688,
      "learning_rate": 0.0001658730158730159,
      "loss": 0.3507,
      "step": 48
    },
    {
      "epoch": 0.7716535433070866,
      "grad_norm": 1.6828356981277466,
      "learning_rate": 0.0001650793650793651,
      "loss": 0.4188,
      "step": 49
    },
    {
      "epoch": 0.7874015748031497,
      "grad_norm": 1.6923037767410278,
      "learning_rate": 0.00016428571428571428,
      "loss": 0.5214,
      "step": 50
    },
    {
      "epoch": 0.8031496062992126,
      "grad_norm": 1.3915568590164185,
      "learning_rate": 0.0001634920634920635,
      "loss": 0.3451,
      "step": 51
    },
    {
      "epoch": 0.8188976377952756,
      "grad_norm": 1.5584542751312256,
      "learning_rate": 0.0001626984126984127,
      "loss": 0.4497,
      "step": 52
    },
    {
      "epoch": 0.8346456692913385,
      "grad_norm": 2.2468090057373047,
      "learning_rate": 0.00016190476190476192,
      "loss": 0.5416,
      "step": 53
    },
    {
      "epoch": 0.8503937007874016,
      "grad_norm": 1.9666602611541748,
      "learning_rate": 0.0001611111111111111,
      "loss": 0.406,
      "step": 54
    },
    {
      "epoch": 0.8661417322834646,
      "grad_norm": 1.4973253011703491,
      "learning_rate": 0.00016031746031746033,
      "loss": 0.3295,
      "step": 55
    },
    {
      "epoch": 0.8818897637795275,
      "grad_norm": 1.2596267461776733,
      "learning_rate": 0.00015952380952380954,
      "loss": 0.3023,
      "step": 56
    },
    {
      "epoch": 0.8976377952755905,
      "grad_norm": 1.7468661069869995,
      "learning_rate": 0.00015873015873015873,
      "loss": 0.3501,
      "step": 57
    },
    {
      "epoch": 0.9133858267716536,
      "grad_norm": 1.6842601299285889,
      "learning_rate": 0.00015793650793650795,
      "loss": 0.3268,
      "step": 58
    },
    {
      "epoch": 0.9291338582677166,
      "grad_norm": 1.2835358381271362,
      "learning_rate": 0.00015714285714285716,
      "loss": 0.253,
      "step": 59
    },
    {
      "epoch": 0.9448818897637795,
      "grad_norm": 2.2003257274627686,
      "learning_rate": 0.00015634920634920635,
      "loss": 0.3845,
      "step": 60
    },
    {
      "epoch": 0.9606299212598425,
      "grad_norm": 6.542925834655762,
      "learning_rate": 0.00015555555555555556,
      "loss": 0.3752,
      "step": 61
    },
    {
      "epoch": 0.9763779527559056,
      "grad_norm": 3.2406811714172363,
      "learning_rate": 0.00015476190476190478,
      "loss": 0.5157,
      "step": 62
    },
    {
      "epoch": 0.9921259842519685,
      "grad_norm": 125.69523620605469,
      "learning_rate": 0.000153968253968254,
      "loss": 0.6105,
      "step": 63
    },
    {
      "epoch": 1.0,
      "grad_norm": 11.730782508850098,
      "learning_rate": 0.00015317460317460318,
      "loss": 0.1335,
      "step": 64
    },
    {
      "epoch": 1.0,
      "eval_loss": 0.38410627841949463,
      "eval_runtime": 21.3433,
      "eval_samples_per_second": 2.999,
      "eval_steps_per_second": 0.375,
      "step": 64
    },
    {
      "epoch": 1.015748031496063,
      "grad_norm": 32.78862380981445,
      "learning_rate": 0.00015238095238095237,
      "loss": 0.4979,
      "step": 65
    },
    {
      "epoch": 1.031496062992126,
      "grad_norm": 1.745581865310669,
      "learning_rate": 0.00015158730158730158,
      "loss": 0.4486,
      "step": 66
    },
    {
      "epoch": 1.047244094488189,
      "grad_norm": 1.5634509325027466,
      "learning_rate": 0.0001507936507936508,
      "loss": 0.3273,
      "step": 67
    },
    {
      "epoch": 1.0629921259842519,
      "grad_norm": 1.3514970541000366,
      "learning_rate": 0.00015000000000000001,
      "loss": 0.3426,
      "step": 68
    },
    {
      "epoch": 1.078740157480315,
      "grad_norm": 1.394451379776001,
      "learning_rate": 0.00014920634920634923,
      "loss": 0.3322,
      "step": 69
    },
    {
      "epoch": 1.094488188976378,
      "grad_norm": 1.37984037399292,
      "learning_rate": 0.00014841269841269842,
      "loss": 0.3112,
      "step": 70
    },
    {
      "epoch": 1.110236220472441,
      "grad_norm": 1.3448426723480225,
      "learning_rate": 0.00014761904761904763,
      "loss": 0.3059,
      "step": 71
    },
    {
      "epoch": 1.125984251968504,
      "grad_norm": 1.375622272491455,
      "learning_rate": 0.00014682539682539682,
      "loss": 0.3355,
      "step": 72
    },
    {
      "epoch": 1.141732283464567,
      "grad_norm": 1.4394969940185547,
      "learning_rate": 0.00014603174603174603,
      "loss": 0.2733,
      "step": 73
    },
    {
      "epoch": 1.1574803149606299,
      "grad_norm": 1.540462851524353,
      "learning_rate": 0.00014523809523809525,
      "loss": 0.4089,
      "step": 74
    },
    {
      "epoch": 1.1732283464566928,
      "grad_norm": 1.9492518901824951,
      "learning_rate": 0.00014444444444444444,
      "loss": 0.3453,
      "step": 75
    },
    {
      "epoch": 1.188976377952756,
      "grad_norm": 3.4683785438537598,
      "learning_rate": 0.00014365079365079365,
      "loss": 0.4694,
      "step": 76
    },
    {
      "epoch": 1.204724409448819,
      "grad_norm": 2.0165913105010986,
      "learning_rate": 0.00014285714285714287,
      "loss": 0.3623,
      "step": 77
    },
    {
      "epoch": 1.220472440944882,
      "grad_norm": 1.763242483139038,
      "learning_rate": 0.00014206349206349208,
      "loss": 0.2978,
      "step": 78
    },
    {
      "epoch": 1.236220472440945,
      "grad_norm": 1.8628298044204712,
      "learning_rate": 0.0001412698412698413,
      "loss": 0.3031,
      "step": 79
    },
    {
      "epoch": 1.2519685039370079,
      "grad_norm": 1.6428312063217163,
      "learning_rate": 0.00014047619047619049,
      "loss": 0.2694,
      "step": 80
    },
    {
      "epoch": 1.2677165354330708,
      "grad_norm": 1.9064754247665405,
      "learning_rate": 0.00013968253968253967,
      "loss": 0.2884,
      "step": 81
    },
    {
      "epoch": 1.2834645669291338,
      "grad_norm": 2.6578309535980225,
      "learning_rate": 0.0001388888888888889,
      "loss": 0.4188,
      "step": 82
    },
    {
      "epoch": 1.2992125984251968,
      "grad_norm": 2.504312753677368,
      "learning_rate": 0.0001380952380952381,
      "loss": 0.4232,
      "step": 83
    },
    {
      "epoch": 1.3149606299212597,
      "grad_norm": 2.1143362522125244,
      "learning_rate": 0.00013730158730158732,
      "loss": 0.3818,
      "step": 84
    },
    {
      "epoch": 1.330708661417323,
      "grad_norm": 1.8358464241027832,
      "learning_rate": 0.0001365079365079365,
      "loss": 0.33,
      "step": 85
    },
    {
      "epoch": 1.3464566929133859,
      "grad_norm": 2.557461738586426,
      "learning_rate": 0.00013571428571428572,
      "loss": 0.3008,
      "step": 86
    },
    {
      "epoch": 1.3622047244094488,
      "grad_norm": 2.1440603733062744,
      "learning_rate": 0.00013492063492063494,
      "loss": 0.3588,
      "step": 87
    },
    {
      "epoch": 1.3779527559055118,
      "grad_norm": 2.188957929611206,
      "learning_rate": 0.00013412698412698412,
      "loss": 0.409,
      "step": 88
    },
    {
      "epoch": 1.3937007874015748,
      "grad_norm": 1.3684735298156738,
      "learning_rate": 0.00013333333333333334,
      "loss": 0.2327,
      "step": 89
    },
    {
      "epoch": 1.4094488188976377,
      "grad_norm": 2.128376007080078,
      "learning_rate": 0.00013253968253968255,
      "loss": 0.3285,
      "step": 90
    },
    {
      "epoch": 1.425196850393701,
      "grad_norm": 2.01208233833313,
      "learning_rate": 0.00013174603174603174,
      "loss": 0.2409,
      "step": 91
    },
    {
      "epoch": 1.4409448818897639,
      "grad_norm": 1.489335536956787,
      "learning_rate": 0.00013095238095238096,
      "loss": 0.2558,
      "step": 92
    },
    {
      "epoch": 1.4566929133858268,
      "grad_norm": 1.838681697845459,
      "learning_rate": 0.00013015873015873017,
      "loss": 0.309,
      "step": 93
    },
    {
      "epoch": 1.4724409448818898,
      "grad_norm": 1.4299134016036987,
      "learning_rate": 0.0001293650793650794,
      "loss": 0.2387,
      "step": 94
    },
    {
      "epoch": 1.4881889763779528,
      "grad_norm": 1.7235240936279297,
      "learning_rate": 0.00012857142857142858,
      "loss": 0.236,
      "step": 95
    },
    {
      "epoch": 1.5039370078740157,
      "grad_norm": 1.9900957345962524,
      "learning_rate": 0.00012777777777777776,
      "loss": 0.2718,
      "step": 96
    },
    {
      "epoch": 1.5196850393700787,
      "grad_norm": 1.971561312675476,
      "learning_rate": 0.00012698412698412698,
      "loss": 0.3079,
      "step": 97
    },
    {
      "epoch": 1.5354330708661417,
      "grad_norm": 2.1540791988372803,
      "learning_rate": 0.0001261904761904762,
      "loss": 0.3533,
      "step": 98
    },
    {
      "epoch": 1.5511811023622046,
      "grad_norm": 3.294546365737915,
      "learning_rate": 0.0001253968253968254,
      "loss": 0.399,
      "step": 99
    },
    {
      "epoch": 1.5669291338582676,
      "grad_norm": 2.101137161254883,
      "learning_rate": 0.00012460317460317462,
      "loss": 0.4173,
      "step": 100
    },
    {
      "epoch": 1.5826771653543306,
      "grad_norm": 2.0106465816497803,
      "learning_rate": 0.0001238095238095238,
      "loss": 0.3484,
      "step": 101
    },
    {
      "epoch": 1.5984251968503937,
      "grad_norm": 1.8793835639953613,
      "learning_rate": 0.00012301587301587303,
      "loss": 0.3336,
      "step": 102
    },
    {
      "epoch": 1.6141732283464567,
      "grad_norm": 1.769931674003601,
      "learning_rate": 0.00012222222222222224,
      "loss": 0.3426,
      "step": 103
    },
    {
      "epoch": 1.6299212598425197,
      "grad_norm": 2.280139923095703,
      "learning_rate": 0.00012142857142857143,
      "loss": 0.4314,
      "step": 104
    },
    {
      "epoch": 1.6456692913385826,
      "grad_norm": 1.7232383489608765,
      "learning_rate": 0.00012063492063492063,
      "loss": 0.2887,
      "step": 105
    },
    {
      "epoch": 1.6614173228346458,
      "grad_norm": 2.151813507080078,
      "learning_rate": 0.00011984126984126985,
      "loss": 0.2765,
      "step": 106
    },
    {
      "epoch": 1.6771653543307088,
      "grad_norm": 1.9582260847091675,
      "learning_rate": 0.00011904761904761905,
      "loss": 0.3961,
      "step": 107
    },
    {
      "epoch": 1.6929133858267718,
      "grad_norm": 2.024562120437622,
      "learning_rate": 0.00011825396825396826,
      "loss": 0.3602,
      "step": 108
    },
    {
      "epoch": 1.7086614173228347,
      "grad_norm": 2.3845021724700928,
      "learning_rate": 0.00011746031746031746,
      "loss": 0.3487,
      "step": 109
    },
    {
      "epoch": 1.7244094488188977,
      "grad_norm": 1.5476869344711304,
      "learning_rate": 0.00011666666666666668,
      "loss": 0.265,
      "step": 110
    },
    {
      "epoch": 1.7401574803149606,
      "grad_norm": 1.6451506614685059,
      "learning_rate": 0.0001158730158730159,
      "loss": 0.3032,
      "step": 111
    },
    {
      "epoch": 1.7559055118110236,
      "grad_norm": 2.5987555980682373,
      "learning_rate": 0.00011507936507936508,
      "loss": 0.4066,
      "step": 112
    },
    {
      "epoch": 1.7716535433070866,
      "grad_norm": 1.9586522579193115,
      "learning_rate": 0.00011428571428571428,
      "loss": 0.308,
      "step": 113
    },
    {
      "epoch": 1.7874015748031495,
      "grad_norm": 1.7919501066207886,
      "learning_rate": 0.0001134920634920635,
      "loss": 0.3446,
      "step": 114
    },
    {
      "epoch": 1.8031496062992125,
      "grad_norm": 2.1075942516326904,
      "learning_rate": 0.0001126984126984127,
      "loss": 0.2476,
      "step": 115
    },
    {
      "epoch": 1.8188976377952755,
      "grad_norm": 2.6541597843170166,
      "learning_rate": 0.00011190476190476191,
      "loss": 0.3831,
      "step": 116
    },
    {
      "epoch": 1.8346456692913384,
      "grad_norm": 1.9596030712127686,
      "learning_rate": 0.00011111111111111112,
      "loss": 0.2566,
      "step": 117
    },
    {
      "epoch": 1.8503937007874016,
      "grad_norm": 2.2717809677124023,
      "learning_rate": 0.00011031746031746033,
      "loss": 0.3642,
      "step": 118
    },
    {
      "epoch": 1.8661417322834646,
      "grad_norm": 1.8920717239379883,
      "learning_rate": 0.00010952380952380953,
      "loss": 0.2993,
      "step": 119
    },
    {
      "epoch": 1.8818897637795275,
      "grad_norm": 1.871362328529358,
      "learning_rate": 0.00010873015873015872,
      "loss": 0.2745,
      "step": 120
    },
    {
      "epoch": 1.8976377952755905,
      "grad_norm": 1.6640174388885498,
      "learning_rate": 0.00010793650793650794,
      "loss": 0.2368,
      "step": 121
    },
    {
      "epoch": 1.9133858267716537,
      "grad_norm": 2.150925397872925,
      "learning_rate": 0.00010714285714285715,
      "loss": 0.2666,
      "step": 122
    },
    {
      "epoch": 1.9291338582677167,
      "grad_norm": 2.502284526824951,
      "learning_rate": 0.00010634920634920635,
      "loss": 0.324,
      "step": 123
    },
    {
      "epoch": 1.9448818897637796,
      "grad_norm": 2.0422449111938477,
      "learning_rate": 0.00010555555555555557,
      "loss": 0.3707,
      "step": 124
    },
    {
      "epoch": 1.9606299212598426,
      "grad_norm": 1.908556342124939,
      "learning_rate": 0.00010476190476190477,
      "loss": 0.3044,
      "step": 125
    },
    {
      "epoch": 1.9763779527559056,
      "grad_norm": 1.7334545850753784,
      "learning_rate": 0.00010396825396825398,
      "loss": 0.2614,
      "step": 126
    },
    {
      "epoch": 1.9921259842519685,
      "grad_norm": 2.288590908050537,
      "learning_rate": 0.00010317460317460319,
      "loss": 0.2642,
      "step": 127
    },
    {
      "epoch": 2.0,
      "grad_norm": 2.062268018722534,
      "learning_rate": 0.00010238095238095237,
      "loss": 0.2041,
      "step": 128
    },
    {
      "epoch": 2.0,
      "eval_loss": 0.3182927966117859,
      "eval_runtime": 21.0728,
      "eval_samples_per_second": 3.037,
      "eval_steps_per_second": 0.38,
      "step": 128
    },
    {
      "epoch": 2.015748031496063,
      "grad_norm": 2.10591459274292,
      "learning_rate": 0.00010158730158730159,
      "loss": 0.3254,
      "step": 129
    },
    {
      "epoch": 2.031496062992126,
      "grad_norm": 1.3043678998947144,
      "learning_rate": 0.00010079365079365079,
      "loss": 0.1635,
      "step": 130
    },
    {
      "epoch": 2.047244094488189,
      "grad_norm": 1.677348256111145,
      "learning_rate": 0.0001,
      "loss": 0.2558,
      "step": 131
    },
    {
      "epoch": 2.062992125984252,
      "grad_norm": 1.7451097965240479,
      "learning_rate": 9.920634920634922e-05,
      "loss": 0.2773,
      "step": 132
    },
    {
      "epoch": 2.078740157480315,
      "grad_norm": 2.3676819801330566,
      "learning_rate": 9.841269841269841e-05,
      "loss": 0.3855,
      "step": 133
    },
    {
      "epoch": 2.094488188976378,
      "grad_norm": 2.1864981651306152,
      "learning_rate": 9.761904761904762e-05,
      "loss": 0.3222,
      "step": 134
    },
    {
      "epoch": 2.1102362204724407,
      "grad_norm": 2.135253429412842,
      "learning_rate": 9.682539682539682e-05,
      "loss": 0.2994,
      "step": 135
    },
    {
      "epoch": 2.1259842519685037,
      "grad_norm": 1.6539878845214844,
      "learning_rate": 9.603174603174604e-05,
      "loss": 0.2174,
      "step": 136
    },
    {
      "epoch": 2.141732283464567,
      "grad_norm": 1.9839508533477783,
      "learning_rate": 9.523809523809524e-05,
      "loss": 0.2505,
      "step": 137
    },
    {
      "epoch": 2.15748031496063,
      "grad_norm": 1.8000147342681885,
      "learning_rate": 9.444444444444444e-05,
      "loss": 0.2191,
      "step": 138
    },
    {
      "epoch": 2.173228346456693,
      "grad_norm": 2.040043354034424,
      "learning_rate": 9.365079365079366e-05,
      "loss": 0.2622,
      "step": 139
    },
    {
      "epoch": 2.188976377952756,
      "grad_norm": 1.9660704135894775,
      "learning_rate": 9.285714285714286e-05,
      "loss": 0.2133,
      "step": 140
    },
    {
      "epoch": 2.204724409448819,
      "grad_norm": 2.5500683784484863,
      "learning_rate": 9.206349206349206e-05,
      "loss": 0.2579,
      "step": 141
    },
    {
      "epoch": 2.220472440944882,
      "grad_norm": 2.6057305335998535,
      "learning_rate": 9.126984126984128e-05,
      "loss": 0.3077,
      "step": 142
    },
    {
      "epoch": 2.236220472440945,
      "grad_norm": 2.1529440879821777,
      "learning_rate": 9.047619047619048e-05,
      "loss": 0.1935,
      "step": 143
    },
    {
      "epoch": 2.251968503937008,
      "grad_norm": 1.9283859729766846,
      "learning_rate": 8.968253968253969e-05,
      "loss": 0.1683,
      "step": 144
    },
    {
      "epoch": 2.267716535433071,
      "grad_norm": 4.255101203918457,
      "learning_rate": 8.888888888888889e-05,
      "loss": 0.3928,
      "step": 145
    },
    {
      "epoch": 2.283464566929134,
      "grad_norm": 2.543056011199951,
      "learning_rate": 8.80952380952381e-05,
      "loss": 0.2318,
      "step": 146
    },
    {
      "epoch": 2.2992125984251968,
      "grad_norm": 2.5640785694122314,
      "learning_rate": 8.730158730158731e-05,
      "loss": 0.256,
      "step": 147
    },
    {
      "epoch": 2.3149606299212597,
      "grad_norm": 2.525496244430542,
      "learning_rate": 8.650793650793651e-05,
      "loss": 0.2453,
      "step": 148
    },
    {
      "epoch": 2.3307086614173227,
      "grad_norm": 2.5470335483551025,
      "learning_rate": 8.571428571428571e-05,
      "loss": 0.2733,
      "step": 149
    },
    {
      "epoch": 2.3464566929133857,
      "grad_norm": 2.5498080253601074,
      "learning_rate": 8.492063492063493e-05,
      "loss": 0.269,
      "step": 150
    },
    {
      "epoch": 2.362204724409449,
      "grad_norm": 3.048393964767456,
      "learning_rate": 8.412698412698413e-05,
      "loss": 0.2954,
      "step": 151
    },
    {
      "epoch": 2.377952755905512,
      "grad_norm": 2.051837921142578,
      "learning_rate": 8.333333333333334e-05,
      "loss": 0.2398,
      "step": 152
    },
    {
      "epoch": 2.393700787401575,
      "grad_norm": 2.1074423789978027,
      "learning_rate": 8.253968253968255e-05,
      "loss": 0.1736,
      "step": 153
    },
    {
      "epoch": 2.409448818897638,
      "grad_norm": 2.5150201320648193,
      "learning_rate": 8.174603174603175e-05,
      "loss": 0.3152,
      "step": 154
    },
    {
      "epoch": 2.425196850393701,
      "grad_norm": 2.5052995681762695,
      "learning_rate": 8.095238095238096e-05,
      "loss": 0.2381,
      "step": 155
    },
    {
      "epoch": 2.440944881889764,
      "grad_norm": 2.0568888187408447,
      "learning_rate": 8.015873015873016e-05,
      "loss": 0.2645,
      "step": 156
    },
    {
      "epoch": 2.456692913385827,
      "grad_norm": 2.471665620803833,
      "learning_rate": 7.936507936507937e-05,
      "loss": 0.1949,
      "step": 157
    },
    {
      "epoch": 2.47244094488189,
      "grad_norm": 2.1175425052642822,
      "learning_rate": 7.857142857142858e-05,
      "loss": 0.3159,
      "step": 158
    },
    {
      "epoch": 2.4881889763779528,
      "grad_norm": 2.7063076496124268,
      "learning_rate": 7.777777777777778e-05,
      "loss": 0.3768,
      "step": 159
    },
    {
      "epoch": 2.5039370078740157,
      "grad_norm": 1.9972110986709595,
      "learning_rate": 7.6984126984127e-05,
      "loss": 0.2854,
      "step": 160
    },
    {
      "epoch": 2.5196850393700787,
      "grad_norm": 2.138082981109619,
      "learning_rate": 7.619047619047618e-05,
      "loss": 0.2796,
      "step": 161
    },
    {
      "epoch": 2.5354330708661417,
      "grad_norm": 1.7919983863830566,
      "learning_rate": 7.53968253968254e-05,
      "loss": 0.1886,
      "step": 162
    },
    {
      "epoch": 2.5511811023622046,
      "grad_norm": 2.421928882598877,
      "learning_rate": 7.460317460317461e-05,
      "loss": 0.2661,
      "step": 163
    },
    {
      "epoch": 2.5669291338582676,
      "grad_norm": 1.9975031614303589,
      "learning_rate": 7.380952380952382e-05,
      "loss": 0.1968,
      "step": 164
    },
    {
      "epoch": 2.5826771653543306,
      "grad_norm": 2.0155351161956787,
      "learning_rate": 7.301587301587302e-05,
      "loss": 0.2829,
      "step": 165
    },
    {
      "epoch": 2.5984251968503935,
      "grad_norm": 2.2947115898132324,
      "learning_rate": 7.222222222222222e-05,
      "loss": 0.2081,
      "step": 166
    },
    {
      "epoch": 2.6141732283464565,
      "grad_norm": 1.9683176279067993,
      "learning_rate": 7.142857142857143e-05,
      "loss": 0.2264,
      "step": 167
    },
    {
      "epoch": 2.6299212598425195,
      "grad_norm": 2.2848334312438965,
      "learning_rate": 7.063492063492065e-05,
      "loss": 0.2954,
      "step": 168
    },
    {
      "epoch": 2.6456692913385824,
      "grad_norm": 2.253952980041504,
      "learning_rate": 6.984126984126984e-05,
      "loss": 0.2081,
      "step": 169
    },
    {
      "epoch": 2.661417322834646,
      "grad_norm": 2.5048093795776367,
      "learning_rate": 6.904761904761905e-05,
      "loss": 0.3095,
      "step": 170
    },
    {
      "epoch": 2.677165354330709,
      "grad_norm": 2.590207815170288,
      "learning_rate": 6.825396825396825e-05,
      "loss": 0.3647,
      "step": 171
    },
    {
      "epoch": 2.6929133858267718,
      "grad_norm": 2.1894516944885254,
      "learning_rate": 6.746031746031747e-05,
      "loss": 0.2312,
      "step": 172
    },
    {
      "epoch": 2.7086614173228347,
      "grad_norm": 2.610740900039673,
      "learning_rate": 6.666666666666667e-05,
      "loss": 0.272,
      "step": 173
    },
    {
      "epoch": 2.7244094488188977,
      "grad_norm": 2.501737356185913,
      "learning_rate": 6.587301587301587e-05,
      "loss": 0.2224,
      "step": 174
    },
    {
      "epoch": 2.7401574803149606,
      "grad_norm": 2.470640182495117,
      "learning_rate": 6.507936507936509e-05,
      "loss": 0.2056,
      "step": 175
    },
    {
      "epoch": 2.7559055118110236,
      "grad_norm": 3.0782885551452637,
      "learning_rate": 6.428571428571429e-05,
      "loss": 0.2712,
      "step": 176
    },
    {
      "epoch": 2.7716535433070866,
      "grad_norm": 3.054872512817383,
      "learning_rate": 6.349206349206349e-05,
      "loss": 0.3317,
      "step": 177
    },
    {
      "epoch": 2.7874015748031495,
      "grad_norm": 2.8595480918884277,
      "learning_rate": 6.26984126984127e-05,
      "loss": 0.3295,
      "step": 178
    },
    {
      "epoch": 2.8031496062992125,
      "grad_norm": 2.7422428131103516,
      "learning_rate": 6.19047619047619e-05,
      "loss": 0.2954,
      "step": 179
    },
    {
      "epoch": 2.8188976377952755,
      "grad_norm": 1.9640673398971558,
      "learning_rate": 6.111111111111112e-05,
      "loss": 0.193,
      "step": 180
    },
    {
      "epoch": 2.8346456692913384,
      "grad_norm": 2.0778565406799316,
      "learning_rate": 6.0317460317460316e-05,
      "loss": 0.1641,
      "step": 181
    },
    {
      "epoch": 2.850393700787402,
      "grad_norm": 2.4479618072509766,
      "learning_rate": 5.9523809523809524e-05,
      "loss": 0.2412,
      "step": 182
    },
    {
      "epoch": 2.866141732283465,
      "grad_norm": 2.5711357593536377,
      "learning_rate": 5.873015873015873e-05,
      "loss": 0.2959,
      "step": 183
    },
    {
      "epoch": 2.8818897637795278,
      "grad_norm": 2.9728593826293945,
      "learning_rate": 5.793650793650795e-05,
      "loss": 0.3645,
      "step": 184
    },
    {
      "epoch": 2.8976377952755907,
      "grad_norm": 2.0194597244262695,
      "learning_rate": 5.714285714285714e-05,
      "loss": 0.1502,
      "step": 185
    },
    {
      "epoch": 2.9133858267716537,
      "grad_norm": 2.4805350303649902,
      "learning_rate": 5.634920634920635e-05,
      "loss": 0.1834,
      "step": 186
    },
    {
      "epoch": 2.9291338582677167,
      "grad_norm": 2.595513105392456,
      "learning_rate": 5.555555555555556e-05,
      "loss": 0.2464,
      "step": 187
    },
    {
      "epoch": 2.9448818897637796,
      "grad_norm": 2.4954440593719482,
      "learning_rate": 5.4761904761904766e-05,
      "loss": 0.2433,
      "step": 188
    },
    {
      "epoch": 2.9606299212598426,
      "grad_norm": 2.0151891708374023,
      "learning_rate": 5.396825396825397e-05,
      "loss": 0.2193,
      "step": 189
    },
    {
      "epoch": 2.9763779527559056,
      "grad_norm": 2.495774745941162,
      "learning_rate": 5.3174603174603176e-05,
      "loss": 0.2044,
      "step": 190
    },
    {
      "epoch": 2.9921259842519685,
      "grad_norm": 2.4585587978363037,
      "learning_rate": 5.2380952380952384e-05,
      "loss": 0.2781,
      "step": 191
    },
    {
      "epoch": 3.0,
      "grad_norm": 2.2578125,
      "learning_rate": 5.158730158730159e-05,
      "loss": 0.1988,
      "step": 192
    },
    {
      "epoch": 3.0,
      "eval_loss": 0.29737889766693115,
      "eval_runtime": 21.1024,
      "eval_samples_per_second": 3.033,
      "eval_steps_per_second": 0.379,
      "step": 192
    }
  ],
  "logging_steps": 1,
  "max_steps": 252,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 4,
  "save_steps": 500,
  "stateful_callbacks": {
    "EarlyStoppingCallback": {
      "args": {
        "early_stopping_patience": 5,
        "early_stopping_threshold": 0.0
      },
      "attributes": {
        "early_stopping_patience_counter": 0
      }
    },
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 8326532799922176.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
