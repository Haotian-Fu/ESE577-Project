{
  "best_metric": 0.22728084027767181,
  "best_model_checkpoint": "./fine_tuned_model_epoch8\\checkpoint-320",
  "epoch": 7.881889763779528,
  "eval_steps": 500,
  "global_step": 504,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.015748031496062992,
      "grad_norm": NaN,
      "learning_rate": 0.0002,
      "loss": 15.2911,
      "step": 1
    },
    {
      "epoch": 0.031496062992125984,
      "grad_norm": NaN,
      "learning_rate": 0.0002,
      "loss": 14.7429,
      "step": 2
    },
    {
      "epoch": 0.047244094488188976,
      "grad_norm": NaN,
      "learning_rate": 0.0002,
      "loss": 16.3986,
      "step": 3
    },
    {
      "epoch": 0.06299212598425197,
      "grad_norm": NaN,
      "learning_rate": 0.0002,
      "loss": 14.807,
      "step": 4
    },
    {
      "epoch": 0.07874015748031496,
      "grad_norm": Infinity,
      "learning_rate": 0.0002,
      "loss": 14.8893,
      "step": 5
    },
    {
      "epoch": 0.09448818897637795,
      "grad_norm": 433.0682067871094,
      "learning_rate": 0.0001996031746031746,
      "loss": 14.6601,
      "step": 6
    },
    {
      "epoch": 0.11023622047244094,
      "grad_norm": 224.33905029296875,
      "learning_rate": 0.00019920634920634922,
      "loss": 9.2891,
      "step": 7
    },
    {
      "epoch": 0.12598425196850394,
      "grad_norm": 165.7130126953125,
      "learning_rate": 0.00019880952380952382,
      "loss": 4.4512,
      "step": 8
    },
    {
      "epoch": 0.14173228346456693,
      "grad_norm": 61.74953079223633,
      "learning_rate": 0.00019841269841269844,
      "loss": 1.8237,
      "step": 9
    },
    {
      "epoch": 0.15748031496062992,
      "grad_norm": 12.171860694885254,
      "learning_rate": 0.00019801587301587303,
      "loss": 1.1742,
      "step": 10
    },
    {
      "epoch": 0.1732283464566929,
      "grad_norm": 41.602561950683594,
      "learning_rate": 0.00019761904761904763,
      "loss": 1.1866,
      "step": 11
    },
    {
      "epoch": 0.1889763779527559,
      "grad_norm": 4.983820915222168,
      "learning_rate": 0.00019722222222222225,
      "loss": 1.1562,
      "step": 12
    },
    {
      "epoch": 0.2047244094488189,
      "grad_norm": 3.8152122497558594,
      "learning_rate": 0.00019682539682539682,
      "loss": 1.0509,
      "step": 13
    },
    {
      "epoch": 0.2204724409448819,
      "grad_norm": 4.861351490020752,
      "learning_rate": 0.00019642857142857144,
      "loss": 1.1156,
      "step": 14
    },
    {
      "epoch": 0.23622047244094488,
      "grad_norm": 3.240114450454712,
      "learning_rate": 0.00019603174603174603,
      "loss": 0.8859,
      "step": 15
    },
    {
      "epoch": 0.25196850393700787,
      "grad_norm": 3.6167287826538086,
      "learning_rate": 0.00019563492063492062,
      "loss": 0.9633,
      "step": 16
    },
    {
      "epoch": 0.2677165354330709,
      "grad_norm": 3.121445417404175,
      "learning_rate": 0.00019523809523809525,
      "loss": 0.9668,
      "step": 17
    },
    {
      "epoch": 0.28346456692913385,
      "grad_norm": 2.7703170776367188,
      "learning_rate": 0.00019484126984126984,
      "loss": 0.9225,
      "step": 18
    },
    {
      "epoch": 0.2992125984251969,
      "grad_norm": 2.192172050476074,
      "learning_rate": 0.00019444444444444446,
      "loss": 0.7547,
      "step": 19
    },
    {
      "epoch": 0.31496062992125984,
      "grad_norm": 3.5195865631103516,
      "learning_rate": 0.00019404761904761905,
      "loss": 1.0294,
      "step": 20
    },
    {
      "epoch": 0.33070866141732286,
      "grad_norm": 4.188701152801514,
      "learning_rate": 0.00019365079365079365,
      "loss": 0.7592,
      "step": 21
    },
    {
      "epoch": 0.3464566929133858,
      "grad_norm": 12.986133575439453,
      "learning_rate": 0.00019325396825396827,
      "loss": 0.6141,
      "step": 22
    },
    {
      "epoch": 0.36220472440944884,
      "grad_norm": 3.9167630672454834,
      "learning_rate": 0.00019285714285714286,
      "loss": 0.6959,
      "step": 23
    },
    {
      "epoch": 0.3779527559055118,
      "grad_norm": 4.161969184875488,
      "learning_rate": 0.00019246031746031748,
      "loss": 0.8047,
      "step": 24
    },
    {
      "epoch": 0.3937007874015748,
      "grad_norm": 4.475394248962402,
      "learning_rate": 0.00019206349206349208,
      "loss": 0.572,
      "step": 25
    },
    {
      "epoch": 0.4094488188976378,
      "grad_norm": 2.2951812744140625,
      "learning_rate": 0.00019166666666666667,
      "loss": 0.7128,
      "step": 26
    },
    {
      "epoch": 0.4251968503937008,
      "grad_norm": 1.9565112590789795,
      "learning_rate": 0.0001912698412698413,
      "loss": 0.7271,
      "step": 27
    },
    {
      "epoch": 0.4409448818897638,
      "grad_norm": 6.345472812652588,
      "learning_rate": 0.0001908730158730159,
      "loss": 0.4957,
      "step": 28
    },
    {
      "epoch": 0.4566929133858268,
      "grad_norm": 3.8634824752807617,
      "learning_rate": 0.00019047619047619048,
      "loss": 0.5958,
      "step": 29
    },
    {
      "epoch": 0.47244094488188976,
      "grad_norm": 1.8414191007614136,
      "learning_rate": 0.00019007936507936508,
      "loss": 0.5752,
      "step": 30
    },
    {
      "epoch": 0.4881889763779528,
      "grad_norm": 8.01447582244873,
      "learning_rate": 0.0001896825396825397,
      "loss": 0.7695,
      "step": 31
    },
    {
      "epoch": 0.5039370078740157,
      "grad_norm": 2.407512664794922,
      "learning_rate": 0.0001892857142857143,
      "loss": 0.5894,
      "step": 32
    },
    {
      "epoch": 0.5196850393700787,
      "grad_norm": 4.859897613525391,
      "learning_rate": 0.00018888888888888888,
      "loss": 0.5066,
      "step": 33
    },
    {
      "epoch": 0.5354330708661418,
      "grad_norm": 4.564650535583496,
      "learning_rate": 0.0001884920634920635,
      "loss": 0.5181,
      "step": 34
    },
    {
      "epoch": 0.5511811023622047,
      "grad_norm": 1.5613516569137573,
      "learning_rate": 0.0001880952380952381,
      "loss": 0.5964,
      "step": 35
    },
    {
      "epoch": 0.5669291338582677,
      "grad_norm": 3.1876168251037598,
      "learning_rate": 0.0001876984126984127,
      "loss": 0.577,
      "step": 36
    },
    {
      "epoch": 0.5826771653543307,
      "grad_norm": 1.708217978477478,
      "learning_rate": 0.00018730158730158731,
      "loss": 0.4752,
      "step": 37
    },
    {
      "epoch": 0.5984251968503937,
      "grad_norm": 1.3809243440628052,
      "learning_rate": 0.0001869047619047619,
      "loss": 0.5498,
      "step": 38
    },
    {
      "epoch": 0.6141732283464567,
      "grad_norm": 3.4020936489105225,
      "learning_rate": 0.00018650793650793653,
      "loss": 0.5283,
      "step": 39
    },
    {
      "epoch": 0.6299212598425197,
      "grad_norm": 2.040498733520508,
      "learning_rate": 0.00018611111111111112,
      "loss": 0.6837,
      "step": 40
    },
    {
      "epoch": 0.6456692913385826,
      "grad_norm": 1.5687823295593262,
      "learning_rate": 0.00018571428571428572,
      "loss": 0.4832,
      "step": 41
    },
    {
      "epoch": 0.6614173228346457,
      "grad_norm": 1.3591985702514648,
      "learning_rate": 0.00018531746031746034,
      "loss": 0.4852,
      "step": 42
    },
    {
      "epoch": 0.6771653543307087,
      "grad_norm": 0.9735559821128845,
      "learning_rate": 0.00018492063492063493,
      "loss": 0.3113,
      "step": 43
    },
    {
      "epoch": 0.6929133858267716,
      "grad_norm": 2.077511787414551,
      "learning_rate": 0.00018452380952380955,
      "loss": 0.4649,
      "step": 44
    },
    {
      "epoch": 0.7086614173228346,
      "grad_norm": 1.3470317125320435,
      "learning_rate": 0.00018412698412698412,
      "loss": 0.4633,
      "step": 45
    },
    {
      "epoch": 0.7244094488188977,
      "grad_norm": 1.7997905015945435,
      "learning_rate": 0.00018373015873015874,
      "loss": 0.4096,
      "step": 46
    },
    {
      "epoch": 0.7401574803149606,
      "grad_norm": 1.3983244895935059,
      "learning_rate": 0.00018333333333333334,
      "loss": 0.3829,
      "step": 47
    },
    {
      "epoch": 0.7559055118110236,
      "grad_norm": 1.1342058181762695,
      "learning_rate": 0.00018293650793650793,
      "loss": 0.3477,
      "step": 48
    },
    {
      "epoch": 0.7716535433070866,
      "grad_norm": 1.7172402143478394,
      "learning_rate": 0.00018253968253968255,
      "loss": 0.4477,
      "step": 49
    },
    {
      "epoch": 0.7874015748031497,
      "grad_norm": 2.0650856494903564,
      "learning_rate": 0.00018214285714285714,
      "loss": 0.5591,
      "step": 50
    },
    {
      "epoch": 0.8031496062992126,
      "grad_norm": 1.4535460472106934,
      "learning_rate": 0.00018174603174603177,
      "loss": 0.3767,
      "step": 51
    },
    {
      "epoch": 0.8188976377952756,
      "grad_norm": 1.27092707157135,
      "learning_rate": 0.00018134920634920636,
      "loss": 0.3772,
      "step": 52
    },
    {
      "epoch": 0.8346456692913385,
      "grad_norm": 1.7781442403793335,
      "learning_rate": 0.00018095238095238095,
      "loss": 0.5091,
      "step": 53
    },
    {
      "epoch": 0.8503937007874016,
      "grad_norm": 1.6461998224258423,
      "learning_rate": 0.00018055555555555557,
      "loss": 0.4011,
      "step": 54
    },
    {
      "epoch": 0.8661417322834646,
      "grad_norm": 1.5765858888626099,
      "learning_rate": 0.00018015873015873017,
      "loss": 0.3269,
      "step": 55
    },
    {
      "epoch": 0.8818897637795275,
      "grad_norm": 1.274756669998169,
      "learning_rate": 0.00017976190476190476,
      "loss": 0.3384,
      "step": 56
    },
    {
      "epoch": 0.8976377952755905,
      "grad_norm": 1.3688441514968872,
      "learning_rate": 0.00017936507936507938,
      "loss": 0.3197,
      "step": 57
    },
    {
      "epoch": 0.9133858267716536,
      "grad_norm": 1.5049198865890503,
      "learning_rate": 0.00017896825396825398,
      "loss": 0.3397,
      "step": 58
    },
    {
      "epoch": 0.9291338582677166,
      "grad_norm": 1.2261995077133179,
      "learning_rate": 0.0001785714285714286,
      "loss": 0.2516,
      "step": 59
    },
    {
      "epoch": 0.9448818897637795,
      "grad_norm": 3.5724287033081055,
      "learning_rate": 0.0001781746031746032,
      "loss": 0.369,
      "step": 60
    },
    {
      "epoch": 0.9606299212598425,
      "grad_norm": 1.6112473011016846,
      "learning_rate": 0.00017777777777777779,
      "loss": 0.3678,
      "step": 61
    },
    {
      "epoch": 0.9763779527559056,
      "grad_norm": 2.134126901626587,
      "learning_rate": 0.00017738095238095238,
      "loss": 0.4895,
      "step": 62
    },
    {
      "epoch": 0.9921259842519685,
      "grad_norm": 1.5902079343795776,
      "learning_rate": 0.00017698412698412697,
      "loss": 0.4785,
      "step": 63
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.6243681311607361,
      "learning_rate": 0.0001765873015873016,
      "loss": 0.1171,
      "step": 64
    },
    {
      "epoch": 1.0,
      "eval_loss": 0.3820173740386963,
      "eval_runtime": 20.9821,
      "eval_samples_per_second": 3.05,
      "eval_steps_per_second": 0.381,
      "step": 64
    },
    {
      "epoch": 1.015748031496063,
      "grad_norm": 1.3908839225769043,
      "learning_rate": 0.0001761904761904762,
      "loss": 0.392,
      "step": 65
    },
    {
      "epoch": 1.031496062992126,
      "grad_norm": 1.4478265047073364,
      "learning_rate": 0.0001757936507936508,
      "loss": 0.3981,
      "step": 66
    },
    {
      "epoch": 1.047244094488189,
      "grad_norm": 1.4867650270462036,
      "learning_rate": 0.0001753968253968254,
      "loss": 0.3325,
      "step": 67
    },
    {
      "epoch": 1.0629921259842519,
      "grad_norm": 1.528521180152893,
      "learning_rate": 0.000175,
      "loss": 0.3842,
      "step": 68
    },
    {
      "epoch": 1.078740157480315,
      "grad_norm": 1.387339472770691,
      "learning_rate": 0.00017460317460317462,
      "loss": 0.3228,
      "step": 69
    },
    {
      "epoch": 1.094488188976378,
      "grad_norm": 1.9451435804367065,
      "learning_rate": 0.0001742063492063492,
      "loss": 0.3823,
      "step": 70
    },
    {
      "epoch": 1.110236220472441,
      "grad_norm": 1.2850340604782104,
      "learning_rate": 0.00017380952380952383,
      "loss": 0.2941,
      "step": 71
    },
    {
      "epoch": 1.125984251968504,
      "grad_norm": 1.9033786058425903,
      "learning_rate": 0.00017341269841269843,
      "loss": 0.3596,
      "step": 72
    },
    {
      "epoch": 1.141732283464567,
      "grad_norm": 1.6210358142852783,
      "learning_rate": 0.00017301587301587302,
      "loss": 0.2749,
      "step": 73
    },
    {
      "epoch": 1.1574803149606299,
      "grad_norm": 1.5955774784088135,
      "learning_rate": 0.00017261904761904764,
      "loss": 0.3732,
      "step": 74
    },
    {
      "epoch": 1.1732283464566928,
      "grad_norm": 1.7886656522750854,
      "learning_rate": 0.00017222222222222224,
      "loss": 0.306,
      "step": 75
    },
    {
      "epoch": 1.188976377952756,
      "grad_norm": 2.3294215202331543,
      "learning_rate": 0.00017182539682539683,
      "loss": 0.3966,
      "step": 76
    },
    {
      "epoch": 1.204724409448819,
      "grad_norm": 2.0741255283355713,
      "learning_rate": 0.00017142857142857143,
      "loss": 0.3874,
      "step": 77
    },
    {
      "epoch": 1.220472440944882,
      "grad_norm": 1.5881668329238892,
      "learning_rate": 0.00017103174603174602,
      "loss": 0.3259,
      "step": 78
    },
    {
      "epoch": 1.236220472440945,
      "grad_norm": 1.627063512802124,
      "learning_rate": 0.00017063492063492064,
      "loss": 0.3039,
      "step": 79
    },
    {
      "epoch": 1.2519685039370079,
      "grad_norm": 1.6420118808746338,
      "learning_rate": 0.00017023809523809523,
      "loss": 0.271,
      "step": 80
    },
    {
      "epoch": 1.2677165354330708,
      "grad_norm": 1.6271945238113403,
      "learning_rate": 0.00016984126984126986,
      "loss": 0.2817,
      "step": 81
    },
    {
      "epoch": 1.2834645669291338,
      "grad_norm": 3.3429927825927734,
      "learning_rate": 0.00016944444444444445,
      "loss": 0.4534,
      "step": 82
    },
    {
      "epoch": 1.2992125984251968,
      "grad_norm": 2.314985752105713,
      "learning_rate": 0.00016904761904761904,
      "loss": 0.4114,
      "step": 83
    },
    {
      "epoch": 1.3149606299212597,
      "grad_norm": 1.9130069017410278,
      "learning_rate": 0.00016865079365079366,
      "loss": 0.3769,
      "step": 84
    },
    {
      "epoch": 1.330708661417323,
      "grad_norm": 1.5841271877288818,
      "learning_rate": 0.00016825396825396826,
      "loss": 0.3492,
      "step": 85
    },
    {
      "epoch": 1.3464566929133859,
      "grad_norm": 3.537999391555786,
      "learning_rate": 0.00016785714285714288,
      "loss": 0.3494,
      "step": 86
    },
    {
      "epoch": 1.3622047244094488,
      "grad_norm": 1.966240644454956,
      "learning_rate": 0.00016746031746031747,
      "loss": 0.3536,
      "step": 87
    },
    {
      "epoch": 1.3779527559055118,
      "grad_norm": 2.4739272594451904,
      "learning_rate": 0.00016706349206349207,
      "loss": 0.37,
      "step": 88
    },
    {
      "epoch": 1.3937007874015748,
      "grad_norm": 1.8670167922973633,
      "learning_rate": 0.0001666666666666667,
      "loss": 0.229,
      "step": 89
    },
    {
      "epoch": 1.4094488188976377,
      "grad_norm": 1.6482937335968018,
      "learning_rate": 0.00016626984126984128,
      "loss": 0.3092,
      "step": 90
    },
    {
      "epoch": 1.425196850393701,
      "grad_norm": 1.4660123586654663,
      "learning_rate": 0.0001658730158730159,
      "loss": 0.2359,
      "step": 91
    },
    {
      "epoch": 1.4409448818897639,
      "grad_norm": 1.5922447443008423,
      "learning_rate": 0.00016547619047619047,
      "loss": 0.3062,
      "step": 92
    },
    {
      "epoch": 1.4566929133858268,
      "grad_norm": 1.678743839263916,
      "learning_rate": 0.0001650793650793651,
      "loss": 0.3348,
      "step": 93
    },
    {
      "epoch": 1.4724409448818898,
      "grad_norm": 1.4790122509002686,
      "learning_rate": 0.00016468253968253969,
      "loss": 0.2633,
      "step": 94
    },
    {
      "epoch": 1.4881889763779528,
      "grad_norm": 1.41916024684906,
      "learning_rate": 0.00016428571428571428,
      "loss": 0.2302,
      "step": 95
    },
    {
      "epoch": 1.5039370078740157,
      "grad_norm": 1.3545938730239868,
      "learning_rate": 0.0001638888888888889,
      "loss": 0.2762,
      "step": 96
    },
    {
      "epoch": 1.5196850393700787,
      "grad_norm": 1.7564231157302856,
      "learning_rate": 0.0001634920634920635,
      "loss": 0.3226,
      "step": 97
    },
    {
      "epoch": 1.5354330708661417,
      "grad_norm": 2.1029298305511475,
      "learning_rate": 0.0001630952380952381,
      "loss": 0.3263,
      "step": 98
    },
    {
      "epoch": 1.5511811023622046,
      "grad_norm": 2.365849494934082,
      "learning_rate": 0.0001626984126984127,
      "loss": 0.3796,
      "step": 99
    },
    {
      "epoch": 1.5669291338582676,
      "grad_norm": 1.975176453590393,
      "learning_rate": 0.0001623015873015873,
      "loss": 0.4038,
      "step": 100
    },
    {
      "epoch": 1.5826771653543306,
      "grad_norm": 1.741062045097351,
      "learning_rate": 0.00016190476190476192,
      "loss": 0.3356,
      "step": 101
    },
    {
      "epoch": 1.5984251968503937,
      "grad_norm": 1.7319364547729492,
      "learning_rate": 0.00016150793650793652,
      "loss": 0.2883,
      "step": 102
    },
    {
      "epoch": 1.6141732283464567,
      "grad_norm": 1.5000462532043457,
      "learning_rate": 0.0001611111111111111,
      "loss": 0.3319,
      "step": 103
    },
    {
      "epoch": 1.6299212598425197,
      "grad_norm": 2.3536272048950195,
      "learning_rate": 0.00016071428571428573,
      "loss": 0.4528,
      "step": 104
    },
    {
      "epoch": 1.6456692913385826,
      "grad_norm": 2.1542575359344482,
      "learning_rate": 0.00016031746031746033,
      "loss": 0.2734,
      "step": 105
    },
    {
      "epoch": 1.6614173228346458,
      "grad_norm": 1.6822230815887451,
      "learning_rate": 0.00015992063492063495,
      "loss": 0.235,
      "step": 106
    },
    {
      "epoch": 1.6771653543307088,
      "grad_norm": 2.083859920501709,
      "learning_rate": 0.00015952380952380954,
      "loss": 0.4121,
      "step": 107
    },
    {
      "epoch": 1.6929133858267718,
      "grad_norm": 1.7096972465515137,
      "learning_rate": 0.00015912698412698414,
      "loss": 0.3514,
      "step": 108
    },
    {
      "epoch": 1.7086614173228347,
      "grad_norm": 1.9930963516235352,
      "learning_rate": 0.00015873015873015873,
      "loss": 0.3313,
      "step": 109
    },
    {
      "epoch": 1.7244094488188977,
      "grad_norm": 1.475851058959961,
      "learning_rate": 0.00015833333333333332,
      "loss": 0.2444,
      "step": 110
    },
    {
      "epoch": 1.7401574803149606,
      "grad_norm": 1.4857220649719238,
      "learning_rate": 0.00015793650793650795,
      "loss": 0.3113,
      "step": 111
    },
    {
      "epoch": 1.7559055118110236,
      "grad_norm": 2.817281484603882,
      "learning_rate": 0.00015753968253968254,
      "loss": 0.4561,
      "step": 112
    },
    {
      "epoch": 1.7716535433070866,
      "grad_norm": 2.060410499572754,
      "learning_rate": 0.00015714285714285716,
      "loss": 0.303,
      "step": 113
    },
    {
      "epoch": 1.7874015748031495,
      "grad_norm": 2.097001552581787,
      "learning_rate": 0.00015674603174603175,
      "loss": 0.356,
      "step": 114
    },
    {
      "epoch": 1.8031496062992125,
      "grad_norm": 1.4525930881500244,
      "learning_rate": 0.00015634920634920635,
      "loss": 0.2303,
      "step": 115
    },
    {
      "epoch": 1.8188976377952755,
      "grad_norm": 2.1399197578430176,
      "learning_rate": 0.00015595238095238097,
      "loss": 0.3717,
      "step": 116
    },
    {
      "epoch": 1.8346456692913384,
      "grad_norm": 2.0702733993530273,
      "learning_rate": 0.00015555555555555556,
      "loss": 0.2632,
      "step": 117
    },
    {
      "epoch": 1.8503937007874016,
      "grad_norm": 2.021906614303589,
      "learning_rate": 0.00015515873015873016,
      "loss": 0.3554,
      "step": 118
    },
    {
      "epoch": 1.8661417322834646,
      "grad_norm": 1.534460186958313,
      "learning_rate": 0.00015476190476190478,
      "loss": 0.2896,
      "step": 119
    },
    {
      "epoch": 1.8818897637795275,
      "grad_norm": 1.279995083808899,
      "learning_rate": 0.00015436507936507937,
      "loss": 0.2268,
      "step": 120
    },
    {
      "epoch": 1.8976377952755905,
      "grad_norm": 1.5187493562698364,
      "learning_rate": 0.000153968253968254,
      "loss": 0.2386,
      "step": 121
    },
    {
      "epoch": 1.9133858267716537,
      "grad_norm": 1.8985463380813599,
      "learning_rate": 0.0001535714285714286,
      "loss": 0.2935,
      "step": 122
    },
    {
      "epoch": 1.9291338582677167,
      "grad_norm": 2.359837055206299,
      "learning_rate": 0.00015317460317460318,
      "loss": 0.3279,
      "step": 123
    },
    {
      "epoch": 1.9448818897637796,
      "grad_norm": 2.06973934173584,
      "learning_rate": 0.00015277777777777777,
      "loss": 0.3422,
      "step": 124
    },
    {
      "epoch": 1.9606299212598426,
      "grad_norm": 1.9678869247436523,
      "learning_rate": 0.00015238095238095237,
      "loss": 0.2959,
      "step": 125
    },
    {
      "epoch": 1.9763779527559056,
      "grad_norm": 1.7255127429962158,
      "learning_rate": 0.000151984126984127,
      "loss": 0.282,
      "step": 126
    },
    {
      "epoch": 1.9921259842519685,
      "grad_norm": 2.030247926712036,
      "learning_rate": 0.00015158730158730158,
      "loss": 0.253,
      "step": 127
    },
    {
      "epoch": 2.0,
      "grad_norm": 1.5754952430725098,
      "learning_rate": 0.0001511904761904762,
      "loss": 0.1987,
      "step": 128
    },
    {
      "epoch": 2.0,
      "eval_loss": 0.32184889912605286,
      "eval_runtime": 21.0287,
      "eval_samples_per_second": 3.043,
      "eval_steps_per_second": 0.38,
      "step": 128
    },
    {
      "epoch": 2.015748031496063,
      "grad_norm": 1.6904338598251343,
      "learning_rate": 0.0001507936507936508,
      "loss": 0.311,
      "step": 129
    },
    {
      "epoch": 2.031496062992126,
      "grad_norm": 1.2484811544418335,
      "learning_rate": 0.0001503968253968254,
      "loss": 0.1681,
      "step": 130
    },
    {
      "epoch": 2.047244094488189,
      "grad_norm": 1.4905502796173096,
      "learning_rate": 0.00015000000000000001,
      "loss": 0.2364,
      "step": 131
    },
    {
      "epoch": 2.062992125984252,
      "grad_norm": 1.7125612497329712,
      "learning_rate": 0.0001496031746031746,
      "loss": 0.2751,
      "step": 132
    },
    {
      "epoch": 2.078740157480315,
      "grad_norm": 2.116318702697754,
      "learning_rate": 0.00014920634920634923,
      "loss": 0.3425,
      "step": 133
    },
    {
      "epoch": 2.094488188976378,
      "grad_norm": 1.7455250024795532,
      "learning_rate": 0.00014880952380952382,
      "loss": 0.3455,
      "step": 134
    },
    {
      "epoch": 2.1102362204724407,
      "grad_norm": 2.2866604328155518,
      "learning_rate": 0.00014841269841269842,
      "loss": 0.2881,
      "step": 135
    },
    {
      "epoch": 2.1259842519685037,
      "grad_norm": 1.6871932744979858,
      "learning_rate": 0.00014801587301587304,
      "loss": 0.2161,
      "step": 136
    },
    {
      "epoch": 2.141732283464567,
      "grad_norm": 1.865064263343811,
      "learning_rate": 0.00014761904761904763,
      "loss": 0.2385,
      "step": 137
    },
    {
      "epoch": 2.15748031496063,
      "grad_norm": 1.5797851085662842,
      "learning_rate": 0.00014722222222222223,
      "loss": 0.2013,
      "step": 138
    },
    {
      "epoch": 2.173228346456693,
      "grad_norm": 1.9740902185440063,
      "learning_rate": 0.00014682539682539682,
      "loss": 0.2699,
      "step": 139
    },
    {
      "epoch": 2.188976377952756,
      "grad_norm": 1.9616936445236206,
      "learning_rate": 0.00014642857142857141,
      "loss": 0.2085,
      "step": 140
    },
    {
      "epoch": 2.204724409448819,
      "grad_norm": 2.622150421142578,
      "learning_rate": 0.00014603174603174603,
      "loss": 0.3344,
      "step": 141
    },
    {
      "epoch": 2.220472440944882,
      "grad_norm": 2.6817970275878906,
      "learning_rate": 0.00014563492063492063,
      "loss": 0.2478,
      "step": 142
    },
    {
      "epoch": 2.236220472440945,
      "grad_norm": 1.7167726755142212,
      "learning_rate": 0.00014523809523809525,
      "loss": 0.1861,
      "step": 143
    },
    {
      "epoch": 2.251968503937008,
      "grad_norm": 1.7041146755218506,
      "learning_rate": 0.00014484126984126984,
      "loss": 0.176,
      "step": 144
    },
    {
      "epoch": 2.267716535433071,
      "grad_norm": 2.54337477684021,
      "learning_rate": 0.00014444444444444444,
      "loss": 0.3154,
      "step": 145
    },
    {
      "epoch": 2.283464566929134,
      "grad_norm": 2.2240216732025146,
      "learning_rate": 0.00014404761904761906,
      "loss": 0.225,
      "step": 146
    },
    {
      "epoch": 2.2992125984251968,
      "grad_norm": 2.4545581340789795,
      "learning_rate": 0.00014365079365079365,
      "loss": 0.2548,
      "step": 147
    },
    {
      "epoch": 2.3149606299212597,
      "grad_norm": 2.1531567573547363,
      "learning_rate": 0.00014325396825396827,
      "loss": 0.2071,
      "step": 148
    },
    {
      "epoch": 2.3307086614173227,
      "grad_norm": 2.2477309703826904,
      "learning_rate": 0.00014285714285714287,
      "loss": 0.2817,
      "step": 149
    },
    {
      "epoch": 2.3464566929133857,
      "grad_norm": 1.8494067192077637,
      "learning_rate": 0.00014246031746031746,
      "loss": 0.2202,
      "step": 150
    },
    {
      "epoch": 2.362204724409449,
      "grad_norm": 2.6232306957244873,
      "learning_rate": 0.00014206349206349208,
      "loss": 0.3045,
      "step": 151
    },
    {
      "epoch": 2.377952755905512,
      "grad_norm": 1.8471623659133911,
      "learning_rate": 0.00014166666666666668,
      "loss": 0.2116,
      "step": 152
    },
    {
      "epoch": 2.393700787401575,
      "grad_norm": 1.6874144077301025,
      "learning_rate": 0.0001412698412698413,
      "loss": 0.1578,
      "step": 153
    },
    {
      "epoch": 2.409448818897638,
      "grad_norm": 2.5560171604156494,
      "learning_rate": 0.0001408730158730159,
      "loss": 0.3363,
      "step": 154
    },
    {
      "epoch": 2.425196850393701,
      "grad_norm": 2.6381895542144775,
      "learning_rate": 0.00014047619047619049,
      "loss": 0.2668,
      "step": 155
    },
    {
      "epoch": 2.440944881889764,
      "grad_norm": 1.9336203336715698,
      "learning_rate": 0.00014007936507936508,
      "loss": 0.2418,
      "step": 156
    },
    {
      "epoch": 2.456692913385827,
      "grad_norm": 1.9924594163894653,
      "learning_rate": 0.00013968253968253967,
      "loss": 0.1823,
      "step": 157
    },
    {
      "epoch": 2.47244094488189,
      "grad_norm": 4.717596054077148,
      "learning_rate": 0.0001392857142857143,
      "loss": 0.318,
      "step": 158
    },
    {
      "epoch": 2.4881889763779528,
      "grad_norm": 2.840956449508667,
      "learning_rate": 0.0001388888888888889,
      "loss": 0.3815,
      "step": 159
    },
    {
      "epoch": 2.5039370078740157,
      "grad_norm": 1.780990719795227,
      "learning_rate": 0.00013849206349206348,
      "loss": 0.2917,
      "step": 160
    },
    {
      "epoch": 2.5196850393700787,
      "grad_norm": 2.117501974105835,
      "learning_rate": 0.0001380952380952381,
      "loss": 0.2642,
      "step": 161
    },
    {
      "epoch": 2.5354330708661417,
      "grad_norm": 1.7382467985153198,
      "learning_rate": 0.0001376984126984127,
      "loss": 0.1805,
      "step": 162
    },
    {
      "epoch": 2.5511811023622046,
      "grad_norm": 1.8112269639968872,
      "learning_rate": 0.00013730158730158732,
      "loss": 0.2708,
      "step": 163
    },
    {
      "epoch": 2.5669291338582676,
      "grad_norm": 1.4702037572860718,
      "learning_rate": 0.0001369047619047619,
      "loss": 0.1683,
      "step": 164
    },
    {
      "epoch": 2.5826771653543306,
      "grad_norm": 1.8469096422195435,
      "learning_rate": 0.0001365079365079365,
      "loss": 0.258,
      "step": 165
    },
    {
      "epoch": 2.5984251968503935,
      "grad_norm": 2.1171681880950928,
      "learning_rate": 0.00013611111111111113,
      "loss": 0.2444,
      "step": 166
    },
    {
      "epoch": 2.6141732283464565,
      "grad_norm": 1.9175876379013062,
      "learning_rate": 0.00013571428571428572,
      "loss": 0.2148,
      "step": 167
    },
    {
      "epoch": 2.6299212598425195,
      "grad_norm": 2.8274314403533936,
      "learning_rate": 0.00013531746031746034,
      "loss": 0.2815,
      "step": 168
    },
    {
      "epoch": 2.6456692913385824,
      "grad_norm": 2.059065580368042,
      "learning_rate": 0.00013492063492063494,
      "loss": 0.1853,
      "step": 169
    },
    {
      "epoch": 2.661417322834646,
      "grad_norm": 2.260807514190674,
      "learning_rate": 0.00013452380952380953,
      "loss": 0.3196,
      "step": 170
    },
    {
      "epoch": 2.677165354330709,
      "grad_norm": 2.634159564971924,
      "learning_rate": 0.00013412698412698412,
      "loss": 0.3509,
      "step": 171
    },
    {
      "epoch": 2.6929133858267718,
      "grad_norm": 1.7028346061706543,
      "learning_rate": 0.00013373015873015872,
      "loss": 0.1954,
      "step": 172
    },
    {
      "epoch": 2.7086614173228347,
      "grad_norm": 2.0830795764923096,
      "learning_rate": 0.00013333333333333334,
      "loss": 0.2404,
      "step": 173
    },
    {
      "epoch": 2.7244094488188977,
      "grad_norm": 2.1290485858917236,
      "learning_rate": 0.00013293650793650793,
      "loss": 0.21,
      "step": 174
    },
    {
      "epoch": 2.7401574803149606,
      "grad_norm": 2.376554489135742,
      "learning_rate": 0.00013253968253968255,
      "loss": 0.215,
      "step": 175
    },
    {
      "epoch": 2.7559055118110236,
      "grad_norm": 2.0598766803741455,
      "learning_rate": 0.00013214285714285715,
      "loss": 0.2751,
      "step": 176
    },
    {
      "epoch": 2.7716535433070866,
      "grad_norm": 3.034533977508545,
      "learning_rate": 0.00013174603174603174,
      "loss": 0.3034,
      "step": 177
    },
    {
      "epoch": 2.7874015748031495,
      "grad_norm": 2.8083488941192627,
      "learning_rate": 0.00013134920634920636,
      "loss": 0.3729,
      "step": 178
    },
    {
      "epoch": 2.8031496062992125,
      "grad_norm": 2.4944064617156982,
      "learning_rate": 0.00013095238095238096,
      "loss": 0.3075,
      "step": 179
    },
    {
      "epoch": 2.8188976377952755,
      "grad_norm": 1.7174040079116821,
      "learning_rate": 0.00013055555555555555,
      "loss": 0.1774,
      "step": 180
    },
    {
      "epoch": 2.8346456692913384,
      "grad_norm": 1.5711172819137573,
      "learning_rate": 0.00013015873015873017,
      "loss": 0.1455,
      "step": 181
    },
    {
      "epoch": 2.850393700787402,
      "grad_norm": 2.3123834133148193,
      "learning_rate": 0.00012976190476190477,
      "loss": 0.2029,
      "step": 182
    },
    {
      "epoch": 2.866141732283465,
      "grad_norm": 2.3587799072265625,
      "learning_rate": 0.0001293650793650794,
      "loss": 0.2618,
      "step": 183
    },
    {
      "epoch": 2.8818897637795278,
      "grad_norm": 2.144653081893921,
      "learning_rate": 0.00012896825396825398,
      "loss": 0.2831,
      "step": 184
    },
    {
      "epoch": 2.8976377952755907,
      "grad_norm": 1.7953280210494995,
      "learning_rate": 0.00012857142857142858,
      "loss": 0.1406,
      "step": 185
    },
    {
      "epoch": 2.9133858267716537,
      "grad_norm": 2.6263248920440674,
      "learning_rate": 0.0001281746031746032,
      "loss": 0.1725,
      "step": 186
    },
    {
      "epoch": 2.9291338582677167,
      "grad_norm": 1.9911167621612549,
      "learning_rate": 0.00012777777777777776,
      "loss": 0.2215,
      "step": 187
    },
    {
      "epoch": 2.9448818897637796,
      "grad_norm": 2.2523036003112793,
      "learning_rate": 0.00012738095238095238,
      "loss": 0.2203,
      "step": 188
    },
    {
      "epoch": 2.9606299212598426,
      "grad_norm": 2.489804983139038,
      "learning_rate": 0.00012698412698412698,
      "loss": 0.2603,
      "step": 189
    },
    {
      "epoch": 2.9763779527559056,
      "grad_norm": 2.4655003547668457,
      "learning_rate": 0.0001265873015873016,
      "loss": 0.2088,
      "step": 190
    },
    {
      "epoch": 2.9921259842519685,
      "grad_norm": 2.276545524597168,
      "learning_rate": 0.0001261904761904762,
      "loss": 0.2829,
      "step": 191
    },
    {
      "epoch": 3.0,
      "grad_norm": 2.3196136951446533,
      "learning_rate": 0.0001257936507936508,
      "loss": 0.2107,
      "step": 192
    },
    {
      "epoch": 3.0,
      "eval_loss": 0.28974366188049316,
      "eval_runtime": 21.1155,
      "eval_samples_per_second": 3.031,
      "eval_steps_per_second": 0.379,
      "step": 192
    },
    {
      "epoch": 3.015748031496063,
      "grad_norm": 2.2644145488739014,
      "learning_rate": 0.0001253968253968254,
      "loss": 0.1996,
      "step": 193
    },
    {
      "epoch": 3.031496062992126,
      "grad_norm": 2.13639497756958,
      "learning_rate": 0.000125,
      "loss": 0.2401,
      "step": 194
    },
    {
      "epoch": 3.047244094488189,
      "grad_norm": 2.282648801803589,
      "learning_rate": 0.00012460317460317462,
      "loss": 0.2665,
      "step": 195
    },
    {
      "epoch": 3.062992125984252,
      "grad_norm": 2.172711133956909,
      "learning_rate": 0.00012420634920634922,
      "loss": 0.2511,
      "step": 196
    },
    {
      "epoch": 3.078740157480315,
      "grad_norm": 1.336927890777588,
      "learning_rate": 0.0001238095238095238,
      "loss": 0.1263,
      "step": 197
    },
    {
      "epoch": 3.094488188976378,
      "grad_norm": 2.339054584503174,
      "learning_rate": 0.00012341269841269843,
      "loss": 0.275,
      "step": 198
    },
    {
      "epoch": 3.1102362204724407,
      "grad_norm": 2.382046699523926,
      "learning_rate": 0.00012301587301587303,
      "loss": 0.2192,
      "step": 199
    },
    {
      "epoch": 3.1259842519685037,
      "grad_norm": 2.8365464210510254,
      "learning_rate": 0.00012261904761904762,
      "loss": 0.3453,
      "step": 200
    },
    {
      "epoch": 3.141732283464567,
      "grad_norm": 2.8860130310058594,
      "learning_rate": 0.00012222222222222224,
      "loss": 0.1774,
      "step": 201
    },
    {
      "epoch": 3.15748031496063,
      "grad_norm": 1.8316560983657837,
      "learning_rate": 0.00012182539682539682,
      "loss": 0.1423,
      "step": 202
    },
    {
      "epoch": 3.173228346456693,
      "grad_norm": 1.9884192943572998,
      "learning_rate": 0.00012142857142857143,
      "loss": 0.1275,
      "step": 203
    },
    {
      "epoch": 3.188976377952756,
      "grad_norm": 2.889086961746216,
      "learning_rate": 0.00012103174603174602,
      "loss": 0.2495,
      "step": 204
    },
    {
      "epoch": 3.204724409448819,
      "grad_norm": 2.1737070083618164,
      "learning_rate": 0.00012063492063492063,
      "loss": 0.1041,
      "step": 205
    },
    {
      "epoch": 3.220472440944882,
      "grad_norm": 4.075921058654785,
      "learning_rate": 0.00012023809523809524,
      "loss": 0.2535,
      "step": 206
    },
    {
      "epoch": 3.236220472440945,
      "grad_norm": 3.0082035064697266,
      "learning_rate": 0.00011984126984126985,
      "loss": 0.2407,
      "step": 207
    },
    {
      "epoch": 3.251968503937008,
      "grad_norm": 2.4665191173553467,
      "learning_rate": 0.00011944444444444445,
      "loss": 0.1674,
      "step": 208
    },
    {
      "epoch": 3.267716535433071,
      "grad_norm": 2.6073338985443115,
      "learning_rate": 0.00011904761904761905,
      "loss": 0.1468,
      "step": 209
    },
    {
      "epoch": 3.283464566929134,
      "grad_norm": 1.8876603841781616,
      "learning_rate": 0.00011865079365079366,
      "loss": 0.1249,
      "step": 210
    },
    {
      "epoch": 3.2992125984251968,
      "grad_norm": 2.0223097801208496,
      "learning_rate": 0.00011825396825396826,
      "loss": 0.1341,
      "step": 211
    },
    {
      "epoch": 3.3149606299212597,
      "grad_norm": 2.3129518032073975,
      "learning_rate": 0.00011785714285714287,
      "loss": 0.1045,
      "step": 212
    },
    {
      "epoch": 3.3307086614173227,
      "grad_norm": 2.0858242511749268,
      "learning_rate": 0.00011746031746031746,
      "loss": 0.1468,
      "step": 213
    },
    {
      "epoch": 3.3464566929133857,
      "grad_norm": 2.2993431091308594,
      "learning_rate": 0.00011706349206349207,
      "loss": 0.1563,
      "step": 214
    },
    {
      "epoch": 3.362204724409449,
      "grad_norm": 2.503553867340088,
      "learning_rate": 0.00011666666666666668,
      "loss": 0.1794,
      "step": 215
    },
    {
      "epoch": 3.377952755905512,
      "grad_norm": 2.070436477661133,
      "learning_rate": 0.00011626984126984129,
      "loss": 0.1098,
      "step": 216
    },
    {
      "epoch": 3.393700787401575,
      "grad_norm": 2.8557088375091553,
      "learning_rate": 0.0001158730158730159,
      "loss": 0.1667,
      "step": 217
    },
    {
      "epoch": 3.409448818897638,
      "grad_norm": 2.861720085144043,
      "learning_rate": 0.00011547619047619047,
      "loss": 0.1878,
      "step": 218
    },
    {
      "epoch": 3.425196850393701,
      "grad_norm": 2.221318244934082,
      "learning_rate": 0.00011507936507936508,
      "loss": 0.1298,
      "step": 219
    },
    {
      "epoch": 3.440944881889764,
      "grad_norm": 2.158196210861206,
      "learning_rate": 0.00011468253968253968,
      "loss": 0.1212,
      "step": 220
    },
    {
      "epoch": 3.456692913385827,
      "grad_norm": 2.5167784690856934,
      "learning_rate": 0.00011428571428571428,
      "loss": 0.1311,
      "step": 221
    },
    {
      "epoch": 3.47244094488189,
      "grad_norm": 2.146698474884033,
      "learning_rate": 0.00011388888888888889,
      "loss": 0.1361,
      "step": 222
    },
    {
      "epoch": 3.4881889763779528,
      "grad_norm": 5.640661239624023,
      "learning_rate": 0.0001134920634920635,
      "loss": 0.1689,
      "step": 223
    },
    {
      "epoch": 3.5039370078740157,
      "grad_norm": 2.381878614425659,
      "learning_rate": 0.00011309523809523809,
      "loss": 0.1453,
      "step": 224
    },
    {
      "epoch": 3.5196850393700787,
      "grad_norm": 2.1898605823516846,
      "learning_rate": 0.0001126984126984127,
      "loss": 0.1307,
      "step": 225
    },
    {
      "epoch": 3.5354330708661417,
      "grad_norm": 2.9843311309814453,
      "learning_rate": 0.00011230158730158731,
      "loss": 0.1724,
      "step": 226
    },
    {
      "epoch": 3.5511811023622046,
      "grad_norm": 2.413039207458496,
      "learning_rate": 0.00011190476190476191,
      "loss": 0.1686,
      "step": 227
    },
    {
      "epoch": 3.5669291338582676,
      "grad_norm": 2.7569971084594727,
      "learning_rate": 0.00011150793650793652,
      "loss": 0.1271,
      "step": 228
    },
    {
      "epoch": 3.5826771653543306,
      "grad_norm": 1.9328231811523438,
      "learning_rate": 0.00011111111111111112,
      "loss": 0.0809,
      "step": 229
    },
    {
      "epoch": 3.5984251968503935,
      "grad_norm": 3.853731393814087,
      "learning_rate": 0.00011071428571428572,
      "loss": 0.1696,
      "step": 230
    },
    {
      "epoch": 3.6141732283464565,
      "grad_norm": 1.8131320476531982,
      "learning_rate": 0.00011031746031746033,
      "loss": 0.0759,
      "step": 231
    },
    {
      "epoch": 3.6299212598425195,
      "grad_norm": 2.6501708030700684,
      "learning_rate": 0.00010992063492063494,
      "loss": 0.1464,
      "step": 232
    },
    {
      "epoch": 3.6456692913385824,
      "grad_norm": 2.7041311264038086,
      "learning_rate": 0.00010952380952380953,
      "loss": 0.1706,
      "step": 233
    },
    {
      "epoch": 3.661417322834646,
      "grad_norm": 3.1073830127716064,
      "learning_rate": 0.00010912698412698413,
      "loss": 0.1787,
      "step": 234
    },
    {
      "epoch": 3.677165354330709,
      "grad_norm": 5.48389196395874,
      "learning_rate": 0.00010873015873015872,
      "loss": 0.204,
      "step": 235
    },
    {
      "epoch": 3.6929133858267718,
      "grad_norm": 1.990247368812561,
      "learning_rate": 0.00010833333333333333,
      "loss": 0.1356,
      "step": 236
    },
    {
      "epoch": 3.7086614173228347,
      "grad_norm": 2.2346434593200684,
      "learning_rate": 0.00010793650793650794,
      "loss": 0.1739,
      "step": 237
    },
    {
      "epoch": 3.7244094488188977,
      "grad_norm": 2.046290874481201,
      "learning_rate": 0.00010753968253968254,
      "loss": 0.0779,
      "step": 238
    },
    {
      "epoch": 3.7401574803149606,
      "grad_norm": 2.217334270477295,
      "learning_rate": 0.00010714285714285715,
      "loss": 0.161,
      "step": 239
    },
    {
      "epoch": 3.7559055118110236,
      "grad_norm": 2.3918778896331787,
      "learning_rate": 0.00010674603174603174,
      "loss": 0.1284,
      "step": 240
    },
    {
      "epoch": 3.7716535433070866,
      "grad_norm": 2.33229923248291,
      "learning_rate": 0.00010634920634920635,
      "loss": 0.133,
      "step": 241
    },
    {
      "epoch": 3.7874015748031495,
      "grad_norm": 3.737699031829834,
      "learning_rate": 0.00010595238095238096,
      "loss": 0.1469,
      "step": 242
    },
    {
      "epoch": 3.8031496062992125,
      "grad_norm": 3.3132073879241943,
      "learning_rate": 0.00010555555555555557,
      "loss": 0.1968,
      "step": 243
    },
    {
      "epoch": 3.8188976377952755,
      "grad_norm": 2.7442851066589355,
      "learning_rate": 0.00010515873015873016,
      "loss": 0.2302,
      "step": 244
    },
    {
      "epoch": 3.8346456692913384,
      "grad_norm": 2.5042812824249268,
      "learning_rate": 0.00010476190476190477,
      "loss": 0.1921,
      "step": 245
    },
    {
      "epoch": 3.850393700787402,
      "grad_norm": 3.066495656967163,
      "learning_rate": 0.00010436507936507938,
      "loss": 0.2252,
      "step": 246
    },
    {
      "epoch": 3.866141732283465,
      "grad_norm": 2.601206064224243,
      "learning_rate": 0.00010396825396825398,
      "loss": 0.1628,
      "step": 247
    },
    {
      "epoch": 3.8818897637795278,
      "grad_norm": 3.073747396469116,
      "learning_rate": 0.00010357142857142859,
      "loss": 0.2506,
      "step": 248
    },
    {
      "epoch": 3.8976377952755907,
      "grad_norm": 2.54162335395813,
      "learning_rate": 0.00010317460317460319,
      "loss": 0.133,
      "step": 249
    },
    {
      "epoch": 3.9133858267716537,
      "grad_norm": 2.785240888595581,
      "learning_rate": 0.00010277777777777778,
      "loss": 0.1562,
      "step": 250
    },
    {
      "epoch": 3.9291338582677167,
      "grad_norm": 2.340440511703491,
      "learning_rate": 0.00010238095238095237,
      "loss": 0.1506,
      "step": 251
    },
    {
      "epoch": 3.9448818897637796,
      "grad_norm": 2.8922476768493652,
      "learning_rate": 0.00010198412698412698,
      "loss": 0.1929,
      "step": 252
    },
    {
      "epoch": 3.9606299212598426,
      "grad_norm": 1.5728784799575806,
      "learning_rate": 0.00010158730158730159,
      "loss": 0.055,
      "step": 253
    },
    {
      "epoch": 3.9763779527559056,
      "grad_norm": 3.043267011642456,
      "learning_rate": 0.0001011904761904762,
      "loss": 0.2163,
      "step": 254
    },
    {
      "epoch": 3.9921259842519685,
      "grad_norm": 2.613538980484009,
      "learning_rate": 0.00010079365079365079,
      "loss": 0.1342,
      "step": 255
    },
    {
      "epoch": 4.0,
      "grad_norm": 0.9985538125038147,
      "learning_rate": 0.0001003968253968254,
      "loss": 0.0448,
      "step": 256
    },
    {
      "epoch": 4.0,
      "eval_loss": 0.23399639129638672,
      "eval_runtime": 23.2479,
      "eval_samples_per_second": 2.753,
      "eval_steps_per_second": 0.344,
      "step": 256
    },
    {
      "epoch": 4.015748031496063,
      "grad_norm": 1.8375444412231445,
      "learning_rate": 0.0001,
      "loss": 0.0956,
      "step": 257
    },
    {
      "epoch": 4.031496062992126,
      "grad_norm": 1.7975623607635498,
      "learning_rate": 9.960317460317461e-05,
      "loss": 0.1089,
      "step": 258
    },
    {
      "epoch": 4.047244094488189,
      "grad_norm": 1.8304307460784912,
      "learning_rate": 9.920634920634922e-05,
      "loss": 0.0848,
      "step": 259
    },
    {
      "epoch": 4.062992125984252,
      "grad_norm": 2.1605825424194336,
      "learning_rate": 9.880952380952381e-05,
      "loss": 0.0958,
      "step": 260
    },
    {
      "epoch": 4.078740157480315,
      "grad_norm": 3.019425868988037,
      "learning_rate": 9.841269841269841e-05,
      "loss": 0.152,
      "step": 261
    },
    {
      "epoch": 4.094488188976378,
      "grad_norm": 2.8490195274353027,
      "learning_rate": 9.801587301587302e-05,
      "loss": 0.1884,
      "step": 262
    },
    {
      "epoch": 4.110236220472441,
      "grad_norm": 2.1349971294403076,
      "learning_rate": 9.761904761904762e-05,
      "loss": 0.0962,
      "step": 263
    },
    {
      "epoch": 4.125984251968504,
      "grad_norm": 1.6372445821762085,
      "learning_rate": 9.722222222222223e-05,
      "loss": 0.061,
      "step": 264
    },
    {
      "epoch": 4.141732283464567,
      "grad_norm": 2.5463123321533203,
      "learning_rate": 9.682539682539682e-05,
      "loss": 0.1207,
      "step": 265
    },
    {
      "epoch": 4.15748031496063,
      "grad_norm": 1.6401474475860596,
      "learning_rate": 9.642857142857143e-05,
      "loss": 0.0592,
      "step": 266
    },
    {
      "epoch": 4.173228346456693,
      "grad_norm": 2.264448881149292,
      "learning_rate": 9.603174603174604e-05,
      "loss": 0.143,
      "step": 267
    },
    {
      "epoch": 4.188976377952756,
      "grad_norm": 2.6619653701782227,
      "learning_rate": 9.563492063492065e-05,
      "loss": 0.095,
      "step": 268
    },
    {
      "epoch": 4.2047244094488185,
      "grad_norm": 2.8636703491210938,
      "learning_rate": 9.523809523809524e-05,
      "loss": 0.127,
      "step": 269
    },
    {
      "epoch": 4.2204724409448815,
      "grad_norm": 3.675490140914917,
      "learning_rate": 9.484126984126985e-05,
      "loss": 0.1695,
      "step": 270
    },
    {
      "epoch": 4.2362204724409445,
      "grad_norm": 2.4496803283691406,
      "learning_rate": 9.444444444444444e-05,
      "loss": 0.0872,
      "step": 271
    },
    {
      "epoch": 4.251968503937007,
      "grad_norm": 3.230928897857666,
      "learning_rate": 9.404761904761905e-05,
      "loss": 0.1701,
      "step": 272
    },
    {
      "epoch": 4.267716535433071,
      "grad_norm": 2.092967987060547,
      "learning_rate": 9.365079365079366e-05,
      "loss": 0.0541,
      "step": 273
    },
    {
      "epoch": 4.283464566929134,
      "grad_norm": 1.8310832977294922,
      "learning_rate": 9.325396825396826e-05,
      "loss": 0.0724,
      "step": 274
    },
    {
      "epoch": 4.299212598425197,
      "grad_norm": 2.112551212310791,
      "learning_rate": 9.285714285714286e-05,
      "loss": 0.1025,
      "step": 275
    },
    {
      "epoch": 4.31496062992126,
      "grad_norm": 2.0924017429351807,
      "learning_rate": 9.246031746031747e-05,
      "loss": 0.0552,
      "step": 276
    },
    {
      "epoch": 4.330708661417323,
      "grad_norm": 2.89267897605896,
      "learning_rate": 9.206349206349206e-05,
      "loss": 0.1407,
      "step": 277
    },
    {
      "epoch": 4.346456692913386,
      "grad_norm": 2.3043622970581055,
      "learning_rate": 9.166666666666667e-05,
      "loss": 0.1143,
      "step": 278
    },
    {
      "epoch": 4.362204724409449,
      "grad_norm": 2.6150660514831543,
      "learning_rate": 9.126984126984128e-05,
      "loss": 0.1133,
      "step": 279
    },
    {
      "epoch": 4.377952755905512,
      "grad_norm": 3.445690631866455,
      "learning_rate": 9.087301587301588e-05,
      "loss": 0.128,
      "step": 280
    },
    {
      "epoch": 4.393700787401575,
      "grad_norm": 2.8390514850616455,
      "learning_rate": 9.047619047619048e-05,
      "loss": 0.1162,
      "step": 281
    },
    {
      "epoch": 4.409448818897638,
      "grad_norm": 2.152296781539917,
      "learning_rate": 9.007936507936508e-05,
      "loss": 0.0617,
      "step": 282
    },
    {
      "epoch": 4.425196850393701,
      "grad_norm": 2.8421971797943115,
      "learning_rate": 8.968253968253969e-05,
      "loss": 0.1426,
      "step": 283
    },
    {
      "epoch": 4.440944881889764,
      "grad_norm": 3.563105821609497,
      "learning_rate": 8.92857142857143e-05,
      "loss": 0.117,
      "step": 284
    },
    {
      "epoch": 4.456692913385827,
      "grad_norm": 1.657082200050354,
      "learning_rate": 8.888888888888889e-05,
      "loss": 0.0624,
      "step": 285
    },
    {
      "epoch": 4.47244094488189,
      "grad_norm": 3.3650267124176025,
      "learning_rate": 8.849206349206349e-05,
      "loss": 0.1148,
      "step": 286
    },
    {
      "epoch": 4.488188976377953,
      "grad_norm": 2.1918795108795166,
      "learning_rate": 8.80952380952381e-05,
      "loss": 0.0698,
      "step": 287
    },
    {
      "epoch": 4.503937007874016,
      "grad_norm": 2.920023202896118,
      "learning_rate": 8.76984126984127e-05,
      "loss": 0.1097,
      "step": 288
    },
    {
      "epoch": 4.519685039370079,
      "grad_norm": 1.683565378189087,
      "learning_rate": 8.730158730158731e-05,
      "loss": 0.071,
      "step": 289
    },
    {
      "epoch": 4.535433070866142,
      "grad_norm": 1.9648772478103638,
      "learning_rate": 8.690476190476192e-05,
      "loss": 0.0629,
      "step": 290
    },
    {
      "epoch": 4.551181102362205,
      "grad_norm": 2.5245444774627686,
      "learning_rate": 8.650793650793651e-05,
      "loss": 0.0889,
      "step": 291
    },
    {
      "epoch": 4.566929133858268,
      "grad_norm": 2.676302909851074,
      "learning_rate": 8.611111111111112e-05,
      "loss": 0.128,
      "step": 292
    },
    {
      "epoch": 4.582677165354331,
      "grad_norm": 3.393159866333008,
      "learning_rate": 8.571428571428571e-05,
      "loss": 0.151,
      "step": 293
    },
    {
      "epoch": 4.5984251968503935,
      "grad_norm": 3.1957204341888428,
      "learning_rate": 8.531746031746032e-05,
      "loss": 0.0896,
      "step": 294
    },
    {
      "epoch": 4.6141732283464565,
      "grad_norm": 2.735110282897949,
      "learning_rate": 8.492063492063493e-05,
      "loss": 0.1038,
      "step": 295
    },
    {
      "epoch": 4.6299212598425195,
      "grad_norm": 3.2717955112457275,
      "learning_rate": 8.452380952380952e-05,
      "loss": 0.1359,
      "step": 296
    },
    {
      "epoch": 4.645669291338582,
      "grad_norm": 2.9938409328460693,
      "learning_rate": 8.412698412698413e-05,
      "loss": 0.1633,
      "step": 297
    },
    {
      "epoch": 4.661417322834645,
      "grad_norm": 3.04311466217041,
      "learning_rate": 8.373015873015874e-05,
      "loss": 0.1779,
      "step": 298
    },
    {
      "epoch": 4.677165354330708,
      "grad_norm": 2.4587786197662354,
      "learning_rate": 8.333333333333334e-05,
      "loss": 0.1025,
      "step": 299
    },
    {
      "epoch": 4.692913385826771,
      "grad_norm": 2.7821083068847656,
      "learning_rate": 8.293650793650795e-05,
      "loss": 0.1376,
      "step": 300
    },
    {
      "epoch": 4.708661417322834,
      "grad_norm": 1.9175655841827393,
      "learning_rate": 8.253968253968255e-05,
      "loss": 0.1133,
      "step": 301
    },
    {
      "epoch": 4.724409448818898,
      "grad_norm": 2.1690356731414795,
      "learning_rate": 8.214285714285714e-05,
      "loss": 0.0735,
      "step": 302
    },
    {
      "epoch": 4.740157480314961,
      "grad_norm": 2.5142946243286133,
      "learning_rate": 8.174603174603175e-05,
      "loss": 0.0959,
      "step": 303
    },
    {
      "epoch": 4.755905511811024,
      "grad_norm": 5.2047295570373535,
      "learning_rate": 8.134920634920635e-05,
      "loss": 0.2513,
      "step": 304
    },
    {
      "epoch": 4.771653543307087,
      "grad_norm": 2.6455912590026855,
      "learning_rate": 8.095238095238096e-05,
      "loss": 0.1215,
      "step": 305
    },
    {
      "epoch": 4.78740157480315,
      "grad_norm": 3.1520745754241943,
      "learning_rate": 8.055555555555556e-05,
      "loss": 0.1701,
      "step": 306
    },
    {
      "epoch": 4.803149606299213,
      "grad_norm": 2.7153382301330566,
      "learning_rate": 8.015873015873016e-05,
      "loss": 0.154,
      "step": 307
    },
    {
      "epoch": 4.818897637795276,
      "grad_norm": 3.4862070083618164,
      "learning_rate": 7.976190476190477e-05,
      "loss": 0.1535,
      "step": 308
    },
    {
      "epoch": 4.834645669291339,
      "grad_norm": 1.9834426641464233,
      "learning_rate": 7.936507936507937e-05,
      "loss": 0.1086,
      "step": 309
    },
    {
      "epoch": 4.850393700787402,
      "grad_norm": 1.8222308158874512,
      "learning_rate": 7.896825396825397e-05,
      "loss": 0.064,
      "step": 310
    },
    {
      "epoch": 4.866141732283465,
      "grad_norm": 2.601335048675537,
      "learning_rate": 7.857142857142858e-05,
      "loss": 0.1597,
      "step": 311
    },
    {
      "epoch": 4.881889763779528,
      "grad_norm": 2.7289934158325195,
      "learning_rate": 7.817460317460317e-05,
      "loss": 0.1237,
      "step": 312
    },
    {
      "epoch": 4.897637795275591,
      "grad_norm": 2.592618227005005,
      "learning_rate": 7.777777777777778e-05,
      "loss": 0.1745,
      "step": 313
    },
    {
      "epoch": 4.913385826771654,
      "grad_norm": 1.8235700130462646,
      "learning_rate": 7.738095238095239e-05,
      "loss": 0.0732,
      "step": 314
    },
    {
      "epoch": 4.929133858267717,
      "grad_norm": 2.4509496688842773,
      "learning_rate": 7.6984126984127e-05,
      "loss": 0.1356,
      "step": 315
    },
    {
      "epoch": 4.94488188976378,
      "grad_norm": 2.4445960521698,
      "learning_rate": 7.658730158730159e-05,
      "loss": 0.1293,
      "step": 316
    },
    {
      "epoch": 4.960629921259843,
      "grad_norm": 2.4070374965667725,
      "learning_rate": 7.619047619047618e-05,
      "loss": 0.1044,
      "step": 317
    },
    {
      "epoch": 4.9763779527559056,
      "grad_norm": 3.3426947593688965,
      "learning_rate": 7.579365079365079e-05,
      "loss": 0.1219,
      "step": 318
    },
    {
      "epoch": 4.9921259842519685,
      "grad_norm": 3.6162362098693848,
      "learning_rate": 7.53968253968254e-05,
      "loss": 0.1603,
      "step": 319
    },
    {
      "epoch": 5.0,
      "grad_norm": 1.9618960618972778,
      "learning_rate": 7.500000000000001e-05,
      "loss": 0.0415,
      "step": 320
    },
    {
      "epoch": 5.0,
      "eval_loss": 0.22728084027767181,
      "eval_runtime": 25.2662,
      "eval_samples_per_second": 2.533,
      "eval_steps_per_second": 0.317,
      "step": 320
    },
    {
      "epoch": 5.015748031496063,
      "grad_norm": 2.7932581901550293,
      "learning_rate": 7.460317460317461e-05,
      "loss": 0.0797,
      "step": 321
    },
    {
      "epoch": 5.031496062992126,
      "grad_norm": 2.0808372497558594,
      "learning_rate": 7.420634920634921e-05,
      "loss": 0.0757,
      "step": 322
    },
    {
      "epoch": 5.047244094488189,
      "grad_norm": 2.273129940032959,
      "learning_rate": 7.380952380952382e-05,
      "loss": 0.1103,
      "step": 323
    },
    {
      "epoch": 5.062992125984252,
      "grad_norm": 2.7592625617980957,
      "learning_rate": 7.341269841269841e-05,
      "loss": 0.1382,
      "step": 324
    },
    {
      "epoch": 5.078740157480315,
      "grad_norm": 2.2491602897644043,
      "learning_rate": 7.301587301587302e-05,
      "loss": 0.0686,
      "step": 325
    },
    {
      "epoch": 5.094488188976378,
      "grad_norm": 2.8467295169830322,
      "learning_rate": 7.261904761904762e-05,
      "loss": 0.0821,
      "step": 326
    },
    {
      "epoch": 5.110236220472441,
      "grad_norm": 2.401291847229004,
      "learning_rate": 7.222222222222222e-05,
      "loss": 0.0981,
      "step": 327
    },
    {
      "epoch": 5.125984251968504,
      "grad_norm": 2.7337088584899902,
      "learning_rate": 7.182539682539683e-05,
      "loss": 0.0713,
      "step": 328
    },
    {
      "epoch": 5.141732283464567,
      "grad_norm": 2.4881842136383057,
      "learning_rate": 7.142857142857143e-05,
      "loss": 0.0846,
      "step": 329
    },
    {
      "epoch": 5.15748031496063,
      "grad_norm": 2.599698781967163,
      "learning_rate": 7.103174603174604e-05,
      "loss": 0.0686,
      "step": 330
    },
    {
      "epoch": 5.173228346456693,
      "grad_norm": 3.051401138305664,
      "learning_rate": 7.063492063492065e-05,
      "loss": 0.1086,
      "step": 331
    },
    {
      "epoch": 5.188976377952756,
      "grad_norm": 6.296233654022217,
      "learning_rate": 7.023809523809524e-05,
      "loss": 0.1074,
      "step": 332
    },
    {
      "epoch": 5.2047244094488185,
      "grad_norm": 3.7171120643615723,
      "learning_rate": 6.984126984126984e-05,
      "loss": 0.1077,
      "step": 333
    },
    {
      "epoch": 5.2204724409448815,
      "grad_norm": 3.114563465118408,
      "learning_rate": 6.944444444444444e-05,
      "loss": 0.082,
      "step": 334
    },
    {
      "epoch": 5.2362204724409445,
      "grad_norm": 3.7274134159088135,
      "learning_rate": 6.904761904761905e-05,
      "loss": 0.1136,
      "step": 335
    },
    {
      "epoch": 5.251968503937007,
      "grad_norm": 2.166649580001831,
      "learning_rate": 6.865079365079366e-05,
      "loss": 0.0534,
      "step": 336
    },
    {
      "epoch": 5.267716535433071,
      "grad_norm": 2.5756123065948486,
      "learning_rate": 6.825396825396825e-05,
      "loss": 0.0706,
      "step": 337
    },
    {
      "epoch": 5.283464566929134,
      "grad_norm": 2.68324875831604,
      "learning_rate": 6.785714285714286e-05,
      "loss": 0.1155,
      "step": 338
    },
    {
      "epoch": 5.299212598425197,
      "grad_norm": 2.456881284713745,
      "learning_rate": 6.746031746031747e-05,
      "loss": 0.12,
      "step": 339
    },
    {
      "epoch": 5.31496062992126,
      "grad_norm": 3.0372073650360107,
      "learning_rate": 6.706349206349206e-05,
      "loss": 0.1658,
      "step": 340
    },
    {
      "epoch": 5.330708661417323,
      "grad_norm": 2.615137815475464,
      "learning_rate": 6.666666666666667e-05,
      "loss": 0.0939,
      "step": 341
    },
    {
      "epoch": 5.346456692913386,
      "grad_norm": 2.63869571685791,
      "learning_rate": 6.626984126984128e-05,
      "loss": 0.0849,
      "step": 342
    },
    {
      "epoch": 5.362204724409449,
      "grad_norm": 2.7369887828826904,
      "learning_rate": 6.587301587301587e-05,
      "loss": 0.0786,
      "step": 343
    },
    {
      "epoch": 5.377952755905512,
      "grad_norm": 1.712794542312622,
      "learning_rate": 6.547619047619048e-05,
      "loss": 0.067,
      "step": 344
    },
    {
      "epoch": 5.393700787401575,
      "grad_norm": 2.6336004734039307,
      "learning_rate": 6.507936507936509e-05,
      "loss": 0.1006,
      "step": 345
    },
    {
      "epoch": 5.409448818897638,
      "grad_norm": 2.1741926670074463,
      "learning_rate": 6.46825396825397e-05,
      "loss": 0.0727,
      "step": 346
    },
    {
      "epoch": 5.425196850393701,
      "grad_norm": 2.757657051086426,
      "learning_rate": 6.428571428571429e-05,
      "loss": 0.0735,
      "step": 347
    },
    {
      "epoch": 5.440944881889764,
      "grad_norm": 2.2290167808532715,
      "learning_rate": 6.388888888888888e-05,
      "loss": 0.0746,
      "step": 348
    },
    {
      "epoch": 5.456692913385827,
      "grad_norm": 3.088135242462158,
      "learning_rate": 6.349206349206349e-05,
      "loss": 0.114,
      "step": 349
    },
    {
      "epoch": 5.47244094488189,
      "grad_norm": 1.9842259883880615,
      "learning_rate": 6.30952380952381e-05,
      "loss": 0.0848,
      "step": 350
    },
    {
      "epoch": 5.488188976377953,
      "grad_norm": 1.6865404844284058,
      "learning_rate": 6.26984126984127e-05,
      "loss": 0.0498,
      "step": 351
    },
    {
      "epoch": 5.503937007874016,
      "grad_norm": 2.8519933223724365,
      "learning_rate": 6.230158730158731e-05,
      "loss": 0.1486,
      "step": 352
    },
    {
      "epoch": 5.519685039370079,
      "grad_norm": 2.1382246017456055,
      "learning_rate": 6.19047619047619e-05,
      "loss": 0.0689,
      "step": 353
    },
    {
      "epoch": 5.535433070866142,
      "grad_norm": 2.77420973777771,
      "learning_rate": 6.150793650793651e-05,
      "loss": 0.0863,
      "step": 354
    },
    {
      "epoch": 5.551181102362205,
      "grad_norm": 2.70417857170105,
      "learning_rate": 6.111111111111112e-05,
      "loss": 0.0914,
      "step": 355
    },
    {
      "epoch": 5.566929133858268,
      "grad_norm": 2.2866291999816895,
      "learning_rate": 6.0714285714285715e-05,
      "loss": 0.0862,
      "step": 356
    },
    {
      "epoch": 5.582677165354331,
      "grad_norm": 3.5378260612487793,
      "learning_rate": 6.0317460317460316e-05,
      "loss": 0.1351,
      "step": 357
    },
    {
      "epoch": 5.5984251968503935,
      "grad_norm": 2.7225241661071777,
      "learning_rate": 5.992063492063492e-05,
      "loss": 0.0682,
      "step": 358
    },
    {
      "epoch": 5.6141732283464565,
      "grad_norm": 3.0939807891845703,
      "learning_rate": 5.9523809523809524e-05,
      "loss": 0.1355,
      "step": 359
    },
    {
      "epoch": 5.6299212598425195,
      "grad_norm": 3.8232932090759277,
      "learning_rate": 5.912698412698413e-05,
      "loss": 0.0785,
      "step": 360
    },
    {
      "epoch": 5.645669291338582,
      "grad_norm": 3.4989922046661377,
      "learning_rate": 5.873015873015873e-05,
      "loss": 0.0966,
      "step": 361
    },
    {
      "epoch": 5.661417322834645,
      "grad_norm": 3.433372974395752,
      "learning_rate": 5.833333333333334e-05,
      "loss": 0.0974,
      "step": 362
    },
    {
      "epoch": 5.677165354330708,
      "grad_norm": 2.391860008239746,
      "learning_rate": 5.793650793650795e-05,
      "loss": 0.0651,
      "step": 363
    },
    {
      "epoch": 5.692913385826771,
      "grad_norm": 4.355274677276611,
      "learning_rate": 5.753968253968254e-05,
      "loss": 0.173,
      "step": 364
    },
    {
      "epoch": 5.708661417322834,
      "grad_norm": 1.9503495693206787,
      "learning_rate": 5.714285714285714e-05,
      "loss": 0.0584,
      "step": 365
    },
    {
      "epoch": 5.724409448818898,
      "grad_norm": 2.9850265979766846,
      "learning_rate": 5.674603174603175e-05,
      "loss": 0.0876,
      "step": 366
    },
    {
      "epoch": 5.740157480314961,
      "grad_norm": 2.2880334854125977,
      "learning_rate": 5.634920634920635e-05,
      "loss": 0.0756,
      "step": 367
    },
    {
      "epoch": 5.755905511811024,
      "grad_norm": 2.7493696212768555,
      "learning_rate": 5.595238095238096e-05,
      "loss": 0.1026,
      "step": 368
    },
    {
      "epoch": 5.771653543307087,
      "grad_norm": 1.5755099058151245,
      "learning_rate": 5.555555555555556e-05,
      "loss": 0.0484,
      "step": 369
    },
    {
      "epoch": 5.78740157480315,
      "grad_norm": 2.9904658794403076,
      "learning_rate": 5.5158730158730166e-05,
      "loss": 0.113,
      "step": 370
    },
    {
      "epoch": 5.803149606299213,
      "grad_norm": 2.4151456356048584,
      "learning_rate": 5.4761904761904766e-05,
      "loss": 0.0879,
      "step": 371
    },
    {
      "epoch": 5.818897637795276,
      "grad_norm": 2.576409339904785,
      "learning_rate": 5.436507936507936e-05,
      "loss": 0.0651,
      "step": 372
    },
    {
      "epoch": 5.834645669291339,
      "grad_norm": 1.2561450004577637,
      "learning_rate": 5.396825396825397e-05,
      "loss": 0.03,
      "step": 373
    },
    {
      "epoch": 5.850393700787402,
      "grad_norm": 2.3782236576080322,
      "learning_rate": 5.3571428571428575e-05,
      "loss": 0.0682,
      "step": 374
    },
    {
      "epoch": 5.866141732283465,
      "grad_norm": 2.9417247772216797,
      "learning_rate": 5.3174603174603176e-05,
      "loss": 0.0938,
      "step": 375
    },
    {
      "epoch": 5.881889763779528,
      "grad_norm": 3.44270920753479,
      "learning_rate": 5.2777777777777784e-05,
      "loss": 0.1409,
      "step": 376
    },
    {
      "epoch": 5.897637795275591,
      "grad_norm": 3.8164470195770264,
      "learning_rate": 5.2380952380952384e-05,
      "loss": 0.1084,
      "step": 377
    },
    {
      "epoch": 5.913385826771654,
      "grad_norm": 2.9460036754608154,
      "learning_rate": 5.198412698412699e-05,
      "loss": 0.095,
      "step": 378
    },
    {
      "epoch": 5.929133858267717,
      "grad_norm": 2.5020174980163574,
      "learning_rate": 5.158730158730159e-05,
      "loss": 0.0824,
      "step": 379
    },
    {
      "epoch": 5.94488188976378,
      "grad_norm": 1.9177418947219849,
      "learning_rate": 5.119047619047619e-05,
      "loss": 0.0418,
      "step": 380
    },
    {
      "epoch": 5.960629921259843,
      "grad_norm": 3.0370874404907227,
      "learning_rate": 5.0793650793650794e-05,
      "loss": 0.1218,
      "step": 381
    },
    {
      "epoch": 5.9763779527559056,
      "grad_norm": 3.7915334701538086,
      "learning_rate": 5.0396825396825395e-05,
      "loss": 0.1257,
      "step": 382
    },
    {
      "epoch": 5.9921259842519685,
      "grad_norm": 2.0469117164611816,
      "learning_rate": 5e-05,
      "loss": 0.0648,
      "step": 383
    },
    {
      "epoch": 6.0,
      "grad_norm": 1.1536458730697632,
      "learning_rate": 4.960317460317461e-05,
      "loss": 0.0286,
      "step": 384
    },
    {
      "epoch": 6.0,
      "eval_loss": 0.2297797054052353,
      "eval_runtime": 25.3711,
      "eval_samples_per_second": 2.523,
      "eval_steps_per_second": 0.315,
      "step": 384
    },
    {
      "epoch": 6.015748031496063,
      "grad_norm": 1.3529694080352783,
      "learning_rate": 4.9206349206349204e-05,
      "loss": 0.0414,
      "step": 385
    },
    {
      "epoch": 6.031496062992126,
      "grad_norm": 2.1502127647399902,
      "learning_rate": 4.880952380952381e-05,
      "loss": 0.0939,
      "step": 386
    },
    {
      "epoch": 6.047244094488189,
      "grad_norm": 2.1515538692474365,
      "learning_rate": 4.841269841269841e-05,
      "loss": 0.0831,
      "step": 387
    },
    {
      "epoch": 6.062992125984252,
      "grad_norm": 2.294009208679199,
      "learning_rate": 4.801587301587302e-05,
      "loss": 0.0682,
      "step": 388
    },
    {
      "epoch": 6.078740157480315,
      "grad_norm": 2.5084378719329834,
      "learning_rate": 4.761904761904762e-05,
      "loss": 0.1147,
      "step": 389
    },
    {
      "epoch": 6.094488188976378,
      "grad_norm": 1.901352882385254,
      "learning_rate": 4.722222222222222e-05,
      "loss": 0.0727,
      "step": 390
    },
    {
      "epoch": 6.110236220472441,
      "grad_norm": 3.3133790493011475,
      "learning_rate": 4.682539682539683e-05,
      "loss": 0.0685,
      "step": 391
    },
    {
      "epoch": 6.125984251968504,
      "grad_norm": 2.3845021724700928,
      "learning_rate": 4.642857142857143e-05,
      "loss": 0.0902,
      "step": 392
    },
    {
      "epoch": 6.141732283464567,
      "grad_norm": 1.554107904434204,
      "learning_rate": 4.603174603174603e-05,
      "loss": 0.0514,
      "step": 393
    },
    {
      "epoch": 6.15748031496063,
      "grad_norm": 2.241018533706665,
      "learning_rate": 4.563492063492064e-05,
      "loss": 0.0758,
      "step": 394
    },
    {
      "epoch": 6.173228346456693,
      "grad_norm": 1.6727367639541626,
      "learning_rate": 4.523809523809524e-05,
      "loss": 0.0417,
      "step": 395
    },
    {
      "epoch": 6.188976377952756,
      "grad_norm": 1.8201533555984497,
      "learning_rate": 4.4841269841269846e-05,
      "loss": 0.0626,
      "step": 396
    },
    {
      "epoch": 6.2047244094488185,
      "grad_norm": 2.959956645965576,
      "learning_rate": 4.4444444444444447e-05,
      "loss": 0.0634,
      "step": 397
    },
    {
      "epoch": 6.2204724409448815,
      "grad_norm": 2.875436544418335,
      "learning_rate": 4.404761904761905e-05,
      "loss": 0.1,
      "step": 398
    },
    {
      "epoch": 6.2362204724409445,
      "grad_norm": 3.518483877182007,
      "learning_rate": 4.3650793650793655e-05,
      "loss": 0.0921,
      "step": 399
    },
    {
      "epoch": 6.251968503937007,
      "grad_norm": 2.371061325073242,
      "learning_rate": 4.3253968253968256e-05,
      "loss": 0.0784,
      "step": 400
    },
    {
      "epoch": 6.267716535433071,
      "grad_norm": 2.4038896560668945,
      "learning_rate": 4.2857142857142856e-05,
      "loss": 0.0551,
      "step": 401
    },
    {
      "epoch": 6.283464566929134,
      "grad_norm": 2.1469359397888184,
      "learning_rate": 4.2460317460317464e-05,
      "loss": 0.0604,
      "step": 402
    },
    {
      "epoch": 6.299212598425197,
      "grad_norm": 2.421200752258301,
      "learning_rate": 4.2063492063492065e-05,
      "loss": 0.0742,
      "step": 403
    },
    {
      "epoch": 6.31496062992126,
      "grad_norm": 1.5127770900726318,
      "learning_rate": 4.166666666666667e-05,
      "loss": 0.0448,
      "step": 404
    },
    {
      "epoch": 6.330708661417323,
      "grad_norm": 2.4610707759857178,
      "learning_rate": 4.126984126984127e-05,
      "loss": 0.0804,
      "step": 405
    },
    {
      "epoch": 6.346456692913386,
      "grad_norm": 2.8137941360473633,
      "learning_rate": 4.0873015873015874e-05,
      "loss": 0.0736,
      "step": 406
    },
    {
      "epoch": 6.362204724409449,
      "grad_norm": 4.2086381912231445,
      "learning_rate": 4.047619047619048e-05,
      "loss": 0.0818,
      "step": 407
    },
    {
      "epoch": 6.377952755905512,
      "grad_norm": 2.318967580795288,
      "learning_rate": 4.007936507936508e-05,
      "loss": 0.0672,
      "step": 408
    },
    {
      "epoch": 6.393700787401575,
      "grad_norm": 2.746323347091675,
      "learning_rate": 3.968253968253968e-05,
      "loss": 0.0928,
      "step": 409
    },
    {
      "epoch": 6.409448818897638,
      "grad_norm": 2.623225450515747,
      "learning_rate": 3.928571428571429e-05,
      "loss": 0.0765,
      "step": 410
    },
    {
      "epoch": 6.425196850393701,
      "grad_norm": 1.8200181722640991,
      "learning_rate": 3.888888888888889e-05,
      "loss": 0.0518,
      "step": 411
    },
    {
      "epoch": 6.440944881889764,
      "grad_norm": 1.7909958362579346,
      "learning_rate": 3.84920634920635e-05,
      "loss": 0.0558,
      "step": 412
    },
    {
      "epoch": 6.456692913385827,
      "grad_norm": 2.3244829177856445,
      "learning_rate": 3.809523809523809e-05,
      "loss": 0.069,
      "step": 413
    },
    {
      "epoch": 6.47244094488189,
      "grad_norm": 2.598940134048462,
      "learning_rate": 3.76984126984127e-05,
      "loss": 0.0404,
      "step": 414
    },
    {
      "epoch": 6.488188976377953,
      "grad_norm": 3.3263802528381348,
      "learning_rate": 3.730158730158731e-05,
      "loss": 0.0873,
      "step": 415
    },
    {
      "epoch": 6.503937007874016,
      "grad_norm": 2.141528606414795,
      "learning_rate": 3.690476190476191e-05,
      "loss": 0.0874,
      "step": 416
    },
    {
      "epoch": 6.519685039370079,
      "grad_norm": 3.3857054710388184,
      "learning_rate": 3.650793650793651e-05,
      "loss": 0.1125,
      "step": 417
    },
    {
      "epoch": 6.535433070866142,
      "grad_norm": 2.9315996170043945,
      "learning_rate": 3.611111111111111e-05,
      "loss": 0.0793,
      "step": 418
    },
    {
      "epoch": 6.551181102362205,
      "grad_norm": 0.9216564297676086,
      "learning_rate": 3.571428571428572e-05,
      "loss": 0.0357,
      "step": 419
    },
    {
      "epoch": 6.566929133858268,
      "grad_norm": 3.0507192611694336,
      "learning_rate": 3.5317460317460324e-05,
      "loss": 0.0919,
      "step": 420
    },
    {
      "epoch": 6.582677165354331,
      "grad_norm": 2.2233519554138184,
      "learning_rate": 3.492063492063492e-05,
      "loss": 0.0585,
      "step": 421
    },
    {
      "epoch": 6.5984251968503935,
      "grad_norm": 2.8834478855133057,
      "learning_rate": 3.4523809523809526e-05,
      "loss": 0.0746,
      "step": 422
    },
    {
      "epoch": 6.6141732283464565,
      "grad_norm": 2.173095226287842,
      "learning_rate": 3.412698412698413e-05,
      "loss": 0.0564,
      "step": 423
    },
    {
      "epoch": 6.6299212598425195,
      "grad_norm": 3.2433536052703857,
      "learning_rate": 3.3730158730158734e-05,
      "loss": 0.0893,
      "step": 424
    },
    {
      "epoch": 6.645669291338582,
      "grad_norm": 2.406437873840332,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 0.0837,
      "step": 425
    },
    {
      "epoch": 6.661417322834645,
      "grad_norm": 2.8681628704071045,
      "learning_rate": 3.2936507936507936e-05,
      "loss": 0.0865,
      "step": 426
    },
    {
      "epoch": 6.677165354330708,
      "grad_norm": 2.136667013168335,
      "learning_rate": 3.253968253968254e-05,
      "loss": 0.0476,
      "step": 427
    },
    {
      "epoch": 6.692913385826771,
      "grad_norm": 2.5620951652526855,
      "learning_rate": 3.2142857142857144e-05,
      "loss": 0.0725,
      "step": 428
    },
    {
      "epoch": 6.708661417322834,
      "grad_norm": 2.1216208934783936,
      "learning_rate": 3.1746031746031745e-05,
      "loss": 0.0559,
      "step": 429
    },
    {
      "epoch": 6.724409448818898,
      "grad_norm": 2.172302007675171,
      "learning_rate": 3.134920634920635e-05,
      "loss": 0.0514,
      "step": 430
    },
    {
      "epoch": 6.740157480314961,
      "grad_norm": 2.5000014305114746,
      "learning_rate": 3.095238095238095e-05,
      "loss": 0.0695,
      "step": 431
    },
    {
      "epoch": 6.755905511811024,
      "grad_norm": 2.5582478046417236,
      "learning_rate": 3.055555555555556e-05,
      "loss": 0.0631,
      "step": 432
    },
    {
      "epoch": 6.771653543307087,
      "grad_norm": 2.1701149940490723,
      "learning_rate": 3.0158730158730158e-05,
      "loss": 0.0647,
      "step": 433
    },
    {
      "epoch": 6.78740157480315,
      "grad_norm": 3.6049060821533203,
      "learning_rate": 2.9761904761904762e-05,
      "loss": 0.0765,
      "step": 434
    },
    {
      "epoch": 6.803149606299213,
      "grad_norm": 1.5077459812164307,
      "learning_rate": 2.9365079365079366e-05,
      "loss": 0.0354,
      "step": 435
    },
    {
      "epoch": 6.818897637795276,
      "grad_norm": 3.1031296253204346,
      "learning_rate": 2.8968253968253974e-05,
      "loss": 0.0826,
      "step": 436
    },
    {
      "epoch": 6.834645669291339,
      "grad_norm": 3.648273229598999,
      "learning_rate": 2.857142857142857e-05,
      "loss": 0.0801,
      "step": 437
    },
    {
      "epoch": 6.850393700787402,
      "grad_norm": 2.6620888710021973,
      "learning_rate": 2.8174603174603175e-05,
      "loss": 0.1064,
      "step": 438
    },
    {
      "epoch": 6.866141732283465,
      "grad_norm": 3.1890528202056885,
      "learning_rate": 2.777777777777778e-05,
      "loss": 0.129,
      "step": 439
    },
    {
      "epoch": 6.881889763779528,
      "grad_norm": 4.039255619049072,
      "learning_rate": 2.7380952380952383e-05,
      "loss": 0.1197,
      "step": 440
    },
    {
      "epoch": 6.897637795275591,
      "grad_norm": 2.202502489089966,
      "learning_rate": 2.6984126984126984e-05,
      "loss": 0.0706,
      "step": 441
    },
    {
      "epoch": 6.913385826771654,
      "grad_norm": 1.9395724534988403,
      "learning_rate": 2.6587301587301588e-05,
      "loss": 0.0483,
      "step": 442
    },
    {
      "epoch": 6.929133858267717,
      "grad_norm": 2.395683765411377,
      "learning_rate": 2.6190476190476192e-05,
      "loss": 0.0602,
      "step": 443
    },
    {
      "epoch": 6.94488188976378,
      "grad_norm": 2.9417531490325928,
      "learning_rate": 2.5793650793650796e-05,
      "loss": 0.0936,
      "step": 444
    },
    {
      "epoch": 6.960629921259843,
      "grad_norm": 2.0942647457122803,
      "learning_rate": 2.5396825396825397e-05,
      "loss": 0.0756,
      "step": 445
    },
    {
      "epoch": 6.9763779527559056,
      "grad_norm": 3.0247576236724854,
      "learning_rate": 2.5e-05,
      "loss": 0.0922,
      "step": 446
    },
    {
      "epoch": 6.9921259842519685,
      "grad_norm": 1.7142014503479004,
      "learning_rate": 2.4603174603174602e-05,
      "loss": 0.0465,
      "step": 447
    },
    {
      "epoch": 7.0,
      "grad_norm": 2.074195623397827,
      "learning_rate": 2.4206349206349206e-05,
      "loss": 0.0454,
      "step": 448
    },
    {
      "epoch": 7.0,
      "eval_loss": 0.24016617238521576,
      "eval_runtime": 22.2575,
      "eval_samples_per_second": 2.875,
      "eval_steps_per_second": 0.359,
      "step": 448
    },
    {
      "epoch": 7.015748031496063,
      "grad_norm": 1.8489636182785034,
      "learning_rate": 2.380952380952381e-05,
      "loss": 0.0385,
      "step": 449
    },
    {
      "epoch": 7.031496062992126,
      "grad_norm": 1.8477826118469238,
      "learning_rate": 2.3412698412698414e-05,
      "loss": 0.068,
      "step": 450
    },
    {
      "epoch": 7.047244094488189,
      "grad_norm": 1.7077455520629883,
      "learning_rate": 2.3015873015873015e-05,
      "loss": 0.0593,
      "step": 451
    },
    {
      "epoch": 7.062992125984252,
      "grad_norm": 1.204309344291687,
      "learning_rate": 2.261904761904762e-05,
      "loss": 0.0346,
      "step": 452
    },
    {
      "epoch": 7.078740157480315,
      "grad_norm": 1.6263070106506348,
      "learning_rate": 2.2222222222222223e-05,
      "loss": 0.0446,
      "step": 453
    },
    {
      "epoch": 7.094488188976378,
      "grad_norm": 2.67702054977417,
      "learning_rate": 2.1825396825396827e-05,
      "loss": 0.1074,
      "step": 454
    },
    {
      "epoch": 7.110236220472441,
      "grad_norm": 1.9504843950271606,
      "learning_rate": 2.1428571428571428e-05,
      "loss": 0.0594,
      "step": 455
    },
    {
      "epoch": 7.125984251968504,
      "grad_norm": 1.7961266040802002,
      "learning_rate": 2.1031746031746032e-05,
      "loss": 0.0583,
      "step": 456
    },
    {
      "epoch": 7.141732283464567,
      "grad_norm": 1.905921459197998,
      "learning_rate": 2.0634920634920636e-05,
      "loss": 0.0527,
      "step": 457
    },
    {
      "epoch": 7.15748031496063,
      "grad_norm": 2.241222381591797,
      "learning_rate": 2.023809523809524e-05,
      "loss": 0.0777,
      "step": 458
    },
    {
      "epoch": 7.173228346456693,
      "grad_norm": 1.4167876243591309,
      "learning_rate": 1.984126984126984e-05,
      "loss": 0.0489,
      "step": 459
    },
    {
      "epoch": 7.188976377952756,
      "grad_norm": 2.2755794525146484,
      "learning_rate": 1.9444444444444445e-05,
      "loss": 0.0615,
      "step": 460
    },
    {
      "epoch": 7.2047244094488185,
      "grad_norm": 2.180434465408325,
      "learning_rate": 1.9047619047619046e-05,
      "loss": 0.0644,
      "step": 461
    },
    {
      "epoch": 7.2204724409448815,
      "grad_norm": 2.568936347961426,
      "learning_rate": 1.8650793650793654e-05,
      "loss": 0.0721,
      "step": 462
    },
    {
      "epoch": 7.2362204724409445,
      "grad_norm": 1.7701448202133179,
      "learning_rate": 1.8253968253968254e-05,
      "loss": 0.0442,
      "step": 463
    },
    {
      "epoch": 7.251968503937007,
      "grad_norm": 2.132054090499878,
      "learning_rate": 1.785714285714286e-05,
      "loss": 0.0606,
      "step": 464
    },
    {
      "epoch": 7.267716535433071,
      "grad_norm": 2.4109749794006348,
      "learning_rate": 1.746031746031746e-05,
      "loss": 0.0822,
      "step": 465
    },
    {
      "epoch": 7.283464566929134,
      "grad_norm": 1.4690580368041992,
      "learning_rate": 1.7063492063492063e-05,
      "loss": 0.0322,
      "step": 466
    },
    {
      "epoch": 7.299212598425197,
      "grad_norm": 1.96597421169281,
      "learning_rate": 1.6666666666666667e-05,
      "loss": 0.0406,
      "step": 467
    },
    {
      "epoch": 7.31496062992126,
      "grad_norm": 2.216336965560913,
      "learning_rate": 1.626984126984127e-05,
      "loss": 0.0577,
      "step": 468
    },
    {
      "epoch": 7.330708661417323,
      "grad_norm": 2.657316207885742,
      "learning_rate": 1.5873015873015872e-05,
      "loss": 0.041,
      "step": 469
    },
    {
      "epoch": 7.346456692913386,
      "grad_norm": 1.7721843719482422,
      "learning_rate": 1.5476190476190476e-05,
      "loss": 0.0488,
      "step": 470
    },
    {
      "epoch": 7.362204724409449,
      "grad_norm": 1.9814865589141846,
      "learning_rate": 1.5079365079365079e-05,
      "loss": 0.0729,
      "step": 471
    },
    {
      "epoch": 7.377952755905512,
      "grad_norm": 2.3428070545196533,
      "learning_rate": 1.4682539682539683e-05,
      "loss": 0.0381,
      "step": 472
    },
    {
      "epoch": 7.393700787401575,
      "grad_norm": 2.6412105560302734,
      "learning_rate": 1.4285714285714285e-05,
      "loss": 0.089,
      "step": 473
    },
    {
      "epoch": 7.409448818897638,
      "grad_norm": 3.329814910888672,
      "learning_rate": 1.388888888888889e-05,
      "loss": 0.099,
      "step": 474
    },
    {
      "epoch": 7.425196850393701,
      "grad_norm": 2.5320885181427,
      "learning_rate": 1.3492063492063492e-05,
      "loss": 0.0627,
      "step": 475
    },
    {
      "epoch": 7.440944881889764,
      "grad_norm": 2.135540008544922,
      "learning_rate": 1.3095238095238096e-05,
      "loss": 0.0513,
      "step": 476
    },
    {
      "epoch": 7.456692913385827,
      "grad_norm": 2.977860450744629,
      "learning_rate": 1.2698412698412699e-05,
      "loss": 0.0974,
      "step": 477
    },
    {
      "epoch": 7.47244094488189,
      "grad_norm": 1.5285924673080444,
      "learning_rate": 1.2301587301587301e-05,
      "loss": 0.0384,
      "step": 478
    },
    {
      "epoch": 7.488188976377953,
      "grad_norm": 1.9912724494934082,
      "learning_rate": 1.1904761904761905e-05,
      "loss": 0.0666,
      "step": 479
    },
    {
      "epoch": 7.503937007874016,
      "grad_norm": 1.3940047025680542,
      "learning_rate": 1.1507936507936508e-05,
      "loss": 0.0435,
      "step": 480
    },
    {
      "epoch": 7.519685039370079,
      "grad_norm": 1.720441222190857,
      "learning_rate": 1.1111111111111112e-05,
      "loss": 0.0338,
      "step": 481
    },
    {
      "epoch": 7.535433070866142,
      "grad_norm": 3.5375804901123047,
      "learning_rate": 1.0714285714285714e-05,
      "loss": 0.0777,
      "step": 482
    },
    {
      "epoch": 7.551181102362205,
      "grad_norm": 2.3070363998413086,
      "learning_rate": 1.0317460317460318e-05,
      "loss": 0.0596,
      "step": 483
    },
    {
      "epoch": 7.566929133858268,
      "grad_norm": 2.4660298824310303,
      "learning_rate": 9.92063492063492e-06,
      "loss": 0.0711,
      "step": 484
    },
    {
      "epoch": 7.582677165354331,
      "grad_norm": 3.3712871074676514,
      "learning_rate": 9.523809523809523e-06,
      "loss": 0.0937,
      "step": 485
    },
    {
      "epoch": 7.5984251968503935,
      "grad_norm": 2.635488510131836,
      "learning_rate": 9.126984126984127e-06,
      "loss": 0.0878,
      "step": 486
    },
    {
      "epoch": 7.6141732283464565,
      "grad_norm": 1.68932044506073,
      "learning_rate": 8.73015873015873e-06,
      "loss": 0.0498,
      "step": 487
    },
    {
      "epoch": 7.6299212598425195,
      "grad_norm": 2.213094711303711,
      "learning_rate": 8.333333333333334e-06,
      "loss": 0.0406,
      "step": 488
    },
    {
      "epoch": 7.645669291338582,
      "grad_norm": 2.207141399383545,
      "learning_rate": 7.936507936507936e-06,
      "loss": 0.0487,
      "step": 489
    },
    {
      "epoch": 7.661417322834645,
      "grad_norm": 2.2906014919281006,
      "learning_rate": 7.5396825396825394e-06,
      "loss": 0.0535,
      "step": 490
    },
    {
      "epoch": 7.677165354330708,
      "grad_norm": 1.8748935461044312,
      "learning_rate": 7.142857142857143e-06,
      "loss": 0.0513,
      "step": 491
    },
    {
      "epoch": 7.692913385826771,
      "grad_norm": 2.4616079330444336,
      "learning_rate": 6.746031746031746e-06,
      "loss": 0.0808,
      "step": 492
    },
    {
      "epoch": 7.708661417322834,
      "grad_norm": 2.5869667530059814,
      "learning_rate": 6.349206349206349e-06,
      "loss": 0.0743,
      "step": 493
    },
    {
      "epoch": 7.724409448818898,
      "grad_norm": 2.5029304027557373,
      "learning_rate": 5.9523809523809525e-06,
      "loss": 0.0628,
      "step": 494
    },
    {
      "epoch": 7.740157480314961,
      "grad_norm": 1.6743732690811157,
      "learning_rate": 5.555555555555556e-06,
      "loss": 0.0428,
      "step": 495
    },
    {
      "epoch": 7.755905511811024,
      "grad_norm": 2.7715373039245605,
      "learning_rate": 5.158730158730159e-06,
      "loss": 0.067,
      "step": 496
    },
    {
      "epoch": 7.771653543307087,
      "grad_norm": 2.7600364685058594,
      "learning_rate": 4.7619047619047615e-06,
      "loss": 0.0677,
      "step": 497
    },
    {
      "epoch": 7.78740157480315,
      "grad_norm": 3.2899649143218994,
      "learning_rate": 4.365079365079365e-06,
      "loss": 0.0589,
      "step": 498
    },
    {
      "epoch": 7.803149606299213,
      "grad_norm": 2.291698932647705,
      "learning_rate": 3.968253968253968e-06,
      "loss": 0.0766,
      "step": 499
    },
    {
      "epoch": 7.818897637795276,
      "grad_norm": 2.1675424575805664,
      "learning_rate": 3.5714285714285714e-06,
      "loss": 0.0609,
      "step": 500
    },
    {
      "epoch": 7.834645669291339,
      "grad_norm": 2.1443095207214355,
      "learning_rate": 3.1746031746031746e-06,
      "loss": 0.0467,
      "step": 501
    },
    {
      "epoch": 7.850393700787402,
      "grad_norm": 2.1285552978515625,
      "learning_rate": 2.777777777777778e-06,
      "loss": 0.0749,
      "step": 502
    },
    {
      "epoch": 7.866141732283465,
      "grad_norm": 1.8573487997055054,
      "learning_rate": 2.3809523809523808e-06,
      "loss": 0.0559,
      "step": 503
    },
    {
      "epoch": 7.881889763779528,
      "grad_norm": 3.046948194503784,
      "learning_rate": 1.984126984126984e-06,
      "loss": 0.0593,
      "step": 504
    },
    {
      "epoch": 7.881889763779528,
      "eval_loss": 0.25149786472320557,
      "eval_runtime": 25.7605,
      "eval_samples_per_second": 2.484,
      "eval_steps_per_second": 0.311,
      "step": 504
    }
  ],
  "logging_steps": 1,
  "max_steps": 504,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 8,
  "save_steps": 500,
  "stateful_callbacks": {
    "EarlyStoppingCallback": {
      "args": {
        "early_stopping_patience": 5,
        "early_stopping_threshold": 0.0
      },
      "attributes": {
        "early_stopping_patience_counter": 3
      }
    },
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.1876271214493696e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
