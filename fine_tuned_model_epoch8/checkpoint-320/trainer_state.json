{
  "best_metric": 0.22484645247459412,
  "best_model_checkpoint": "./fine_tuned_model_epoch10\\checkpoint-320",
  "epoch": 5.0,
  "eval_steps": 500,
  "global_step": 320,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.015748031496062992,
      "grad_norm": NaN,
      "learning_rate": 0.0002,
      "loss": 15.3434,
      "step": 1
    },
    {
      "epoch": 0.031496062992125984,
      "grad_norm": NaN,
      "learning_rate": 0.0002,
      "loss": 14.6708,
      "step": 2
    },
    {
      "epoch": 0.047244094488188976,
      "grad_norm": NaN,
      "learning_rate": 0.0002,
      "loss": 15.768,
      "step": 3
    },
    {
      "epoch": 0.06299212598425197,
      "grad_norm": NaN,
      "learning_rate": 0.0002,
      "loss": 14.7788,
      "step": 4
    },
    {
      "epoch": 0.07874015748031496,
      "grad_norm": Infinity,
      "learning_rate": 0.0002,
      "loss": 14.8392,
      "step": 5
    },
    {
      "epoch": 0.09448818897637795,
      "grad_norm": 583.8530883789062,
      "learning_rate": 0.0001996031746031746,
      "loss": 14.6601,
      "step": 6
    },
    {
      "epoch": 0.11023622047244094,
      "grad_norm": 328.4643249511719,
      "learning_rate": 0.00019920634920634922,
      "loss": 9.6048,
      "step": 7
    },
    {
      "epoch": 0.12598425196850394,
      "grad_norm": 203.10707092285156,
      "learning_rate": 0.00019880952380952382,
      "loss": 4.6512,
      "step": 8
    },
    {
      "epoch": 0.14173228346456693,
      "grad_norm": 74.87577056884766,
      "learning_rate": 0.00019841269841269844,
      "loss": 1.7987,
      "step": 9
    },
    {
      "epoch": 0.15748031496062992,
      "grad_norm": 16.66163444519043,
      "learning_rate": 0.00019801587301587303,
      "loss": 1.1908,
      "step": 10
    },
    {
      "epoch": 0.1732283464566929,
      "grad_norm": 72.95970153808594,
      "learning_rate": 0.00019761904761904763,
      "loss": 1.2354,
      "step": 11
    },
    {
      "epoch": 0.1889763779527559,
      "grad_norm": 4.986556053161621,
      "learning_rate": 0.00019722222222222225,
      "loss": 1.2444,
      "step": 12
    },
    {
      "epoch": 0.2047244094488189,
      "grad_norm": 3.5961251258850098,
      "learning_rate": 0.00019682539682539682,
      "loss": 1.1014,
      "step": 13
    },
    {
      "epoch": 0.2204724409448819,
      "grad_norm": 7.00022029876709,
      "learning_rate": 0.00019642857142857144,
      "loss": 1.156,
      "step": 14
    },
    {
      "epoch": 0.23622047244094488,
      "grad_norm": 3.2920491695404053,
      "learning_rate": 0.00019603174603174603,
      "loss": 0.8957,
      "step": 15
    },
    {
      "epoch": 0.25196850393700787,
      "grad_norm": 4.470938205718994,
      "learning_rate": 0.00019563492063492062,
      "loss": 1.0121,
      "step": 16
    },
    {
      "epoch": 0.2677165354330709,
      "grad_norm": 3.326106548309326,
      "learning_rate": 0.00019523809523809525,
      "loss": 0.9046,
      "step": 17
    },
    {
      "epoch": 0.28346456692913385,
      "grad_norm": 3.0871708393096924,
      "learning_rate": 0.00019484126984126984,
      "loss": 0.9942,
      "step": 18
    },
    {
      "epoch": 0.2992125984251969,
      "grad_norm": 5.344061374664307,
      "learning_rate": 0.00019444444444444446,
      "loss": 0.7209,
      "step": 19
    },
    {
      "epoch": 0.31496062992125984,
      "grad_norm": 3.4552760124206543,
      "learning_rate": 0.00019404761904761905,
      "loss": 0.9846,
      "step": 20
    },
    {
      "epoch": 0.33070866141732286,
      "grad_norm": 3.490283727645874,
      "learning_rate": 0.00019365079365079365,
      "loss": 0.8662,
      "step": 21
    },
    {
      "epoch": 0.3464566929133858,
      "grad_norm": 10.272177696228027,
      "learning_rate": 0.00019325396825396827,
      "loss": 0.6262,
      "step": 22
    },
    {
      "epoch": 0.36220472440944884,
      "grad_norm": 2.0524275302886963,
      "learning_rate": 0.00019285714285714286,
      "loss": 0.7246,
      "step": 23
    },
    {
      "epoch": 0.3779527559055118,
      "grad_norm": 4.360315799713135,
      "learning_rate": 0.00019246031746031748,
      "loss": 0.8067,
      "step": 24
    },
    {
      "epoch": 0.3937007874015748,
      "grad_norm": 4.263572692871094,
      "learning_rate": 0.00019206349206349208,
      "loss": 0.5882,
      "step": 25
    },
    {
      "epoch": 0.4094488188976378,
      "grad_norm": 4.049643039703369,
      "learning_rate": 0.00019166666666666667,
      "loss": 0.7557,
      "step": 26
    },
    {
      "epoch": 0.4251968503937008,
      "grad_norm": 2.399465799331665,
      "learning_rate": 0.0001912698412698413,
      "loss": 0.7363,
      "step": 27
    },
    {
      "epoch": 0.4409448818897638,
      "grad_norm": 3.026445150375366,
      "learning_rate": 0.0001908730158730159,
      "loss": 0.5002,
      "step": 28
    },
    {
      "epoch": 0.4566929133858268,
      "grad_norm": 2.5868873596191406,
      "learning_rate": 0.00019047619047619048,
      "loss": 0.5926,
      "step": 29
    },
    {
      "epoch": 0.47244094488188976,
      "grad_norm": 3.740429639816284,
      "learning_rate": 0.00019007936507936508,
      "loss": 0.5865,
      "step": 30
    },
    {
      "epoch": 0.4881889763779528,
      "grad_norm": 1.867244839668274,
      "learning_rate": 0.0001896825396825397,
      "loss": 0.6925,
      "step": 31
    },
    {
      "epoch": 0.5039370078740157,
      "grad_norm": 3.234661817550659,
      "learning_rate": 0.0001892857142857143,
      "loss": 0.5923,
      "step": 32
    },
    {
      "epoch": 0.5196850393700787,
      "grad_norm": 1.236221432685852,
      "learning_rate": 0.00018888888888888888,
      "loss": 0.4991,
      "step": 33
    },
    {
      "epoch": 0.5354330708661418,
      "grad_norm": 4.695095539093018,
      "learning_rate": 0.0001884920634920635,
      "loss": 0.5047,
      "step": 34
    },
    {
      "epoch": 0.5511811023622047,
      "grad_norm": 1.4647661447525024,
      "learning_rate": 0.0001880952380952381,
      "loss": 0.5849,
      "step": 35
    },
    {
      "epoch": 0.5669291338582677,
      "grad_norm": 4.441333770751953,
      "learning_rate": 0.0001876984126984127,
      "loss": 0.5409,
      "step": 36
    },
    {
      "epoch": 0.5826771653543307,
      "grad_norm": 1.3789987564086914,
      "learning_rate": 0.00018730158730158731,
      "loss": 0.5051,
      "step": 37
    },
    {
      "epoch": 0.5984251968503937,
      "grad_norm": 2.009382724761963,
      "learning_rate": 0.0001869047619047619,
      "loss": 0.5427,
      "step": 38
    },
    {
      "epoch": 0.6141732283464567,
      "grad_norm": 1.230749487876892,
      "learning_rate": 0.00018650793650793653,
      "loss": 0.5259,
      "step": 39
    },
    {
      "epoch": 0.6299212598425197,
      "grad_norm": 1.8781542778015137,
      "learning_rate": 0.00018611111111111112,
      "loss": 0.6554,
      "step": 40
    },
    {
      "epoch": 0.6456692913385826,
      "grad_norm": 1.7882753610610962,
      "learning_rate": 0.00018571428571428572,
      "loss": 0.5356,
      "step": 41
    },
    {
      "epoch": 0.6614173228346457,
      "grad_norm": 1.3702940940856934,
      "learning_rate": 0.00018531746031746034,
      "loss": 0.4543,
      "step": 42
    },
    {
      "epoch": 0.6771653543307087,
      "grad_norm": 1.0122159719467163,
      "learning_rate": 0.00018492063492063493,
      "loss": 0.3088,
      "step": 43
    },
    {
      "epoch": 0.6929133858267716,
      "grad_norm": 1.4498697519302368,
      "learning_rate": 0.00018452380952380955,
      "loss": 0.4429,
      "step": 44
    },
    {
      "epoch": 0.7086614173228346,
      "grad_norm": 1.4350976943969727,
      "learning_rate": 0.00018412698412698412,
      "loss": 0.4259,
      "step": 45
    },
    {
      "epoch": 0.7244094488188977,
      "grad_norm": 1.449988603591919,
      "learning_rate": 0.00018373015873015874,
      "loss": 0.4266,
      "step": 46
    },
    {
      "epoch": 0.7401574803149606,
      "grad_norm": 1.5709879398345947,
      "learning_rate": 0.00018333333333333334,
      "loss": 0.4148,
      "step": 47
    },
    {
      "epoch": 0.7559055118110236,
      "grad_norm": 1.1787470579147339,
      "learning_rate": 0.00018293650793650793,
      "loss": 0.3447,
      "step": 48
    },
    {
      "epoch": 0.7716535433070866,
      "grad_norm": 1.5613726377487183,
      "learning_rate": 0.00018253968253968255,
      "loss": 0.4073,
      "step": 49
    },
    {
      "epoch": 0.7874015748031497,
      "grad_norm": 1.5914390087127686,
      "learning_rate": 0.00018214285714285714,
      "loss": 0.5079,
      "step": 50
    },
    {
      "epoch": 0.8031496062992126,
      "grad_norm": 2.2099101543426514,
      "learning_rate": 0.00018174603174603177,
      "loss": 0.389,
      "step": 51
    },
    {
      "epoch": 0.8188976377952756,
      "grad_norm": 1.4670369625091553,
      "learning_rate": 0.00018134920634920636,
      "loss": 0.3986,
      "step": 52
    },
    {
      "epoch": 0.8346456692913385,
      "grad_norm": 2.102114200592041,
      "learning_rate": 0.00018095238095238095,
      "loss": 0.5014,
      "step": 53
    },
    {
      "epoch": 0.8503937007874016,
      "grad_norm": 2.2428619861602783,
      "learning_rate": 0.00018055555555555557,
      "loss": 0.403,
      "step": 54
    },
    {
      "epoch": 0.8661417322834646,
      "grad_norm": 1.4195212125778198,
      "learning_rate": 0.00018015873015873017,
      "loss": 0.3348,
      "step": 55
    },
    {
      "epoch": 0.8818897637795275,
      "grad_norm": 1.1869333982467651,
      "learning_rate": 0.00017976190476190476,
      "loss": 0.3002,
      "step": 56
    },
    {
      "epoch": 0.8976377952755905,
      "grad_norm": 1.3952879905700684,
      "learning_rate": 0.00017936507936507938,
      "loss": 0.3313,
      "step": 57
    },
    {
      "epoch": 0.9133858267716536,
      "grad_norm": 1.5761204957962036,
      "learning_rate": 0.00017896825396825398,
      "loss": 0.3454,
      "step": 58
    },
    {
      "epoch": 0.9291338582677166,
      "grad_norm": 1.2315751314163208,
      "learning_rate": 0.0001785714285714286,
      "loss": 0.2584,
      "step": 59
    },
    {
      "epoch": 0.9448818897637795,
      "grad_norm": 1.6947702169418335,
      "learning_rate": 0.0001781746031746032,
      "loss": 0.3609,
      "step": 60
    },
    {
      "epoch": 0.9606299212598425,
      "grad_norm": 2.22110652923584,
      "learning_rate": 0.00017777777777777779,
      "loss": 0.3678,
      "step": 61
    },
    {
      "epoch": 0.9763779527559056,
      "grad_norm": 2.104053020477295,
      "learning_rate": 0.00017738095238095238,
      "loss": 0.5024,
      "step": 62
    },
    {
      "epoch": 0.9921259842519685,
      "grad_norm": 2.117429256439209,
      "learning_rate": 0.00017698412698412697,
      "loss": 0.4937,
      "step": 63
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.7148504853248596,
      "learning_rate": 0.0001765873015873016,
      "loss": 0.1186,
      "step": 64
    },
    {
      "epoch": 1.0,
      "eval_loss": 0.3847265839576721,
      "eval_runtime": 21.292,
      "eval_samples_per_second": 3.006,
      "eval_steps_per_second": 0.376,
      "step": 64
    },
    {
      "epoch": 1.015748031496063,
      "grad_norm": 1.5992248058319092,
      "learning_rate": 0.0001761904761904762,
      "loss": 0.411,
      "step": 65
    },
    {
      "epoch": 1.031496062992126,
      "grad_norm": 1.4404875040054321,
      "learning_rate": 0.0001757936507936508,
      "loss": 0.4048,
      "step": 66
    },
    {
      "epoch": 1.047244094488189,
      "grad_norm": 1.188262939453125,
      "learning_rate": 0.0001753968253968254,
      "loss": 0.2885,
      "step": 67
    },
    {
      "epoch": 1.0629921259842519,
      "grad_norm": 1.3807932138442993,
      "learning_rate": 0.000175,
      "loss": 0.3542,
      "step": 68
    },
    {
      "epoch": 1.078740157480315,
      "grad_norm": 1.4950343370437622,
      "learning_rate": 0.00017460317460317462,
      "loss": 0.3279,
      "step": 69
    },
    {
      "epoch": 1.094488188976378,
      "grad_norm": 1.5354328155517578,
      "learning_rate": 0.0001742063492063492,
      "loss": 0.3446,
      "step": 70
    },
    {
      "epoch": 1.110236220472441,
      "grad_norm": 1.4717798233032227,
      "learning_rate": 0.00017380952380952383,
      "loss": 0.2789,
      "step": 71
    },
    {
      "epoch": 1.125984251968504,
      "grad_norm": 1.8136041164398193,
      "learning_rate": 0.00017341269841269843,
      "loss": 0.3634,
      "step": 72
    },
    {
      "epoch": 1.141732283464567,
      "grad_norm": 1.5852723121643066,
      "learning_rate": 0.00017301587301587302,
      "loss": 0.2676,
      "step": 73
    },
    {
      "epoch": 1.1574803149606299,
      "grad_norm": 1.5484509468078613,
      "learning_rate": 0.00017261904761904764,
      "loss": 0.3667,
      "step": 74
    },
    {
      "epoch": 1.1732283464566928,
      "grad_norm": 1.8783468008041382,
      "learning_rate": 0.00017222222222222224,
      "loss": 0.304,
      "step": 75
    },
    {
      "epoch": 1.188976377952756,
      "grad_norm": 3.511021614074707,
      "learning_rate": 0.00017182539682539683,
      "loss": 0.41,
      "step": 76
    },
    {
      "epoch": 1.204724409448819,
      "grad_norm": 1.8271812200546265,
      "learning_rate": 0.00017142857142857143,
      "loss": 0.3101,
      "step": 77
    },
    {
      "epoch": 1.220472440944882,
      "grad_norm": 2.1123344898223877,
      "learning_rate": 0.00017103174603174602,
      "loss": 0.3118,
      "step": 78
    },
    {
      "epoch": 1.236220472440945,
      "grad_norm": 2.072206974029541,
      "learning_rate": 0.00017063492063492064,
      "loss": 0.3616,
      "step": 79
    },
    {
      "epoch": 1.2519685039370079,
      "grad_norm": 1.649746298789978,
      "learning_rate": 0.00017023809523809523,
      "loss": 0.2498,
      "step": 80
    },
    {
      "epoch": 1.2677165354330708,
      "grad_norm": 1.8038784265518188,
      "learning_rate": 0.00016984126984126986,
      "loss": 0.286,
      "step": 81
    },
    {
      "epoch": 1.2834645669291338,
      "grad_norm": 2.3300139904022217,
      "learning_rate": 0.00016944444444444445,
      "loss": 0.4299,
      "step": 82
    },
    {
      "epoch": 1.2992125984251968,
      "grad_norm": 2.760509967803955,
      "learning_rate": 0.00016904761904761904,
      "loss": 0.4037,
      "step": 83
    },
    {
      "epoch": 1.3149606299212597,
      "grad_norm": 1.8475110530853271,
      "learning_rate": 0.00016865079365079366,
      "loss": 0.38,
      "step": 84
    },
    {
      "epoch": 1.330708661417323,
      "grad_norm": 1.5729061365127563,
      "learning_rate": 0.00016825396825396826,
      "loss": 0.3465,
      "step": 85
    },
    {
      "epoch": 1.3464566929133859,
      "grad_norm": 1.9598472118377686,
      "learning_rate": 0.00016785714285714288,
      "loss": 0.2706,
      "step": 86
    },
    {
      "epoch": 1.3622047244094488,
      "grad_norm": 1.780557632446289,
      "learning_rate": 0.00016746031746031747,
      "loss": 0.3974,
      "step": 87
    },
    {
      "epoch": 1.3779527559055118,
      "grad_norm": 2.1705539226531982,
      "learning_rate": 0.00016706349206349207,
      "loss": 0.3857,
      "step": 88
    },
    {
      "epoch": 1.3937007874015748,
      "grad_norm": 1.394840121269226,
      "learning_rate": 0.0001666666666666667,
      "loss": 0.2412,
      "step": 89
    },
    {
      "epoch": 1.4094488188976377,
      "grad_norm": 2.241119623184204,
      "learning_rate": 0.00016626984126984128,
      "loss": 0.3192,
      "step": 90
    },
    {
      "epoch": 1.425196850393701,
      "grad_norm": 1.4489061832427979,
      "learning_rate": 0.0001658730158730159,
      "loss": 0.2153,
      "step": 91
    },
    {
      "epoch": 1.4409448818897639,
      "grad_norm": 1.4556970596313477,
      "learning_rate": 0.00016547619047619047,
      "loss": 0.2536,
      "step": 92
    },
    {
      "epoch": 1.4566929133858268,
      "grad_norm": 1.7368398904800415,
      "learning_rate": 0.0001650793650793651,
      "loss": 0.3103,
      "step": 93
    },
    {
      "epoch": 1.4724409448818898,
      "grad_norm": 1.4146943092346191,
      "learning_rate": 0.00016468253968253969,
      "loss": 0.2225,
      "step": 94
    },
    {
      "epoch": 1.4881889763779528,
      "grad_norm": 1.5500367879867554,
      "learning_rate": 0.00016428571428571428,
      "loss": 0.2107,
      "step": 95
    },
    {
      "epoch": 1.5039370078740157,
      "grad_norm": 1.8216962814331055,
      "learning_rate": 0.0001638888888888889,
      "loss": 0.2812,
      "step": 96
    },
    {
      "epoch": 1.5196850393700787,
      "grad_norm": 2.086578845977783,
      "learning_rate": 0.0001634920634920635,
      "loss": 0.3027,
      "step": 97
    },
    {
      "epoch": 1.5354330708661417,
      "grad_norm": 2.0148396492004395,
      "learning_rate": 0.0001630952380952381,
      "loss": 0.3524,
      "step": 98
    },
    {
      "epoch": 1.5511811023622046,
      "grad_norm": 3.2124361991882324,
      "learning_rate": 0.0001626984126984127,
      "loss": 0.3905,
      "step": 99
    },
    {
      "epoch": 1.5669291338582676,
      "grad_norm": 2.3329994678497314,
      "learning_rate": 0.0001623015873015873,
      "loss": 0.3948,
      "step": 100
    },
    {
      "epoch": 1.5826771653543306,
      "grad_norm": 2.055041551589966,
      "learning_rate": 0.00016190476190476192,
      "loss": 0.3272,
      "step": 101
    },
    {
      "epoch": 1.5984251968503937,
      "grad_norm": 1.9085795879364014,
      "learning_rate": 0.00016150793650793652,
      "loss": 0.2986,
      "step": 102
    },
    {
      "epoch": 1.6141732283464567,
      "grad_norm": 1.7493115663528442,
      "learning_rate": 0.0001611111111111111,
      "loss": 0.3081,
      "step": 103
    },
    {
      "epoch": 1.6299212598425197,
      "grad_norm": 2.0977065563201904,
      "learning_rate": 0.00016071428571428573,
      "loss": 0.4288,
      "step": 104
    },
    {
      "epoch": 1.6456692913385826,
      "grad_norm": 1.8580323457717896,
      "learning_rate": 0.00016031746031746033,
      "loss": 0.2919,
      "step": 105
    },
    {
      "epoch": 1.6614173228346458,
      "grad_norm": 2.4846129417419434,
      "learning_rate": 0.00015992063492063495,
      "loss": 0.24,
      "step": 106
    },
    {
      "epoch": 1.6771653543307088,
      "grad_norm": 1.625423789024353,
      "learning_rate": 0.00015952380952380954,
      "loss": 0.2817,
      "step": 107
    },
    {
      "epoch": 1.6929133858267718,
      "grad_norm": 1.8022689819335938,
      "learning_rate": 0.00015912698412698414,
      "loss": 0.303,
      "step": 108
    },
    {
      "epoch": 1.7086614173228347,
      "grad_norm": 2.345024585723877,
      "learning_rate": 0.00015873015873015873,
      "loss": 0.4259,
      "step": 109
    },
    {
      "epoch": 1.7244094488188977,
      "grad_norm": 1.5695953369140625,
      "learning_rate": 0.00015833333333333332,
      "loss": 0.251,
      "step": 110
    },
    {
      "epoch": 1.7401574803149606,
      "grad_norm": 1.8880287408828735,
      "learning_rate": 0.00015793650793650795,
      "loss": 0.3007,
      "step": 111
    },
    {
      "epoch": 1.7559055118110236,
      "grad_norm": 2.405425786972046,
      "learning_rate": 0.00015753968253968254,
      "loss": 0.335,
      "step": 112
    },
    {
      "epoch": 1.7716535433070866,
      "grad_norm": 2.2203590869903564,
      "learning_rate": 0.00015714285714285716,
      "loss": 0.3017,
      "step": 113
    },
    {
      "epoch": 1.7874015748031495,
      "grad_norm": 2.079493522644043,
      "learning_rate": 0.00015674603174603175,
      "loss": 0.3511,
      "step": 114
    },
    {
      "epoch": 1.8031496062992125,
      "grad_norm": 2.004556655883789,
      "learning_rate": 0.00015634920634920635,
      "loss": 0.231,
      "step": 115
    },
    {
      "epoch": 1.8188976377952755,
      "grad_norm": 2.5182228088378906,
      "learning_rate": 0.00015595238095238097,
      "loss": 0.3997,
      "step": 116
    },
    {
      "epoch": 1.8346456692913384,
      "grad_norm": 1.903281331062317,
      "learning_rate": 0.00015555555555555556,
      "loss": 0.2615,
      "step": 117
    },
    {
      "epoch": 1.8503937007874016,
      "grad_norm": 2.0932953357696533,
      "learning_rate": 0.00015515873015873016,
      "loss": 0.3534,
      "step": 118
    },
    {
      "epoch": 1.8661417322834646,
      "grad_norm": 2.145751476287842,
      "learning_rate": 0.00015476190476190478,
      "loss": 0.2921,
      "step": 119
    },
    {
      "epoch": 1.8818897637795275,
      "grad_norm": 1.4177688360214233,
      "learning_rate": 0.00015436507936507937,
      "loss": 0.2311,
      "step": 120
    },
    {
      "epoch": 1.8976377952755905,
      "grad_norm": 1.6322802305221558,
      "learning_rate": 0.000153968253968254,
      "loss": 0.2424,
      "step": 121
    },
    {
      "epoch": 1.9133858267716537,
      "grad_norm": 1.85543692111969,
      "learning_rate": 0.0001535714285714286,
      "loss": 0.2804,
      "step": 122
    },
    {
      "epoch": 1.9291338582677167,
      "grad_norm": 2.515054941177368,
      "learning_rate": 0.00015317460317460318,
      "loss": 0.3189,
      "step": 123
    },
    {
      "epoch": 1.9448818897637796,
      "grad_norm": 1.916776418685913,
      "learning_rate": 0.00015277777777777777,
      "loss": 0.3162,
      "step": 124
    },
    {
      "epoch": 1.9606299212598426,
      "grad_norm": 1.7729259729385376,
      "learning_rate": 0.00015238095238095237,
      "loss": 0.2553,
      "step": 125
    },
    {
      "epoch": 1.9763779527559056,
      "grad_norm": 1.6407512426376343,
      "learning_rate": 0.000151984126984127,
      "loss": 0.2552,
      "step": 126
    },
    {
      "epoch": 1.9921259842519685,
      "grad_norm": 1.8097301721572876,
      "learning_rate": 0.00015158730158730158,
      "loss": 0.2417,
      "step": 127
    },
    {
      "epoch": 2.0,
      "grad_norm": 1.6172059774398804,
      "learning_rate": 0.0001511904761904762,
      "loss": 0.2117,
      "step": 128
    },
    {
      "epoch": 2.0,
      "eval_loss": 0.31933701038360596,
      "eval_runtime": 33.7326,
      "eval_samples_per_second": 1.897,
      "eval_steps_per_second": 0.237,
      "step": 128
    },
    {
      "epoch": 2.015748031496063,
      "grad_norm": 1.8168420791625977,
      "learning_rate": 0.0001507936507936508,
      "loss": 0.3265,
      "step": 129
    },
    {
      "epoch": 2.031496062992126,
      "grad_norm": 1.2530814409255981,
      "learning_rate": 0.0001503968253968254,
      "loss": 0.1596,
      "step": 130
    },
    {
      "epoch": 2.047244094488189,
      "grad_norm": 1.491309404373169,
      "learning_rate": 0.00015000000000000001,
      "loss": 0.2425,
      "step": 131
    },
    {
      "epoch": 2.062992125984252,
      "grad_norm": 1.9385533332824707,
      "learning_rate": 0.0001496031746031746,
      "loss": 0.2939,
      "step": 132
    },
    {
      "epoch": 2.078740157480315,
      "grad_norm": 2.193877696990967,
      "learning_rate": 0.00014920634920634923,
      "loss": 0.3576,
      "step": 133
    },
    {
      "epoch": 2.094488188976378,
      "grad_norm": 1.6798498630523682,
      "learning_rate": 0.00014880952380952382,
      "loss": 0.2957,
      "step": 134
    },
    {
      "epoch": 2.1102362204724407,
      "grad_norm": 2.1306679248809814,
      "learning_rate": 0.00014841269841269842,
      "loss": 0.3099,
      "step": 135
    },
    {
      "epoch": 2.1259842519685037,
      "grad_norm": 1.2666335105895996,
      "learning_rate": 0.00014801587301587304,
      "loss": 0.1866,
      "step": 136
    },
    {
      "epoch": 2.141732283464567,
      "grad_norm": 1.955085277557373,
      "learning_rate": 0.00014761904761904763,
      "loss": 0.2298,
      "step": 137
    },
    {
      "epoch": 2.15748031496063,
      "grad_norm": 1.5285922288894653,
      "learning_rate": 0.00014722222222222223,
      "loss": 0.1822,
      "step": 138
    },
    {
      "epoch": 2.173228346456693,
      "grad_norm": 2.404688596725464,
      "learning_rate": 0.00014682539682539682,
      "loss": 0.2331,
      "step": 139
    },
    {
      "epoch": 2.188976377952756,
      "grad_norm": 2.248095750808716,
      "learning_rate": 0.00014642857142857141,
      "loss": 0.2226,
      "step": 140
    },
    {
      "epoch": 2.204724409448819,
      "grad_norm": 4.002237319946289,
      "learning_rate": 0.00014603174603174603,
      "loss": 0.3408,
      "step": 141
    },
    {
      "epoch": 2.220472440944882,
      "grad_norm": 2.966797113418579,
      "learning_rate": 0.00014563492063492063,
      "loss": 0.2766,
      "step": 142
    },
    {
      "epoch": 2.236220472440945,
      "grad_norm": 2.1574032306671143,
      "learning_rate": 0.00014523809523809525,
      "loss": 0.1791,
      "step": 143
    },
    {
      "epoch": 2.251968503937008,
      "grad_norm": 1.7290380001068115,
      "learning_rate": 0.00014484126984126984,
      "loss": 0.143,
      "step": 144
    },
    {
      "epoch": 2.267716535433071,
      "grad_norm": 2.8611817359924316,
      "learning_rate": 0.00014444444444444444,
      "loss": 0.273,
      "step": 145
    },
    {
      "epoch": 2.283464566929134,
      "grad_norm": 2.0615692138671875,
      "learning_rate": 0.00014404761904761906,
      "loss": 0.1923,
      "step": 146
    },
    {
      "epoch": 2.2992125984251968,
      "grad_norm": 1.9444326162338257,
      "learning_rate": 0.00014365079365079365,
      "loss": 0.2069,
      "step": 147
    },
    {
      "epoch": 2.3149606299212597,
      "grad_norm": 2.1436691284179688,
      "learning_rate": 0.00014325396825396827,
      "loss": 0.2061,
      "step": 148
    },
    {
      "epoch": 2.3307086614173227,
      "grad_norm": 2.5331404209136963,
      "learning_rate": 0.00014285714285714287,
      "loss": 0.3177,
      "step": 149
    },
    {
      "epoch": 2.3464566929133857,
      "grad_norm": 1.9476938247680664,
      "learning_rate": 0.00014246031746031746,
      "loss": 0.2123,
      "step": 150
    },
    {
      "epoch": 2.362204724409449,
      "grad_norm": 2.4329230785369873,
      "learning_rate": 0.00014206349206349208,
      "loss": 0.2544,
      "step": 151
    },
    {
      "epoch": 2.377952755905512,
      "grad_norm": 1.9117599725723267,
      "learning_rate": 0.00014166666666666668,
      "loss": 0.1858,
      "step": 152
    },
    {
      "epoch": 2.393700787401575,
      "grad_norm": 2.034541368484497,
      "learning_rate": 0.0001412698412698413,
      "loss": 0.1602,
      "step": 153
    },
    {
      "epoch": 2.409448818897638,
      "grad_norm": 2.4705650806427,
      "learning_rate": 0.0001408730158730159,
      "loss": 0.3191,
      "step": 154
    },
    {
      "epoch": 2.425196850393701,
      "grad_norm": 2.337388753890991,
      "learning_rate": 0.00014047619047619049,
      "loss": 0.2486,
      "step": 155
    },
    {
      "epoch": 2.440944881889764,
      "grad_norm": 1.7677892446517944,
      "learning_rate": 0.00014007936507936508,
      "loss": 0.202,
      "step": 156
    },
    {
      "epoch": 2.456692913385827,
      "grad_norm": 1.9775930643081665,
      "learning_rate": 0.00013968253968253967,
      "loss": 0.178,
      "step": 157
    },
    {
      "epoch": 2.47244094488189,
      "grad_norm": 1.874971628189087,
      "learning_rate": 0.0001392857142857143,
      "loss": 0.2567,
      "step": 158
    },
    {
      "epoch": 2.4881889763779528,
      "grad_norm": 2.455498456954956,
      "learning_rate": 0.0001388888888888889,
      "loss": 0.3067,
      "step": 159
    },
    {
      "epoch": 2.5039370078740157,
      "grad_norm": 2.178795099258423,
      "learning_rate": 0.00013849206349206348,
      "loss": 0.2744,
      "step": 160
    },
    {
      "epoch": 2.5196850393700787,
      "grad_norm": 2.3675005435943604,
      "learning_rate": 0.0001380952380952381,
      "loss": 0.2632,
      "step": 161
    },
    {
      "epoch": 2.5354330708661417,
      "grad_norm": 1.7574243545532227,
      "learning_rate": 0.0001376984126984127,
      "loss": 0.1708,
      "step": 162
    },
    {
      "epoch": 2.5511811023622046,
      "grad_norm": 2.0122392177581787,
      "learning_rate": 0.00013730158730158732,
      "loss": 0.2461,
      "step": 163
    },
    {
      "epoch": 2.5669291338582676,
      "grad_norm": 1.7055244445800781,
      "learning_rate": 0.0001369047619047619,
      "loss": 0.1848,
      "step": 164
    },
    {
      "epoch": 2.5826771653543306,
      "grad_norm": 2.167203903198242,
      "learning_rate": 0.0001365079365079365,
      "loss": 0.2661,
      "step": 165
    },
    {
      "epoch": 2.5984251968503935,
      "grad_norm": 2.3169775009155273,
      "learning_rate": 0.00013611111111111113,
      "loss": 0.2131,
      "step": 166
    },
    {
      "epoch": 2.6141732283464565,
      "grad_norm": 2.214470863342285,
      "learning_rate": 0.00013571428571428572,
      "loss": 0.2216,
      "step": 167
    },
    {
      "epoch": 2.6299212598425195,
      "grad_norm": 2.5808444023132324,
      "learning_rate": 0.00013531746031746034,
      "loss": 0.2863,
      "step": 168
    },
    {
      "epoch": 2.6456692913385824,
      "grad_norm": 1.6666918992996216,
      "learning_rate": 0.00013492063492063494,
      "loss": 0.1715,
      "step": 169
    },
    {
      "epoch": 2.661417322834646,
      "grad_norm": 2.1366307735443115,
      "learning_rate": 0.00013452380952380953,
      "loss": 0.3044,
      "step": 170
    },
    {
      "epoch": 2.677165354330709,
      "grad_norm": 1.9343048334121704,
      "learning_rate": 0.00013412698412698412,
      "loss": 0.2968,
      "step": 171
    },
    {
      "epoch": 2.6929133858267718,
      "grad_norm": 1.6991769075393677,
      "learning_rate": 0.00013373015873015872,
      "loss": 0.2051,
      "step": 172
    },
    {
      "epoch": 2.7086614173228347,
      "grad_norm": 2.276350736618042,
      "learning_rate": 0.00013333333333333334,
      "loss": 0.2463,
      "step": 173
    },
    {
      "epoch": 2.7244094488188977,
      "grad_norm": 2.0168511867523193,
      "learning_rate": 0.00013293650793650793,
      "loss": 0.2162,
      "step": 174
    },
    {
      "epoch": 2.7401574803149606,
      "grad_norm": 2.3979005813598633,
      "learning_rate": 0.00013253968253968255,
      "loss": 0.1958,
      "step": 175
    },
    {
      "epoch": 2.7559055118110236,
      "grad_norm": 2.7894515991210938,
      "learning_rate": 0.00013214285714285715,
      "loss": 0.2835,
      "step": 176
    },
    {
      "epoch": 2.7716535433070866,
      "grad_norm": 2.741535186767578,
      "learning_rate": 0.00013174603174603174,
      "loss": 0.3056,
      "step": 177
    },
    {
      "epoch": 2.7874015748031495,
      "grad_norm": 2.83585786819458,
      "learning_rate": 0.00013134920634920636,
      "loss": 0.324,
      "step": 178
    },
    {
      "epoch": 2.8031496062992125,
      "grad_norm": 30.695880889892578,
      "learning_rate": 0.00013095238095238096,
      "loss": 0.3749,
      "step": 179
    },
    {
      "epoch": 2.8188976377952755,
      "grad_norm": 1.9171465635299683,
      "learning_rate": 0.00013055555555555555,
      "loss": 0.1617,
      "step": 180
    },
    {
      "epoch": 2.8346456692913384,
      "grad_norm": 1.668194055557251,
      "learning_rate": 0.00013015873015873017,
      "loss": 0.1299,
      "step": 181
    },
    {
      "epoch": 2.850393700787402,
      "grad_norm": 3.9452717304229736,
      "learning_rate": 0.00012976190476190477,
      "loss": 0.2026,
      "step": 182
    },
    {
      "epoch": 2.866141732283465,
      "grad_norm": 2.9692416191101074,
      "learning_rate": 0.0001293650793650794,
      "loss": 0.3148,
      "step": 183
    },
    {
      "epoch": 2.8818897637795278,
      "grad_norm": 2.7886619567871094,
      "learning_rate": 0.00012896825396825398,
      "loss": 0.304,
      "step": 184
    },
    {
      "epoch": 2.8976377952755907,
      "grad_norm": 1.810120940208435,
      "learning_rate": 0.00012857142857142858,
      "loss": 0.1363,
      "step": 185
    },
    {
      "epoch": 2.9133858267716537,
      "grad_norm": 1.8910318613052368,
      "learning_rate": 0.0001281746031746032,
      "loss": 0.1576,
      "step": 186
    },
    {
      "epoch": 2.9291338582677167,
      "grad_norm": 2.3876235485076904,
      "learning_rate": 0.00012777777777777776,
      "loss": 0.228,
      "step": 187
    },
    {
      "epoch": 2.9448818897637796,
      "grad_norm": 2.33223557472229,
      "learning_rate": 0.00012738095238095238,
      "loss": 0.22,
      "step": 188
    },
    {
      "epoch": 2.9606299212598426,
      "grad_norm": 2.524806499481201,
      "learning_rate": 0.00012698412698412698,
      "loss": 0.2208,
      "step": 189
    },
    {
      "epoch": 2.9763779527559056,
      "grad_norm": 2.2608463764190674,
      "learning_rate": 0.0001265873015873016,
      "loss": 0.1857,
      "step": 190
    },
    {
      "epoch": 2.9921259842519685,
      "grad_norm": 3.357273578643799,
      "learning_rate": 0.0001261904761904762,
      "loss": 0.2429,
      "step": 191
    },
    {
      "epoch": 3.0,
      "grad_norm": 1.9070645570755005,
      "learning_rate": 0.0001257936507936508,
      "loss": 0.1406,
      "step": 192
    },
    {
      "epoch": 3.0,
      "eval_loss": 0.2671833634376526,
      "eval_runtime": 21.0443,
      "eval_samples_per_second": 3.041,
      "eval_steps_per_second": 0.38,
      "step": 192
    },
    {
      "epoch": 3.015748031496063,
      "grad_norm": 1.8717414140701294,
      "learning_rate": 0.0001253968253968254,
      "loss": 0.1707,
      "step": 193
    },
    {
      "epoch": 3.031496062992126,
      "grad_norm": 2.8362765312194824,
      "learning_rate": 0.000125,
      "loss": 0.1998,
      "step": 194
    },
    {
      "epoch": 3.047244094488189,
      "grad_norm": 2.2317886352539062,
      "learning_rate": 0.00012460317460317462,
      "loss": 0.2128,
      "step": 195
    },
    {
      "epoch": 3.062992125984252,
      "grad_norm": 2.3005027770996094,
      "learning_rate": 0.00012420634920634922,
      "loss": 0.2321,
      "step": 196
    },
    {
      "epoch": 3.078740157480315,
      "grad_norm": 1.3763986825942993,
      "learning_rate": 0.0001238095238095238,
      "loss": 0.107,
      "step": 197
    },
    {
      "epoch": 3.094488188976378,
      "grad_norm": 2.3660025596618652,
      "learning_rate": 0.00012341269841269843,
      "loss": 0.232,
      "step": 198
    },
    {
      "epoch": 3.1102362204724407,
      "grad_norm": 3.785940408706665,
      "learning_rate": 0.00012301587301587303,
      "loss": 0.2209,
      "step": 199
    },
    {
      "epoch": 3.1259842519685037,
      "grad_norm": 3.0049631595611572,
      "learning_rate": 0.00012261904761904762,
      "loss": 0.3216,
      "step": 200
    },
    {
      "epoch": 3.141732283464567,
      "grad_norm": 2.9273881912231445,
      "learning_rate": 0.00012222222222222224,
      "loss": 0.143,
      "step": 201
    },
    {
      "epoch": 3.15748031496063,
      "grad_norm": 2.102818012237549,
      "learning_rate": 0.00012182539682539682,
      "loss": 0.1246,
      "step": 202
    },
    {
      "epoch": 3.173228346456693,
      "grad_norm": 2.685685634613037,
      "learning_rate": 0.00012142857142857143,
      "loss": 0.1062,
      "step": 203
    },
    {
      "epoch": 3.188976377952756,
      "grad_norm": 3.612135648727417,
      "learning_rate": 0.00012103174603174602,
      "loss": 0.2075,
      "step": 204
    },
    {
      "epoch": 3.204724409448819,
      "grad_norm": 1.962701439857483,
      "learning_rate": 0.00012063492063492063,
      "loss": 0.0733,
      "step": 205
    },
    {
      "epoch": 3.220472440944882,
      "grad_norm": 2.93074631690979,
      "learning_rate": 0.00012023809523809524,
      "loss": 0.2012,
      "step": 206
    },
    {
      "epoch": 3.236220472440945,
      "grad_norm": 4.805657386779785,
      "learning_rate": 0.00011984126984126985,
      "loss": 0.1801,
      "step": 207
    },
    {
      "epoch": 3.251968503937008,
      "grad_norm": 3.2393314838409424,
      "learning_rate": 0.00011944444444444445,
      "loss": 0.1659,
      "step": 208
    },
    {
      "epoch": 3.267716535433071,
      "grad_norm": 2.7559473514556885,
      "learning_rate": 0.00011904761904761905,
      "loss": 0.1359,
      "step": 209
    },
    {
      "epoch": 3.283464566929134,
      "grad_norm": 1.2873072624206543,
      "learning_rate": 0.00011865079365079366,
      "loss": 0.0777,
      "step": 210
    },
    {
      "epoch": 3.2992125984251968,
      "grad_norm": 1.7158453464508057,
      "learning_rate": 0.00011825396825396826,
      "loss": 0.1093,
      "step": 211
    },
    {
      "epoch": 3.3149606299212597,
      "grad_norm": 3.3493313789367676,
      "learning_rate": 0.00011785714285714287,
      "loss": 0.0975,
      "step": 212
    },
    {
      "epoch": 3.3307086614173227,
      "grad_norm": 1.699272871017456,
      "learning_rate": 0.00011746031746031746,
      "loss": 0.1155,
      "step": 213
    },
    {
      "epoch": 3.3464566929133857,
      "grad_norm": 2.855581045150757,
      "learning_rate": 0.00011706349206349207,
      "loss": 0.1477,
      "step": 214
    },
    {
      "epoch": 3.362204724409449,
      "grad_norm": 2.024812936782837,
      "learning_rate": 0.00011666666666666668,
      "loss": 0.1398,
      "step": 215
    },
    {
      "epoch": 3.377952755905512,
      "grad_norm": 2.5960116386413574,
      "learning_rate": 0.00011626984126984129,
      "loss": 0.1247,
      "step": 216
    },
    {
      "epoch": 3.393700787401575,
      "grad_norm": 2.516634702682495,
      "learning_rate": 0.0001158730158730159,
      "loss": 0.1699,
      "step": 217
    },
    {
      "epoch": 3.409448818897638,
      "grad_norm": 2.320646286010742,
      "learning_rate": 0.00011547619047619047,
      "loss": 0.1805,
      "step": 218
    },
    {
      "epoch": 3.425196850393701,
      "grad_norm": 2.4939866065979004,
      "learning_rate": 0.00011507936507936508,
      "loss": 0.1447,
      "step": 219
    },
    {
      "epoch": 3.440944881889764,
      "grad_norm": 2.4812264442443848,
      "learning_rate": 0.00011468253968253968,
      "loss": 0.1008,
      "step": 220
    },
    {
      "epoch": 3.456692913385827,
      "grad_norm": 2.1068856716156006,
      "learning_rate": 0.00011428571428571428,
      "loss": 0.115,
      "step": 221
    },
    {
      "epoch": 3.47244094488189,
      "grad_norm": 2.3918027877807617,
      "learning_rate": 0.00011388888888888889,
      "loss": 0.131,
      "step": 222
    },
    {
      "epoch": 3.4881889763779528,
      "grad_norm": 8.353017807006836,
      "learning_rate": 0.0001134920634920635,
      "loss": 0.1343,
      "step": 223
    },
    {
      "epoch": 3.5039370078740157,
      "grad_norm": 2.4152936935424805,
      "learning_rate": 0.00011309523809523809,
      "loss": 0.1456,
      "step": 224
    },
    {
      "epoch": 3.5196850393700787,
      "grad_norm": 2.0978474617004395,
      "learning_rate": 0.0001126984126984127,
      "loss": 0.131,
      "step": 225
    },
    {
      "epoch": 3.5354330708661417,
      "grad_norm": 3.152831792831421,
      "learning_rate": 0.00011230158730158731,
      "loss": 0.1324,
      "step": 226
    },
    {
      "epoch": 3.5511811023622046,
      "grad_norm": 2.7983334064483643,
      "learning_rate": 0.00011190476190476191,
      "loss": 0.1789,
      "step": 227
    },
    {
      "epoch": 3.5669291338582676,
      "grad_norm": 2.23043155670166,
      "learning_rate": 0.00011150793650793652,
      "loss": 0.1284,
      "step": 228
    },
    {
      "epoch": 3.5826771653543306,
      "grad_norm": 1.7730923891067505,
      "learning_rate": 0.00011111111111111112,
      "loss": 0.0662,
      "step": 229
    },
    {
      "epoch": 3.5984251968503935,
      "grad_norm": 3.4080700874328613,
      "learning_rate": 0.00011071428571428572,
      "loss": 0.1669,
      "step": 230
    },
    {
      "epoch": 3.6141732283464565,
      "grad_norm": 1.4166299104690552,
      "learning_rate": 0.00011031746031746033,
      "loss": 0.0626,
      "step": 231
    },
    {
      "epoch": 3.6299212598425195,
      "grad_norm": 1.767345666885376,
      "learning_rate": 0.00010992063492063494,
      "loss": 0.1077,
      "step": 232
    },
    {
      "epoch": 3.6456692913385824,
      "grad_norm": 2.7420661449432373,
      "learning_rate": 0.00010952380952380953,
      "loss": 0.1589,
      "step": 233
    },
    {
      "epoch": 3.661417322834646,
      "grad_norm": 2.699986219406128,
      "learning_rate": 0.00010912698412698413,
      "loss": 0.1492,
      "step": 234
    },
    {
      "epoch": 3.677165354330709,
      "grad_norm": 2.47833514213562,
      "learning_rate": 0.00010873015873015872,
      "loss": 0.1832,
      "step": 235
    },
    {
      "epoch": 3.6929133858267718,
      "grad_norm": 1.9791715145111084,
      "learning_rate": 0.00010833333333333333,
      "loss": 0.0974,
      "step": 236
    },
    {
      "epoch": 3.7086614173228347,
      "grad_norm": 2.881664514541626,
      "learning_rate": 0.00010793650793650794,
      "loss": 0.1984,
      "step": 237
    },
    {
      "epoch": 3.7244094488188977,
      "grad_norm": 2.4593565464019775,
      "learning_rate": 0.00010753968253968254,
      "loss": 0.083,
      "step": 238
    },
    {
      "epoch": 3.7401574803149606,
      "grad_norm": 2.6728498935699463,
      "learning_rate": 0.00010714285714285715,
      "loss": 0.1533,
      "step": 239
    },
    {
      "epoch": 3.7559055118110236,
      "grad_norm": 1.9896022081375122,
      "learning_rate": 0.00010674603174603174,
      "loss": 0.1146,
      "step": 240
    },
    {
      "epoch": 3.7716535433070866,
      "grad_norm": 2.9686665534973145,
      "learning_rate": 0.00010634920634920635,
      "loss": 0.1676,
      "step": 241
    },
    {
      "epoch": 3.7874015748031495,
      "grad_norm": 2.9375252723693848,
      "learning_rate": 0.00010595238095238096,
      "loss": 0.1244,
      "step": 242
    },
    {
      "epoch": 3.8031496062992125,
      "grad_norm": 2.436972141265869,
      "learning_rate": 0.00010555555555555557,
      "loss": 0.1752,
      "step": 243
    },
    {
      "epoch": 3.8188976377952755,
      "grad_norm": 3.4734623432159424,
      "learning_rate": 0.00010515873015873016,
      "loss": 0.2018,
      "step": 244
    },
    {
      "epoch": 3.8346456692913384,
      "grad_norm": 2.615616798400879,
      "learning_rate": 0.00010476190476190477,
      "loss": 0.1403,
      "step": 245
    },
    {
      "epoch": 3.850393700787402,
      "grad_norm": 2.6657674312591553,
      "learning_rate": 0.00010436507936507938,
      "loss": 0.1983,
      "step": 246
    },
    {
      "epoch": 3.866141732283465,
      "grad_norm": 2.4332077503204346,
      "learning_rate": 0.00010396825396825398,
      "loss": 0.1632,
      "step": 247
    },
    {
      "epoch": 3.8818897637795278,
      "grad_norm": 3.8141696453094482,
      "learning_rate": 0.00010357142857142859,
      "loss": 0.2732,
      "step": 248
    },
    {
      "epoch": 3.8976377952755907,
      "grad_norm": 2.4922635555267334,
      "learning_rate": 0.00010317460317460319,
      "loss": 0.1163,
      "step": 249
    },
    {
      "epoch": 3.9133858267716537,
      "grad_norm": 2.8316521644592285,
      "learning_rate": 0.00010277777777777778,
      "loss": 0.1373,
      "step": 250
    },
    {
      "epoch": 3.9291338582677167,
      "grad_norm": 2.766733169555664,
      "learning_rate": 0.00010238095238095237,
      "loss": 0.1361,
      "step": 251
    },
    {
      "epoch": 3.9448818897637796,
      "grad_norm": 2.9093198776245117,
      "learning_rate": 0.00010198412698412698,
      "loss": 0.15,
      "step": 252
    },
    {
      "epoch": 3.9606299212598426,
      "grad_norm": 1.5992405414581299,
      "learning_rate": 0.00010158730158730159,
      "loss": 0.0574,
      "step": 253
    },
    {
      "epoch": 3.9763779527559056,
      "grad_norm": 2.633735418319702,
      "learning_rate": 0.0001011904761904762,
      "loss": 0.1739,
      "step": 254
    },
    {
      "epoch": 3.9921259842519685,
      "grad_norm": 2.480027198791504,
      "learning_rate": 0.00010079365079365079,
      "loss": 0.1455,
      "step": 255
    },
    {
      "epoch": 4.0,
      "grad_norm": 1.3641060590744019,
      "learning_rate": 0.0001003968253968254,
      "loss": 0.0418,
      "step": 256
    },
    {
      "epoch": 4.0,
      "eval_loss": 0.23028424382209778,
      "eval_runtime": 21.3424,
      "eval_samples_per_second": 2.999,
      "eval_steps_per_second": 0.375,
      "step": 256
    },
    {
      "epoch": 4.015748031496063,
      "grad_norm": 2.62076997756958,
      "learning_rate": 0.0001,
      "loss": 0.1093,
      "step": 257
    },
    {
      "epoch": 4.031496062992126,
      "grad_norm": 2.30275821685791,
      "learning_rate": 9.960317460317461e-05,
      "loss": 0.1471,
      "step": 258
    },
    {
      "epoch": 4.047244094488189,
      "grad_norm": 1.6686588525772095,
      "learning_rate": 9.920634920634922e-05,
      "loss": 0.0628,
      "step": 259
    },
    {
      "epoch": 4.062992125984252,
      "grad_norm": 2.800994873046875,
      "learning_rate": 9.880952380952381e-05,
      "loss": 0.1425,
      "step": 260
    },
    {
      "epoch": 4.078740157480315,
      "grad_norm": 2.590480089187622,
      "learning_rate": 9.841269841269841e-05,
      "loss": 0.1472,
      "step": 261
    },
    {
      "epoch": 4.094488188976378,
      "grad_norm": 2.451633930206299,
      "learning_rate": 9.801587301587302e-05,
      "loss": 0.125,
      "step": 262
    },
    {
      "epoch": 4.110236220472441,
      "grad_norm": 2.1051249504089355,
      "learning_rate": 9.761904761904762e-05,
      "loss": 0.0973,
      "step": 263
    },
    {
      "epoch": 4.125984251968504,
      "grad_norm": 1.612652063369751,
      "learning_rate": 9.722222222222223e-05,
      "loss": 0.0601,
      "step": 264
    },
    {
      "epoch": 4.141732283464567,
      "grad_norm": 2.566707134246826,
      "learning_rate": 9.682539682539682e-05,
      "loss": 0.0914,
      "step": 265
    },
    {
      "epoch": 4.15748031496063,
      "grad_norm": 1.8546699285507202,
      "learning_rate": 9.642857142857143e-05,
      "loss": 0.0564,
      "step": 266
    },
    {
      "epoch": 4.173228346456693,
      "grad_norm": 2.1110358238220215,
      "learning_rate": 9.603174603174604e-05,
      "loss": 0.1092,
      "step": 267
    },
    {
      "epoch": 4.188976377952756,
      "grad_norm": 2.339721202850342,
      "learning_rate": 9.563492063492065e-05,
      "loss": 0.0813,
      "step": 268
    },
    {
      "epoch": 4.2047244094488185,
      "grad_norm": 2.3112168312072754,
      "learning_rate": 9.523809523809524e-05,
      "loss": 0.0937,
      "step": 269
    },
    {
      "epoch": 4.2204724409448815,
      "grad_norm": 3.467313289642334,
      "learning_rate": 9.484126984126985e-05,
      "loss": 0.1692,
      "step": 270
    },
    {
      "epoch": 4.2362204724409445,
      "grad_norm": 3.282102584838867,
      "learning_rate": 9.444444444444444e-05,
      "loss": 0.1107,
      "step": 271
    },
    {
      "epoch": 4.251968503937007,
      "grad_norm": 3.9988043308258057,
      "learning_rate": 9.404761904761905e-05,
      "loss": 0.1449,
      "step": 272
    },
    {
      "epoch": 4.267716535433071,
      "grad_norm": 1.4934122562408447,
      "learning_rate": 9.365079365079366e-05,
      "loss": 0.0487,
      "step": 273
    },
    {
      "epoch": 4.283464566929134,
      "grad_norm": 2.0148098468780518,
      "learning_rate": 9.325396825396826e-05,
      "loss": 0.0652,
      "step": 274
    },
    {
      "epoch": 4.299212598425197,
      "grad_norm": 2.9564945697784424,
      "learning_rate": 9.285714285714286e-05,
      "loss": 0.1087,
      "step": 275
    },
    {
      "epoch": 4.31496062992126,
      "grad_norm": 2.7336223125457764,
      "learning_rate": 9.246031746031747e-05,
      "loss": 0.0509,
      "step": 276
    },
    {
      "epoch": 4.330708661417323,
      "grad_norm": 3.610440492630005,
      "learning_rate": 9.206349206349206e-05,
      "loss": 0.1642,
      "step": 277
    },
    {
      "epoch": 4.346456692913386,
      "grad_norm": 2.4292151927948,
      "learning_rate": 9.166666666666667e-05,
      "loss": 0.0937,
      "step": 278
    },
    {
      "epoch": 4.362204724409449,
      "grad_norm": 2.0979959964752197,
      "learning_rate": 9.126984126984128e-05,
      "loss": 0.0768,
      "step": 279
    },
    {
      "epoch": 4.377952755905512,
      "grad_norm": 3.3560426235198975,
      "learning_rate": 9.087301587301588e-05,
      "loss": 0.1629,
      "step": 280
    },
    {
      "epoch": 4.393700787401575,
      "grad_norm": 3.0165295600891113,
      "learning_rate": 9.047619047619048e-05,
      "loss": 0.1219,
      "step": 281
    },
    {
      "epoch": 4.409448818897638,
      "grad_norm": 1.6020115613937378,
      "learning_rate": 9.007936507936508e-05,
      "loss": 0.0511,
      "step": 282
    },
    {
      "epoch": 4.425196850393701,
      "grad_norm": 2.856144666671753,
      "learning_rate": 8.968253968253969e-05,
      "loss": 0.1461,
      "step": 283
    },
    {
      "epoch": 4.440944881889764,
      "grad_norm": 2.4934911727905273,
      "learning_rate": 8.92857142857143e-05,
      "loss": 0.1182,
      "step": 284
    },
    {
      "epoch": 4.456692913385827,
      "grad_norm": 1.8392165899276733,
      "learning_rate": 8.888888888888889e-05,
      "loss": 0.0786,
      "step": 285
    },
    {
      "epoch": 4.47244094488189,
      "grad_norm": 2.59291410446167,
      "learning_rate": 8.849206349206349e-05,
      "loss": 0.1238,
      "step": 286
    },
    {
      "epoch": 4.488188976377953,
      "grad_norm": 1.890944480895996,
      "learning_rate": 8.80952380952381e-05,
      "loss": 0.0553,
      "step": 287
    },
    {
      "epoch": 4.503937007874016,
      "grad_norm": 2.2590017318725586,
      "learning_rate": 8.76984126984127e-05,
      "loss": 0.1006,
      "step": 288
    },
    {
      "epoch": 4.519685039370079,
      "grad_norm": 1.2589153051376343,
      "learning_rate": 8.730158730158731e-05,
      "loss": 0.0627,
      "step": 289
    },
    {
      "epoch": 4.535433070866142,
      "grad_norm": 1.4407840967178345,
      "learning_rate": 8.690476190476192e-05,
      "loss": 0.0553,
      "step": 290
    },
    {
      "epoch": 4.551181102362205,
      "grad_norm": 2.3539505004882812,
      "learning_rate": 8.650793650793651e-05,
      "loss": 0.0893,
      "step": 291
    },
    {
      "epoch": 4.566929133858268,
      "grad_norm": 2.13606333732605,
      "learning_rate": 8.611111111111112e-05,
      "loss": 0.0973,
      "step": 292
    },
    {
      "epoch": 4.582677165354331,
      "grad_norm": 2.1327576637268066,
      "learning_rate": 8.571428571428571e-05,
      "loss": 0.1037,
      "step": 293
    },
    {
      "epoch": 4.5984251968503935,
      "grad_norm": 2.395749092102051,
      "learning_rate": 8.531746031746032e-05,
      "loss": 0.0901,
      "step": 294
    },
    {
      "epoch": 4.6141732283464565,
      "grad_norm": 2.4078574180603027,
      "learning_rate": 8.492063492063493e-05,
      "loss": 0.0987,
      "step": 295
    },
    {
      "epoch": 4.6299212598425195,
      "grad_norm": 2.7866876125335693,
      "learning_rate": 8.452380952380952e-05,
      "loss": 0.0938,
      "step": 296
    },
    {
      "epoch": 4.645669291338582,
      "grad_norm": 2.7865347862243652,
      "learning_rate": 8.412698412698413e-05,
      "loss": 0.1363,
      "step": 297
    },
    {
      "epoch": 4.661417322834645,
      "grad_norm": 3.0812535285949707,
      "learning_rate": 8.373015873015874e-05,
      "loss": 0.1726,
      "step": 298
    },
    {
      "epoch": 4.677165354330708,
      "grad_norm": 2.7007551193237305,
      "learning_rate": 8.333333333333334e-05,
      "loss": 0.1107,
      "step": 299
    },
    {
      "epoch": 4.692913385826771,
      "grad_norm": 2.8030173778533936,
      "learning_rate": 8.293650793650795e-05,
      "loss": 0.1603,
      "step": 300
    },
    {
      "epoch": 4.708661417322834,
      "grad_norm": 1.732517957687378,
      "learning_rate": 8.253968253968255e-05,
      "loss": 0.0878,
      "step": 301
    },
    {
      "epoch": 4.724409448818898,
      "grad_norm": 1.6013885736465454,
      "learning_rate": 8.214285714285714e-05,
      "loss": 0.0689,
      "step": 302
    },
    {
      "epoch": 4.740157480314961,
      "grad_norm": 1.757267951965332,
      "learning_rate": 8.174603174603175e-05,
      "loss": 0.0872,
      "step": 303
    },
    {
      "epoch": 4.755905511811024,
      "grad_norm": 5.530392646789551,
      "learning_rate": 8.134920634920635e-05,
      "loss": 0.2253,
      "step": 304
    },
    {
      "epoch": 4.771653543307087,
      "grad_norm": 2.527019500732422,
      "learning_rate": 8.095238095238096e-05,
      "loss": 0.1122,
      "step": 305
    },
    {
      "epoch": 4.78740157480315,
      "grad_norm": 4.101542949676514,
      "learning_rate": 8.055555555555556e-05,
      "loss": 0.1889,
      "step": 306
    },
    {
      "epoch": 4.803149606299213,
      "grad_norm": 2.8209335803985596,
      "learning_rate": 8.015873015873016e-05,
      "loss": 0.1338,
      "step": 307
    },
    {
      "epoch": 4.818897637795276,
      "grad_norm": 2.946702241897583,
      "learning_rate": 7.976190476190477e-05,
      "loss": 0.0944,
      "step": 308
    },
    {
      "epoch": 4.834645669291339,
      "grad_norm": 2.421495199203491,
      "learning_rate": 7.936507936507937e-05,
      "loss": 0.0774,
      "step": 309
    },
    {
      "epoch": 4.850393700787402,
      "grad_norm": 1.8744648694992065,
      "learning_rate": 7.896825396825397e-05,
      "loss": 0.067,
      "step": 310
    },
    {
      "epoch": 4.866141732283465,
      "grad_norm": 2.8433101177215576,
      "learning_rate": 7.857142857142858e-05,
      "loss": 0.1506,
      "step": 311
    },
    {
      "epoch": 4.881889763779528,
      "grad_norm": 2.2591192722320557,
      "learning_rate": 7.817460317460317e-05,
      "loss": 0.1303,
      "step": 312
    },
    {
      "epoch": 4.897637795275591,
      "grad_norm": 3.095705270767212,
      "learning_rate": 7.777777777777778e-05,
      "loss": 0.1585,
      "step": 313
    },
    {
      "epoch": 4.913385826771654,
      "grad_norm": 1.6564183235168457,
      "learning_rate": 7.738095238095239e-05,
      "loss": 0.0597,
      "step": 314
    },
    {
      "epoch": 4.929133858267717,
      "grad_norm": 2.958449602127075,
      "learning_rate": 7.6984126984127e-05,
      "loss": 0.1145,
      "step": 315
    },
    {
      "epoch": 4.94488188976378,
      "grad_norm": 2.8311033248901367,
      "learning_rate": 7.658730158730159e-05,
      "loss": 0.1154,
      "step": 316
    },
    {
      "epoch": 4.960629921259843,
      "grad_norm": 2.300243377685547,
      "learning_rate": 7.619047619047618e-05,
      "loss": 0.0808,
      "step": 317
    },
    {
      "epoch": 4.9763779527559056,
      "grad_norm": 2.9505271911621094,
      "learning_rate": 7.579365079365079e-05,
      "loss": 0.1163,
      "step": 318
    },
    {
      "epoch": 4.9921259842519685,
      "grad_norm": 4.878358364105225,
      "learning_rate": 7.53968253968254e-05,
      "loss": 0.1744,
      "step": 319
    },
    {
      "epoch": 5.0,
      "grad_norm": 1.7178130149841309,
      "learning_rate": 7.500000000000001e-05,
      "loss": 0.0392,
      "step": 320
    },
    {
      "epoch": 5.0,
      "eval_loss": 0.22484645247459412,
      "eval_runtime": 21.0997,
      "eval_samples_per_second": 3.033,
      "eval_steps_per_second": 0.379,
      "step": 320
    }
  ],
  "logging_steps": 1,
  "max_steps": 504,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 8,
  "save_steps": 500,
  "stateful_callbacks": {
    "EarlyStoppingCallback": {
      "args": {
        "early_stopping_patience": 5,
        "early_stopping_threshold": 0.0
      },
      "attributes": {
        "early_stopping_patience_counter": 0
      }
    },
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.387755466653696e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
