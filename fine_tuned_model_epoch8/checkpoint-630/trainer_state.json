{
  "best_metric": 0.2612713873386383,
  "best_model_checkpoint": "./fine_tuned_model_ver2\\checkpoint-500",
  "epoch": 9.921259842519685,
  "eval_steps": 500,
  "global_step": 630,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.015748031496062992,
      "grad_norm": NaN,
      "learning_rate": 0.0002,
      "loss": 15.2154,
      "step": 1
    },
    {
      "epoch": 0.031496062992125984,
      "grad_norm": NaN,
      "learning_rate": 0.0002,
      "loss": 14.7262,
      "step": 2
    },
    {
      "epoch": 0.047244094488188976,
      "grad_norm": NaN,
      "learning_rate": 0.0002,
      "loss": 15.8155,
      "step": 3
    },
    {
      "epoch": 0.06299212598425197,
      "grad_norm": NaN,
      "learning_rate": 0.0002,
      "loss": 14.6679,
      "step": 4
    },
    {
      "epoch": 0.07874015748031496,
      "grad_norm": Infinity,
      "learning_rate": 0.0002,
      "loss": 14.9297,
      "step": 5
    },
    {
      "epoch": 0.09448818897637795,
      "grad_norm": 492.9080810546875,
      "learning_rate": 0.0001996825396825397,
      "loss": 14.6601,
      "step": 6
    },
    {
      "epoch": 0.11023622047244094,
      "grad_norm": 275.3794250488281,
      "learning_rate": 0.00019936507936507938,
      "loss": 9.3921,
      "step": 7
    },
    {
      "epoch": 0.12598425196850394,
      "grad_norm": 196.1829376220703,
      "learning_rate": 0.00019904761904761907,
      "loss": 4.7252,
      "step": 8
    },
    {
      "epoch": 0.14173228346456693,
      "grad_norm": 78.94802856445312,
      "learning_rate": 0.00019873015873015875,
      "loss": 1.7109,
      "step": 9
    },
    {
      "epoch": 0.15748031496062992,
      "grad_norm": 17.27251625061035,
      "learning_rate": 0.00019841269841269844,
      "loss": 1.2149,
      "step": 10
    },
    {
      "epoch": 0.1732283464566929,
      "grad_norm": 180.51902770996094,
      "learning_rate": 0.0001980952380952381,
      "loss": 1.3813,
      "step": 11
    },
    {
      "epoch": 0.1889763779527559,
      "grad_norm": 5.112057209014893,
      "learning_rate": 0.00019777777777777778,
      "loss": 1.1848,
      "step": 12
    },
    {
      "epoch": 0.2047244094488189,
      "grad_norm": 3.4304511547088623,
      "learning_rate": 0.00019746031746031747,
      "loss": 1.0532,
      "step": 13
    },
    {
      "epoch": 0.2204724409448819,
      "grad_norm": 3.562178373336792,
      "learning_rate": 0.00019714285714285716,
      "loss": 1.1283,
      "step": 14
    },
    {
      "epoch": 0.23622047244094488,
      "grad_norm": 4.35179328918457,
      "learning_rate": 0.00019682539682539682,
      "loss": 0.8987,
      "step": 15
    },
    {
      "epoch": 0.25196850393700787,
      "grad_norm": 4.783968925476074,
      "learning_rate": 0.0001965079365079365,
      "loss": 1.0132,
      "step": 16
    },
    {
      "epoch": 0.2677165354330709,
      "grad_norm": 4.574904441833496,
      "learning_rate": 0.0001961904761904762,
      "loss": 0.9414,
      "step": 17
    },
    {
      "epoch": 0.28346456692913385,
      "grad_norm": 2.7062573432922363,
      "learning_rate": 0.00019587301587301587,
      "loss": 0.9133,
      "step": 18
    },
    {
      "epoch": 0.2992125984251969,
      "grad_norm": 23.135948181152344,
      "learning_rate": 0.00019555555555555556,
      "loss": 0.7771,
      "step": 19
    },
    {
      "epoch": 0.31496062992125984,
      "grad_norm": 3.5965116024017334,
      "learning_rate": 0.00019523809523809525,
      "loss": 1.0189,
      "step": 20
    },
    {
      "epoch": 0.33070866141732286,
      "grad_norm": 5.197793006896973,
      "learning_rate": 0.00019492063492063493,
      "loss": 0.7988,
      "step": 21
    },
    {
      "epoch": 0.3464566929133858,
      "grad_norm": 2.9950079917907715,
      "learning_rate": 0.00019460317460317462,
      "loss": 0.6165,
      "step": 22
    },
    {
      "epoch": 0.36220472440944884,
      "grad_norm": 2.3671791553497314,
      "learning_rate": 0.0001942857142857143,
      "loss": 0.7149,
      "step": 23
    },
    {
      "epoch": 0.3779527559055118,
      "grad_norm": 8.152050971984863,
      "learning_rate": 0.000193968253968254,
      "loss": 0.8017,
      "step": 24
    },
    {
      "epoch": 0.3937007874015748,
      "grad_norm": 4.8065185546875,
      "learning_rate": 0.00019365079365079365,
      "loss": 0.5994,
      "step": 25
    },
    {
      "epoch": 0.4094488188976378,
      "grad_norm": 2.0006320476531982,
      "learning_rate": 0.00019333333333333333,
      "loss": 0.7729,
      "step": 26
    },
    {
      "epoch": 0.4251968503937008,
      "grad_norm": 14.551419258117676,
      "learning_rate": 0.00019301587301587302,
      "loss": 0.7478,
      "step": 27
    },
    {
      "epoch": 0.4409448818897638,
      "grad_norm": 1.952078938484192,
      "learning_rate": 0.0001926984126984127,
      "loss": 0.4952,
      "step": 28
    },
    {
      "epoch": 0.4566929133858268,
      "grad_norm": 3.536191940307617,
      "learning_rate": 0.0001923809523809524,
      "loss": 0.5882,
      "step": 29
    },
    {
      "epoch": 0.47244094488188976,
      "grad_norm": 3.263230562210083,
      "learning_rate": 0.00019206349206349208,
      "loss": 0.587,
      "step": 30
    },
    {
      "epoch": 0.4881889763779528,
      "grad_norm": 1.7528736591339111,
      "learning_rate": 0.00019174603174603176,
      "loss": 0.6677,
      "step": 31
    },
    {
      "epoch": 0.5039370078740157,
      "grad_norm": 5.376856327056885,
      "learning_rate": 0.00019142857142857145,
      "loss": 0.5565,
      "step": 32
    },
    {
      "epoch": 0.5196850393700787,
      "grad_norm": 2.7484323978424072,
      "learning_rate": 0.00019111111111111114,
      "loss": 0.4887,
      "step": 33
    },
    {
      "epoch": 0.5354330708661418,
      "grad_norm": 1.382040023803711,
      "learning_rate": 0.00019079365079365082,
      "loss": 0.5081,
      "step": 34
    },
    {
      "epoch": 0.5511811023622047,
      "grad_norm": 1.6221051216125488,
      "learning_rate": 0.00019047619047619048,
      "loss": 0.5831,
      "step": 35
    },
    {
      "epoch": 0.5669291338582677,
      "grad_norm": 2.1463124752044678,
      "learning_rate": 0.00019015873015873017,
      "loss": 0.5999,
      "step": 36
    },
    {
      "epoch": 0.5826771653543307,
      "grad_norm": 1.3778120279312134,
      "learning_rate": 0.00018984126984126985,
      "loss": 0.4934,
      "step": 37
    },
    {
      "epoch": 0.5984251968503937,
      "grad_norm": 1.3060948848724365,
      "learning_rate": 0.0001895238095238095,
      "loss": 0.5167,
      "step": 38
    },
    {
      "epoch": 0.6141732283464567,
      "grad_norm": 1.5614885091781616,
      "learning_rate": 0.0001892063492063492,
      "loss": 0.5293,
      "step": 39
    },
    {
      "epoch": 0.6299212598425197,
      "grad_norm": 1.93218994140625,
      "learning_rate": 0.00018888888888888888,
      "loss": 0.7009,
      "step": 40
    },
    {
      "epoch": 0.6456692913385826,
      "grad_norm": 1.6049456596374512,
      "learning_rate": 0.00018857142857142857,
      "loss": 0.4796,
      "step": 41
    },
    {
      "epoch": 0.6614173228346457,
      "grad_norm": 1.2347134351730347,
      "learning_rate": 0.00018825396825396826,
      "loss": 0.4334,
      "step": 42
    },
    {
      "epoch": 0.6771653543307087,
      "grad_norm": 1.1222875118255615,
      "learning_rate": 0.00018793650793650794,
      "loss": 0.3115,
      "step": 43
    },
    {
      "epoch": 0.6929133858267716,
      "grad_norm": 1.436822533607483,
      "learning_rate": 0.00018761904761904763,
      "loss": 0.4462,
      "step": 44
    },
    {
      "epoch": 0.7086614173228346,
      "grad_norm": 1.7411017417907715,
      "learning_rate": 0.00018730158730158731,
      "loss": 0.4701,
      "step": 45
    },
    {
      "epoch": 0.7244094488188977,
      "grad_norm": 1.8104215860366821,
      "learning_rate": 0.000186984126984127,
      "loss": 0.3975,
      "step": 46
    },
    {
      "epoch": 0.7401574803149606,
      "grad_norm": 1.6500015258789062,
      "learning_rate": 0.0001866666666666667,
      "loss": 0.4043,
      "step": 47
    },
    {
      "epoch": 0.7559055118110236,
      "grad_norm": 1.4010249376296997,
      "learning_rate": 0.00018634920634920637,
      "loss": 0.3611,
      "step": 48
    },
    {
      "epoch": 0.7716535433070866,
      "grad_norm": 1.7255009412765503,
      "learning_rate": 0.00018603174603174606,
      "loss": 0.4406,
      "step": 49
    },
    {
      "epoch": 0.7874015748031497,
      "grad_norm": 1.8146610260009766,
      "learning_rate": 0.00018571428571428572,
      "loss": 0.5589,
      "step": 50
    },
    {
      "epoch": 0.8031496062992126,
      "grad_norm": 1.5189307928085327,
      "learning_rate": 0.0001853968253968254,
      "loss": 0.3546,
      "step": 51
    },
    {
      "epoch": 0.8188976377952756,
      "grad_norm": 1.582643747329712,
      "learning_rate": 0.0001850793650793651,
      "loss": 0.4053,
      "step": 52
    },
    {
      "epoch": 0.8346456692913385,
      "grad_norm": 1.9151434898376465,
      "learning_rate": 0.00018476190476190478,
      "loss": 0.5276,
      "step": 53
    },
    {
      "epoch": 0.8503937007874016,
      "grad_norm": 2.310373544692993,
      "learning_rate": 0.00018444444444444446,
      "loss": 0.4046,
      "step": 54
    },
    {
      "epoch": 0.8661417322834646,
      "grad_norm": 1.426217794418335,
      "learning_rate": 0.00018412698412698412,
      "loss": 0.3291,
      "step": 55
    },
    {
      "epoch": 0.8818897637795275,
      "grad_norm": 1.6118390560150146,
      "learning_rate": 0.0001838095238095238,
      "loss": 0.3035,
      "step": 56
    },
    {
      "epoch": 0.8976377952755905,
      "grad_norm": 1.5893152952194214,
      "learning_rate": 0.0001834920634920635,
      "loss": 0.3361,
      "step": 57
    },
    {
      "epoch": 0.9133858267716536,
      "grad_norm": 12.297483444213867,
      "learning_rate": 0.00018317460317460318,
      "loss": 0.3061,
      "step": 58
    },
    {
      "epoch": 0.9291338582677166,
      "grad_norm": 12.622079849243164,
      "learning_rate": 0.00018285714285714286,
      "loss": 0.2551,
      "step": 59
    },
    {
      "epoch": 0.9448818897637795,
      "grad_norm": 1.9327934980392456,
      "learning_rate": 0.00018253968253968255,
      "loss": 0.3518,
      "step": 60
    },
    {
      "epoch": 0.9606299212598425,
      "grad_norm": 1.671635389328003,
      "learning_rate": 0.00018222222222222224,
      "loss": 0.3441,
      "step": 61
    },
    {
      "epoch": 0.9763779527559056,
      "grad_norm": 2.4189558029174805,
      "learning_rate": 0.00018190476190476192,
      "loss": 0.4756,
      "step": 62
    },
    {
      "epoch": 0.9921259842519685,
      "grad_norm": 2.148101329803467,
      "learning_rate": 0.00018158730158730158,
      "loss": 0.5463,
      "step": 63
    },
    {
      "epoch": 1.0078740157480315,
      "grad_norm": 1.7278531789779663,
      "learning_rate": 0.00018126984126984127,
      "loss": 0.3451,
      "step": 64
    },
    {
      "epoch": 1.0236220472440944,
      "grad_norm": 1.36931312084198,
      "learning_rate": 0.00018095238095238095,
      "loss": 0.3627,
      "step": 65
    },
    {
      "epoch": 1.0393700787401574,
      "grad_norm": 1.6563153266906738,
      "learning_rate": 0.00018063492063492064,
      "loss": 0.4344,
      "step": 66
    },
    {
      "epoch": 1.0551181102362204,
      "grad_norm": 1.2839423418045044,
      "learning_rate": 0.00018031746031746033,
      "loss": 0.316,
      "step": 67
    },
    {
      "epoch": 1.0708661417322836,
      "grad_norm": 1.576168417930603,
      "learning_rate": 0.00018,
      "loss": 0.3742,
      "step": 68
    },
    {
      "epoch": 1.0866141732283465,
      "grad_norm": 1.2933582067489624,
      "learning_rate": 0.0001796825396825397,
      "loss": 0.3099,
      "step": 69
    },
    {
      "epoch": 1.1023622047244095,
      "grad_norm": 1.3539211750030518,
      "learning_rate": 0.00017936507936507938,
      "loss": 0.2798,
      "step": 70
    },
    {
      "epoch": 1.1181102362204725,
      "grad_norm": 1.2706421613693237,
      "learning_rate": 0.00017904761904761907,
      "loss": 0.2232,
      "step": 71
    },
    {
      "epoch": 1.1338582677165354,
      "grad_norm": 1.7937172651290894,
      "learning_rate": 0.00017873015873015876,
      "loss": 0.3764,
      "step": 72
    },
    {
      "epoch": 1.1496062992125984,
      "grad_norm": 1.1043874025344849,
      "learning_rate": 0.00017841269841269844,
      "loss": 0.2596,
      "step": 73
    },
    {
      "epoch": 1.1653543307086613,
      "grad_norm": 1.960105538368225,
      "learning_rate": 0.0001780952380952381,
      "loss": 0.4399,
      "step": 74
    },
    {
      "epoch": 1.1811023622047245,
      "grad_norm": 2.698296546936035,
      "learning_rate": 0.00017777777777777779,
      "loss": 0.3644,
      "step": 75
    },
    {
      "epoch": 1.1968503937007875,
      "grad_norm": 2.222734212875366,
      "learning_rate": 0.00017746031746031747,
      "loss": 0.3303,
      "step": 76
    },
    {
      "epoch": 1.2125984251968505,
      "grad_norm": 1.8624070882797241,
      "learning_rate": 0.00017714285714285713,
      "loss": 0.3465,
      "step": 77
    },
    {
      "epoch": 1.2283464566929134,
      "grad_norm": 1.4737240076065063,
      "learning_rate": 0.00017682539682539682,
      "loss": 0.2809,
      "step": 78
    },
    {
      "epoch": 1.2440944881889764,
      "grad_norm": 1.9366862773895264,
      "learning_rate": 0.0001765079365079365,
      "loss": 0.252,
      "step": 79
    },
    {
      "epoch": 1.2598425196850394,
      "grad_norm": 2.8260624408721924,
      "learning_rate": 0.0001761904761904762,
      "loss": 0.3299,
      "step": 80
    },
    {
      "epoch": 1.2755905511811023,
      "grad_norm": 1.7142696380615234,
      "learning_rate": 0.00017587301587301588,
      "loss": 0.2598,
      "step": 81
    },
    {
      "epoch": 1.2913385826771653,
      "grad_norm": 2.9573094844818115,
      "learning_rate": 0.00017555555555555556,
      "loss": 0.5175,
      "step": 82
    },
    {
      "epoch": 1.3070866141732282,
      "grad_norm": 1.8871086835861206,
      "learning_rate": 0.00017523809523809525,
      "loss": 0.3355,
      "step": 83
    },
    {
      "epoch": 1.3228346456692912,
      "grad_norm": 2.3086419105529785,
      "learning_rate": 0.00017492063492063493,
      "loss": 0.444,
      "step": 84
    },
    {
      "epoch": 1.3385826771653544,
      "grad_norm": 1.5367392301559448,
      "learning_rate": 0.00017460317460317462,
      "loss": 0.2808,
      "step": 85
    },
    {
      "epoch": 1.3543307086614174,
      "grad_norm": 2.225208044052124,
      "learning_rate": 0.0001742857142857143,
      "loss": 0.3554,
      "step": 86
    },
    {
      "epoch": 1.3700787401574803,
      "grad_norm": 1.464513897895813,
      "learning_rate": 0.000173968253968254,
      "loss": 0.2999,
      "step": 87
    },
    {
      "epoch": 1.3858267716535433,
      "grad_norm": 1.87464439868927,
      "learning_rate": 0.00017365079365079368,
      "loss": 0.3531,
      "step": 88
    },
    {
      "epoch": 1.4015748031496063,
      "grad_norm": 1.3372321128845215,
      "learning_rate": 0.00017333333333333334,
      "loss": 0.2346,
      "step": 89
    },
    {
      "epoch": 1.4173228346456692,
      "grad_norm": 2.206740617752075,
      "learning_rate": 0.00017301587301587302,
      "loss": 0.3386,
      "step": 90
    },
    {
      "epoch": 1.4330708661417324,
      "grad_norm": 1.4750608205795288,
      "learning_rate": 0.0001726984126984127,
      "loss": 0.2108,
      "step": 91
    },
    {
      "epoch": 1.4488188976377954,
      "grad_norm": 1.6934973001480103,
      "learning_rate": 0.0001723809523809524,
      "loss": 0.2607,
      "step": 92
    },
    {
      "epoch": 1.4645669291338583,
      "grad_norm": 1.362772822380066,
      "learning_rate": 0.00017206349206349208,
      "loss": 0.2364,
      "step": 93
    },
    {
      "epoch": 1.4803149606299213,
      "grad_norm": 1.9367449283599854,
      "learning_rate": 0.00017174603174603174,
      "loss": 0.3011,
      "step": 94
    },
    {
      "epoch": 1.4960629921259843,
      "grad_norm": 1.4972909688949585,
      "learning_rate": 0.00017142857142857143,
      "loss": 0.1987,
      "step": 95
    },
    {
      "epoch": 1.5118110236220472,
      "grad_norm": 3.039458751678467,
      "learning_rate": 0.0001711111111111111,
      "loss": 0.418,
      "step": 96
    },
    {
      "epoch": 1.5275590551181102,
      "grad_norm": 1.7051279544830322,
      "learning_rate": 0.0001707936507936508,
      "loss": 0.2223,
      "step": 97
    },
    {
      "epoch": 1.5433070866141732,
      "grad_norm": 2.635429620742798,
      "learning_rate": 0.00017047619047619048,
      "loss": 0.3712,
      "step": 98
    },
    {
      "epoch": 1.5590551181102361,
      "grad_norm": 2.970446825027466,
      "learning_rate": 0.00017015873015873017,
      "loss": 0.4027,
      "step": 99
    },
    {
      "epoch": 1.574803149606299,
      "grad_norm": 2.80741286277771,
      "learning_rate": 0.00016984126984126986,
      "loss": 0.4614,
      "step": 100
    },
    {
      "epoch": 1.590551181102362,
      "grad_norm": 2.004099130630493,
      "learning_rate": 0.00016952380952380954,
      "loss": 0.3585,
      "step": 101
    },
    {
      "epoch": 1.6062992125984252,
      "grad_norm": 1.6456799507141113,
      "learning_rate": 0.0001692063492063492,
      "loss": 0.2918,
      "step": 102
    },
    {
      "epoch": 1.6220472440944882,
      "grad_norm": 2.3160390853881836,
      "learning_rate": 0.00016888888888888889,
      "loss": 0.391,
      "step": 103
    },
    {
      "epoch": 1.6377952755905512,
      "grad_norm": 2.187971353530884,
      "learning_rate": 0.00016857142857142857,
      "loss": 0.3413,
      "step": 104
    },
    {
      "epoch": 1.6535433070866141,
      "grad_norm": 1.5912220478057861,
      "learning_rate": 0.00016825396825396826,
      "loss": 0.2556,
      "step": 105
    },
    {
      "epoch": 1.6692913385826773,
      "grad_norm": 1.7817541360855103,
      "learning_rate": 0.00016793650793650794,
      "loss": 0.2924,
      "step": 106
    },
    {
      "epoch": 1.6850393700787403,
      "grad_norm": 3.0749478340148926,
      "learning_rate": 0.00016761904761904763,
      "loss": 0.3451,
      "step": 107
    },
    {
      "epoch": 1.7007874015748032,
      "grad_norm": 1.7081619501113892,
      "learning_rate": 0.00016730158730158732,
      "loss": 0.3539,
      "step": 108
    },
    {
      "epoch": 1.7165354330708662,
      "grad_norm": 1.6943938732147217,
      "learning_rate": 0.000166984126984127,
      "loss": 0.3201,
      "step": 109
    },
    {
      "epoch": 1.7322834645669292,
      "grad_norm": 1.1370619535446167,
      "learning_rate": 0.0001666666666666667,
      "loss": 0.2283,
      "step": 110
    },
    {
      "epoch": 1.7480314960629921,
      "grad_norm": 1.69308602809906,
      "learning_rate": 0.00016634920634920637,
      "loss": 0.3105,
      "step": 111
    },
    {
      "epoch": 1.763779527559055,
      "grad_norm": 2.281278371810913,
      "learning_rate": 0.00016603174603174606,
      "loss": 0.3899,
      "step": 112
    },
    {
      "epoch": 1.779527559055118,
      "grad_norm": 3.026559591293335,
      "learning_rate": 0.00016571428571428575,
      "loss": 0.4495,
      "step": 113
    },
    {
      "epoch": 1.795275590551181,
      "grad_norm": 1.4985885620117188,
      "learning_rate": 0.0001653968253968254,
      "loss": 0.2039,
      "step": 114
    },
    {
      "epoch": 1.811023622047244,
      "grad_norm": 1.718200445175171,
      "learning_rate": 0.0001650793650793651,
      "loss": 0.2902,
      "step": 115
    },
    {
      "epoch": 1.826771653543307,
      "grad_norm": 1.640417218208313,
      "learning_rate": 0.00016476190476190475,
      "loss": 0.3224,
      "step": 116
    },
    {
      "epoch": 1.84251968503937,
      "grad_norm": 2.291710138320923,
      "learning_rate": 0.00016444444444444444,
      "loss": 0.3309,
      "step": 117
    },
    {
      "epoch": 1.858267716535433,
      "grad_norm": 1.6005293130874634,
      "learning_rate": 0.00016412698412698412,
      "loss": 0.2471,
      "step": 118
    },
    {
      "epoch": 1.874015748031496,
      "grad_norm": 2.1404128074645996,
      "learning_rate": 0.0001638095238095238,
      "loss": 0.3385,
      "step": 119
    },
    {
      "epoch": 1.889763779527559,
      "grad_norm": 1.6605455875396729,
      "learning_rate": 0.0001634920634920635,
      "loss": 0.2465,
      "step": 120
    },
    {
      "epoch": 1.905511811023622,
      "grad_norm": 1.3046430349349976,
      "learning_rate": 0.00016317460317460318,
      "loss": 0.1824,
      "step": 121
    },
    {
      "epoch": 1.9212598425196852,
      "grad_norm": 2.416494846343994,
      "learning_rate": 0.00016285714285714287,
      "loss": 0.3709,
      "step": 122
    },
    {
      "epoch": 1.9370078740157481,
      "grad_norm": 2.455259323120117,
      "learning_rate": 0.00016253968253968255,
      "loss": 0.3316,
      "step": 123
    },
    {
      "epoch": 1.952755905511811,
      "grad_norm": 2.0166537761688232,
      "learning_rate": 0.00016222222222222224,
      "loss": 0.3499,
      "step": 124
    },
    {
      "epoch": 1.968503937007874,
      "grad_norm": 1.5614423751831055,
      "learning_rate": 0.00016190476190476192,
      "loss": 0.2738,
      "step": 125
    },
    {
      "epoch": 1.984251968503937,
      "grad_norm": 1.7843226194381714,
      "learning_rate": 0.0001615873015873016,
      "loss": 0.2845,
      "step": 126
    },
    {
      "epoch": 2.0,
      "grad_norm": 1.7972784042358398,
      "learning_rate": 0.00016126984126984127,
      "loss": 0.2842,
      "step": 127
    },
    {
      "epoch": 2.015748031496063,
      "grad_norm": 1.943443775177002,
      "learning_rate": 0.00016095238095238096,
      "loss": 0.3468,
      "step": 128
    },
    {
      "epoch": 2.031496062992126,
      "grad_norm": 1.2804718017578125,
      "learning_rate": 0.00016063492063492064,
      "loss": 0.1669,
      "step": 129
    },
    {
      "epoch": 2.047244094488189,
      "grad_norm": 1.5248326063156128,
      "learning_rate": 0.00016031746031746033,
      "loss": 0.2403,
      "step": 130
    },
    {
      "epoch": 2.062992125984252,
      "grad_norm": 1.9609618186950684,
      "learning_rate": 0.00016,
      "loss": 0.3075,
      "step": 131
    },
    {
      "epoch": 2.078740157480315,
      "grad_norm": 1.588955283164978,
      "learning_rate": 0.0001596825396825397,
      "loss": 0.329,
      "step": 132
    },
    {
      "epoch": 2.094488188976378,
      "grad_norm": 2.324918270111084,
      "learning_rate": 0.00015936507936507936,
      "loss": 0.3182,
      "step": 133
    },
    {
      "epoch": 2.1102362204724407,
      "grad_norm": 2.1407594680786133,
      "learning_rate": 0.00015904761904761904,
      "loss": 0.3061,
      "step": 134
    },
    {
      "epoch": 2.1259842519685037,
      "grad_norm": 1.663182020187378,
      "learning_rate": 0.00015873015873015873,
      "loss": 0.2055,
      "step": 135
    },
    {
      "epoch": 2.141732283464567,
      "grad_norm": 1.6779667139053345,
      "learning_rate": 0.00015841269841269842,
      "loss": 0.2231,
      "step": 136
    },
    {
      "epoch": 2.15748031496063,
      "grad_norm": 1.4505025148391724,
      "learning_rate": 0.0001580952380952381,
      "loss": 0.1901,
      "step": 137
    },
    {
      "epoch": 2.173228346456693,
      "grad_norm": 2.0469698905944824,
      "learning_rate": 0.0001577777777777778,
      "loss": 0.2458,
      "step": 138
    },
    {
      "epoch": 2.188976377952756,
      "grad_norm": 1.9220845699310303,
      "learning_rate": 0.00015746031746031747,
      "loss": 0.1849,
      "step": 139
    },
    {
      "epoch": 2.204724409448819,
      "grad_norm": 2.293099880218506,
      "learning_rate": 0.00015714285714285716,
      "loss": 0.2469,
      "step": 140
    },
    {
      "epoch": 2.220472440944882,
      "grad_norm": 3.2543418407440186,
      "learning_rate": 0.00015682539682539682,
      "loss": 0.2503,
      "step": 141
    },
    {
      "epoch": 2.236220472440945,
      "grad_norm": 2.4092319011688232,
      "learning_rate": 0.0001565079365079365,
      "loss": 0.1802,
      "step": 142
    },
    {
      "epoch": 2.251968503937008,
      "grad_norm": 1.5326480865478516,
      "learning_rate": 0.0001561904761904762,
      "loss": 0.1554,
      "step": 143
    },
    {
      "epoch": 2.267716535433071,
      "grad_norm": 2.8689651489257812,
      "learning_rate": 0.00015587301587301588,
      "loss": 0.3407,
      "step": 144
    },
    {
      "epoch": 2.283464566929134,
      "grad_norm": 1.9364327192306519,
      "learning_rate": 0.00015555555555555556,
      "loss": 0.2124,
      "step": 145
    },
    {
      "epoch": 2.2992125984251968,
      "grad_norm": 1.9560917615890503,
      "learning_rate": 0.00015523809523809525,
      "loss": 0.2339,
      "step": 146
    },
    {
      "epoch": 2.3149606299212597,
      "grad_norm": 1.7388056516647339,
      "learning_rate": 0.00015492063492063494,
      "loss": 0.1949,
      "step": 147
    },
    {
      "epoch": 2.3307086614173227,
      "grad_norm": 2.5475637912750244,
      "learning_rate": 0.00015460317460317462,
      "loss": 0.3096,
      "step": 148
    },
    {
      "epoch": 2.3464566929133857,
      "grad_norm": 1.7157784700393677,
      "learning_rate": 0.0001542857142857143,
      "loss": 0.2021,
      "step": 149
    },
    {
      "epoch": 2.362204724409449,
      "grad_norm": 2.3316924571990967,
      "learning_rate": 0.000153968253968254,
      "loss": 0.3028,
      "step": 150
    },
    {
      "epoch": 2.377952755905512,
      "grad_norm": 1.6654961109161377,
      "learning_rate": 0.00015365079365079368,
      "loss": 0.2176,
      "step": 151
    },
    {
      "epoch": 2.393700787401575,
      "grad_norm": 1.474219799041748,
      "learning_rate": 0.00015333333333333334,
      "loss": 0.1546,
      "step": 152
    },
    {
      "epoch": 2.409448818897638,
      "grad_norm": 2.0945281982421875,
      "learning_rate": 0.00015301587301587302,
      "loss": 0.282,
      "step": 153
    },
    {
      "epoch": 2.425196850393701,
      "grad_norm": 2.152529716491699,
      "learning_rate": 0.00015269841269841268,
      "loss": 0.2231,
      "step": 154
    },
    {
      "epoch": 2.440944881889764,
      "grad_norm": 2.0898220539093018,
      "learning_rate": 0.00015238095238095237,
      "loss": 0.2186,
      "step": 155
    },
    {
      "epoch": 2.456692913385827,
      "grad_norm": 1.7807753086090088,
      "learning_rate": 0.00015206349206349205,
      "loss": 0.1913,
      "step": 156
    },
    {
      "epoch": 2.47244094488189,
      "grad_norm": 2.2556188106536865,
      "learning_rate": 0.00015174603174603174,
      "loss": 0.281,
      "step": 157
    },
    {
      "epoch": 2.4881889763779528,
      "grad_norm": 2.576829433441162,
      "learning_rate": 0.00015142857142857143,
      "loss": 0.349,
      "step": 158
    },
    {
      "epoch": 2.5039370078740157,
      "grad_norm": 2.2945168018341064,
      "learning_rate": 0.0001511111111111111,
      "loss": 0.2599,
      "step": 159
    },
    {
      "epoch": 2.5196850393700787,
      "grad_norm": 2.819474935531616,
      "learning_rate": 0.0001507936507936508,
      "loss": 0.2514,
      "step": 160
    },
    {
      "epoch": 2.5354330708661417,
      "grad_norm": 2.0168161392211914,
      "learning_rate": 0.00015047619047619048,
      "loss": 0.1662,
      "step": 161
    },
    {
      "epoch": 2.5511811023622046,
      "grad_norm": 2.358950138092041,
      "learning_rate": 0.00015015873015873017,
      "loss": 0.2517,
      "step": 162
    },
    {
      "epoch": 2.5669291338582676,
      "grad_norm": 1.672620177268982,
      "learning_rate": 0.00014984126984126986,
      "loss": 0.1752,
      "step": 163
    },
    {
      "epoch": 2.5826771653543306,
      "grad_norm": 2.1740479469299316,
      "learning_rate": 0.00014952380952380954,
      "loss": 0.2471,
      "step": 164
    },
    {
      "epoch": 2.5984251968503935,
      "grad_norm": 2.450913667678833,
      "learning_rate": 0.00014920634920634923,
      "loss": 0.2035,
      "step": 165
    },
    {
      "epoch": 2.6141732283464565,
      "grad_norm": 2.196556806564331,
      "learning_rate": 0.0001488888888888889,
      "loss": 0.2125,
      "step": 166
    },
    {
      "epoch": 2.6299212598425195,
      "grad_norm": 3.2495484352111816,
      "learning_rate": 0.00014857142857142857,
      "loss": 0.2807,
      "step": 167
    },
    {
      "epoch": 2.6456692913385824,
      "grad_norm": 2.038132429122925,
      "learning_rate": 0.00014825396825396826,
      "loss": 0.1834,
      "step": 168
    },
    {
      "epoch": 2.661417322834646,
      "grad_norm": 2.3947300910949707,
      "learning_rate": 0.00014793650793650795,
      "loss": 0.2784,
      "step": 169
    },
    {
      "epoch": 2.677165354330709,
      "grad_norm": 2.5916428565979004,
      "learning_rate": 0.00014761904761904763,
      "loss": 0.3509,
      "step": 170
    },
    {
      "epoch": 2.6929133858267718,
      "grad_norm": 1.858810544013977,
      "learning_rate": 0.00014730158730158732,
      "loss": 0.196,
      "step": 171
    },
    {
      "epoch": 2.7086614173228347,
      "grad_norm": 28.184654235839844,
      "learning_rate": 0.000146984126984127,
      "loss": 0.3207,
      "step": 172
    },
    {
      "epoch": 2.7244094488188977,
      "grad_norm": 2.1424224376678467,
      "learning_rate": 0.00014666666666666666,
      "loss": 0.1967,
      "step": 173
    },
    {
      "epoch": 2.7401574803149606,
      "grad_norm": 2.2528629302978516,
      "learning_rate": 0.00014634920634920635,
      "loss": 0.1961,
      "step": 174
    },
    {
      "epoch": 2.7559055118110236,
      "grad_norm": 3.7821853160858154,
      "learning_rate": 0.00014603174603174603,
      "loss": 0.2725,
      "step": 175
    },
    {
      "epoch": 2.7716535433070866,
      "grad_norm": 3.322559356689453,
      "learning_rate": 0.00014571428571428572,
      "loss": 0.344,
      "step": 176
    },
    {
      "epoch": 2.7874015748031495,
      "grad_norm": 2.652209520339966,
      "learning_rate": 0.0001453968253968254,
      "loss": 0.3209,
      "step": 177
    },
    {
      "epoch": 2.8031496062992125,
      "grad_norm": 2.52654767036438,
      "learning_rate": 0.0001450793650793651,
      "loss": 0.2849,
      "step": 178
    },
    {
      "epoch": 2.8188976377952755,
      "grad_norm": 1.7336770296096802,
      "learning_rate": 0.00014476190476190475,
      "loss": 0.1564,
      "step": 179
    },
    {
      "epoch": 2.8346456692913384,
      "grad_norm": 1.7479006052017212,
      "learning_rate": 0.00014444444444444444,
      "loss": 0.1209,
      "step": 180
    },
    {
      "epoch": 2.850393700787402,
      "grad_norm": 3.0663866996765137,
      "learning_rate": 0.00014412698412698412,
      "loss": 0.1849,
      "step": 181
    },
    {
      "epoch": 2.866141732283465,
      "grad_norm": 2.8720479011535645,
      "learning_rate": 0.0001438095238095238,
      "loss": 0.2925,
      "step": 182
    },
    {
      "epoch": 2.8818897637795278,
      "grad_norm": 2.477109670639038,
      "learning_rate": 0.0001434920634920635,
      "loss": 0.2959,
      "step": 183
    },
    {
      "epoch": 2.8976377952755907,
      "grad_norm": 1.820826530456543,
      "learning_rate": 0.00014317460317460318,
      "loss": 0.1199,
      "step": 184
    },
    {
      "epoch": 2.9133858267716537,
      "grad_norm": 2.1700515747070312,
      "learning_rate": 0.00014285714285714287,
      "loss": 0.1573,
      "step": 185
    },
    {
      "epoch": 2.9291338582677167,
      "grad_norm": 2.80179500579834,
      "learning_rate": 0.00014253968253968255,
      "loss": 0.1939,
      "step": 186
    },
    {
      "epoch": 2.9448818897637796,
      "grad_norm": 2.1033778190612793,
      "learning_rate": 0.00014222222222222224,
      "loss": 0.1987,
      "step": 187
    },
    {
      "epoch": 2.9606299212598426,
      "grad_norm": 2.4610745906829834,
      "learning_rate": 0.00014190476190476193,
      "loss": 0.2355,
      "step": 188
    },
    {
      "epoch": 2.9763779527559056,
      "grad_norm": 2.135178565979004,
      "learning_rate": 0.0001415873015873016,
      "loss": 0.169,
      "step": 189
    },
    {
      "epoch": 2.9921259842519685,
      "grad_norm": 2.1384263038635254,
      "learning_rate": 0.0001412698412698413,
      "loss": 0.2337,
      "step": 190
    },
    {
      "epoch": 3.0078740157480315,
      "grad_norm": 3.036877393722534,
      "learning_rate": 0.00014095238095238096,
      "loss": 0.357,
      "step": 191
    },
    {
      "epoch": 3.0236220472440944,
      "grad_norm": 1.5855189561843872,
      "learning_rate": 0.00014063492063492064,
      "loss": 0.1366,
      "step": 192
    },
    {
      "epoch": 3.0393700787401574,
      "grad_norm": 1.956864356994629,
      "learning_rate": 0.0001403174603174603,
      "loss": 0.1839,
      "step": 193
    },
    {
      "epoch": 3.0551181102362204,
      "grad_norm": 2.270493984222412,
      "learning_rate": 0.00014,
      "loss": 0.2227,
      "step": 194
    },
    {
      "epoch": 3.0708661417322833,
      "grad_norm": 1.9018503427505493,
      "learning_rate": 0.00013968253968253967,
      "loss": 0.1573,
      "step": 195
    },
    {
      "epoch": 3.0866141732283463,
      "grad_norm": 2.1669390201568604,
      "learning_rate": 0.00013936507936507936,
      "loss": 0.1748,
      "step": 196
    },
    {
      "epoch": 3.1023622047244093,
      "grad_norm": 2.231562376022339,
      "learning_rate": 0.00013904761904761905,
      "loss": 0.2187,
      "step": 197
    },
    {
      "epoch": 3.1181102362204722,
      "grad_norm": 2.669992685317993,
      "learning_rate": 0.00013873015873015873,
      "loss": 0.2073,
      "step": 198
    },
    {
      "epoch": 3.1338582677165356,
      "grad_norm": 2.7919487953186035,
      "learning_rate": 0.00013841269841269842,
      "loss": 0.2336,
      "step": 199
    },
    {
      "epoch": 3.1496062992125986,
      "grad_norm": 2.505216360092163,
      "learning_rate": 0.0001380952380952381,
      "loss": 0.1151,
      "step": 200
    },
    {
      "epoch": 3.1653543307086616,
      "grad_norm": 3.073833703994751,
      "learning_rate": 0.0001377777777777778,
      "loss": 0.1132,
      "step": 201
    },
    {
      "epoch": 3.1811023622047245,
      "grad_norm": 3.0617218017578125,
      "learning_rate": 0.00013746031746031748,
      "loss": 0.1968,
      "step": 202
    },
    {
      "epoch": 3.1968503937007875,
      "grad_norm": 2.13476300239563,
      "learning_rate": 0.00013714285714285716,
      "loss": 0.1661,
      "step": 203
    },
    {
      "epoch": 3.2125984251968505,
      "grad_norm": 5.27773904800415,
      "learning_rate": 0.00013682539682539685,
      "loss": 0.163,
      "step": 204
    },
    {
      "epoch": 3.2283464566929134,
      "grad_norm": 2.5170090198516846,
      "learning_rate": 0.0001365079365079365,
      "loss": 0.1466,
      "step": 205
    },
    {
      "epoch": 3.2440944881889764,
      "grad_norm": 3.005295753479004,
      "learning_rate": 0.0001361904761904762,
      "loss": 0.194,
      "step": 206
    },
    {
      "epoch": 3.2598425196850394,
      "grad_norm": 1.7320829629898071,
      "learning_rate": 0.00013587301587301588,
      "loss": 0.0672,
      "step": 207
    },
    {
      "epoch": 3.2755905511811023,
      "grad_norm": 2.5410525798797607,
      "learning_rate": 0.00013555555555555556,
      "loss": 0.1418,
      "step": 208
    },
    {
      "epoch": 3.2913385826771653,
      "grad_norm": 2.5181405544281006,
      "learning_rate": 0.00013523809523809525,
      "loss": 0.1341,
      "step": 209
    },
    {
      "epoch": 3.3070866141732282,
      "grad_norm": 1.864715576171875,
      "learning_rate": 0.00013492063492063494,
      "loss": 0.078,
      "step": 210
    },
    {
      "epoch": 3.322834645669291,
      "grad_norm": 1.9702255725860596,
      "learning_rate": 0.00013460317460317462,
      "loss": 0.1236,
      "step": 211
    },
    {
      "epoch": 3.338582677165354,
      "grad_norm": 1.7311753034591675,
      "learning_rate": 0.00013428571428571428,
      "loss": 0.0951,
      "step": 212
    },
    {
      "epoch": 3.354330708661417,
      "grad_norm": 2.3359055519104004,
      "learning_rate": 0.00013396825396825397,
      "loss": 0.1492,
      "step": 213
    },
    {
      "epoch": 3.3700787401574805,
      "grad_norm": 1.8813738822937012,
      "learning_rate": 0.00013365079365079365,
      "loss": 0.1235,
      "step": 214
    },
    {
      "epoch": 3.3858267716535435,
      "grad_norm": 2.1422250270843506,
      "learning_rate": 0.00013333333333333334,
      "loss": 0.1113,
      "step": 215
    },
    {
      "epoch": 3.4015748031496065,
      "grad_norm": 2.829136371612549,
      "learning_rate": 0.00013301587301587303,
      "loss": 0.2083,
      "step": 216
    },
    {
      "epoch": 3.4173228346456694,
      "grad_norm": 1.8105220794677734,
      "learning_rate": 0.0001326984126984127,
      "loss": 0.0924,
      "step": 217
    },
    {
      "epoch": 3.4330708661417324,
      "grad_norm": 2.928586006164551,
      "learning_rate": 0.00013238095238095237,
      "loss": 0.132,
      "step": 218
    },
    {
      "epoch": 3.4488188976377954,
      "grad_norm": 2.1167008876800537,
      "learning_rate": 0.00013206349206349206,
      "loss": 0.1484,
      "step": 219
    },
    {
      "epoch": 3.4645669291338583,
      "grad_norm": 4.076841354370117,
      "learning_rate": 0.00013174603174603174,
      "loss": 0.1578,
      "step": 220
    },
    {
      "epoch": 3.4803149606299213,
      "grad_norm": 2.301943063735962,
      "learning_rate": 0.00013142857142857143,
      "loss": 0.0697,
      "step": 221
    },
    {
      "epoch": 3.4960629921259843,
      "grad_norm": 2.754974603652954,
      "learning_rate": 0.00013111111111111111,
      "loss": 0.169,
      "step": 222
    },
    {
      "epoch": 3.5118110236220472,
      "grad_norm": 2.303882122039795,
      "learning_rate": 0.0001307936507936508,
      "loss": 0.1512,
      "step": 223
    },
    {
      "epoch": 3.52755905511811,
      "grad_norm": 2.312962055206299,
      "learning_rate": 0.0001304761904761905,
      "loss": 0.1154,
      "step": 224
    },
    {
      "epoch": 3.543307086614173,
      "grad_norm": 2.486600637435913,
      "learning_rate": 0.00013015873015873017,
      "loss": 0.1599,
      "step": 225
    },
    {
      "epoch": 3.559055118110236,
      "grad_norm": 2.260207176208496,
      "learning_rate": 0.00012984126984126986,
      "loss": 0.1241,
      "step": 226
    },
    {
      "epoch": 3.574803149606299,
      "grad_norm": 1.8493692874908447,
      "learning_rate": 0.00012952380952380954,
      "loss": 0.105,
      "step": 227
    },
    {
      "epoch": 3.590551181102362,
      "grad_norm": 2.5485427379608154,
      "learning_rate": 0.00012920634920634923,
      "loss": 0.09,
      "step": 228
    },
    {
      "epoch": 3.606299212598425,
      "grad_norm": 2.1699836254119873,
      "learning_rate": 0.00012888888888888892,
      "loss": 0.1273,
      "step": 229
    },
    {
      "epoch": 3.622047244094488,
      "grad_norm": 2.0470430850982666,
      "learning_rate": 0.00012857142857142858,
      "loss": 0.1076,
      "step": 230
    },
    {
      "epoch": 3.637795275590551,
      "grad_norm": 2.274251699447632,
      "learning_rate": 0.00012825396825396826,
      "loss": 0.1052,
      "step": 231
    },
    {
      "epoch": 3.653543307086614,
      "grad_norm": 1.9981039762496948,
      "learning_rate": 0.00012793650793650792,
      "loss": 0.0997,
      "step": 232
    },
    {
      "epoch": 3.6692913385826773,
      "grad_norm": 3.2653610706329346,
      "learning_rate": 0.0001276190476190476,
      "loss": 0.2182,
      "step": 233
    },
    {
      "epoch": 3.6850393700787403,
      "grad_norm": 3.3873820304870605,
      "learning_rate": 0.0001273015873015873,
      "loss": 0.1988,
      "step": 234
    },
    {
      "epoch": 3.7007874015748032,
      "grad_norm": 2.7968482971191406,
      "learning_rate": 0.00012698412698412698,
      "loss": 0.1768,
      "step": 235
    },
    {
      "epoch": 3.716535433070866,
      "grad_norm": 2.552379608154297,
      "learning_rate": 0.00012666666666666666,
      "loss": 0.1179,
      "step": 236
    },
    {
      "epoch": 3.732283464566929,
      "grad_norm": 2.3711721897125244,
      "learning_rate": 0.00012634920634920635,
      "loss": 0.1091,
      "step": 237
    },
    {
      "epoch": 3.748031496062992,
      "grad_norm": 2.3621528148651123,
      "learning_rate": 0.00012603174603174604,
      "loss": 0.1489,
      "step": 238
    },
    {
      "epoch": 3.763779527559055,
      "grad_norm": 1.7580196857452393,
      "learning_rate": 0.00012571428571428572,
      "loss": 0.0777,
      "step": 239
    },
    {
      "epoch": 3.779527559055118,
      "grad_norm": 3.2354109287261963,
      "learning_rate": 0.0001253968253968254,
      "loss": 0.1481,
      "step": 240
    },
    {
      "epoch": 3.795275590551181,
      "grad_norm": 2.17136549949646,
      "learning_rate": 0.0001250793650793651,
      "loss": 0.1342,
      "step": 241
    },
    {
      "epoch": 3.811023622047244,
      "grad_norm": 2.4584455490112305,
      "learning_rate": 0.00012476190476190478,
      "loss": 0.2402,
      "step": 242
    },
    {
      "epoch": 3.826771653543307,
      "grad_norm": 2.567608594894409,
      "learning_rate": 0.00012444444444444444,
      "loss": 0.169,
      "step": 243
    },
    {
      "epoch": 3.84251968503937,
      "grad_norm": 2.531935453414917,
      "learning_rate": 0.00012412698412698413,
      "loss": 0.2301,
      "step": 244
    },
    {
      "epoch": 3.8582677165354333,
      "grad_norm": 2.378885507583618,
      "learning_rate": 0.0001238095238095238,
      "loss": 0.1512,
      "step": 245
    },
    {
      "epoch": 3.8740157480314963,
      "grad_norm": 2.7570555210113525,
      "learning_rate": 0.0001234920634920635,
      "loss": 0.2114,
      "step": 246
    },
    {
      "epoch": 3.8897637795275593,
      "grad_norm": 2.237010955810547,
      "learning_rate": 0.00012317460317460318,
      "loss": 0.1807,
      "step": 247
    },
    {
      "epoch": 3.905511811023622,
      "grad_norm": 2.587489366531372,
      "learning_rate": 0.00012285714285714287,
      "loss": 0.1545,
      "step": 248
    },
    {
      "epoch": 3.921259842519685,
      "grad_norm": 2.308967113494873,
      "learning_rate": 0.00012253968253968256,
      "loss": 0.1103,
      "step": 249
    },
    {
      "epoch": 3.937007874015748,
      "grad_norm": 2.4161641597747803,
      "learning_rate": 0.00012222222222222224,
      "loss": 0.1349,
      "step": 250
    },
    {
      "epoch": 3.952755905511811,
      "grad_norm": 2.048534870147705,
      "learning_rate": 0.00012190476190476193,
      "loss": 0.1409,
      "step": 251
    },
    {
      "epoch": 3.968503937007874,
      "grad_norm": 2.1610159873962402,
      "learning_rate": 0.00012158730158730159,
      "loss": 0.1298,
      "step": 252
    },
    {
      "epoch": 3.984251968503937,
      "grad_norm": 3.7069857120513916,
      "learning_rate": 0.00012126984126984127,
      "loss": 0.1873,
      "step": 253
    },
    {
      "epoch": 4.0,
      "grad_norm": 2.2359378337860107,
      "learning_rate": 0.00012095238095238095,
      "loss": 0.0982,
      "step": 254
    },
    {
      "epoch": 4.015748031496063,
      "grad_norm": 1.7345545291900635,
      "learning_rate": 0.00012063492063492063,
      "loss": 0.0772,
      "step": 255
    },
    {
      "epoch": 4.031496062992126,
      "grad_norm": 2.5681285858154297,
      "learning_rate": 0.00012031746031746032,
      "loss": 0.1473,
      "step": 256
    },
    {
      "epoch": 4.047244094488189,
      "grad_norm": 2.0769965648651123,
      "learning_rate": 0.00012,
      "loss": 0.0796,
      "step": 257
    },
    {
      "epoch": 4.062992125984252,
      "grad_norm": 2.3896007537841797,
      "learning_rate": 0.00011968253968253969,
      "loss": 0.1071,
      "step": 258
    },
    {
      "epoch": 4.078740157480315,
      "grad_norm": 3.129275321960449,
      "learning_rate": 0.00011936507936507938,
      "loss": 0.1207,
      "step": 259
    },
    {
      "epoch": 4.094488188976378,
      "grad_norm": 2.744035243988037,
      "learning_rate": 0.00011904761904761905,
      "loss": 0.1759,
      "step": 260
    },
    {
      "epoch": 4.110236220472441,
      "grad_norm": 2.1848154067993164,
      "learning_rate": 0.00011873015873015873,
      "loss": 0.1016,
      "step": 261
    },
    {
      "epoch": 4.125984251968504,
      "grad_norm": 1.3345518112182617,
      "learning_rate": 0.00011841269841269842,
      "loss": 0.0519,
      "step": 262
    },
    {
      "epoch": 4.141732283464567,
      "grad_norm": 2.08981990814209,
      "learning_rate": 0.0001180952380952381,
      "loss": 0.1164,
      "step": 263
    },
    {
      "epoch": 4.15748031496063,
      "grad_norm": 1.6878387928009033,
      "learning_rate": 0.00011777777777777779,
      "loss": 0.0571,
      "step": 264
    },
    {
      "epoch": 4.173228346456693,
      "grad_norm": 1.8789550065994263,
      "learning_rate": 0.00011746031746031746,
      "loss": 0.0848,
      "step": 265
    },
    {
      "epoch": 4.188976377952756,
      "grad_norm": 2.5562448501586914,
      "learning_rate": 0.00011714285714285715,
      "loss": 0.0987,
      "step": 266
    },
    {
      "epoch": 4.2047244094488185,
      "grad_norm": 3.43996000289917,
      "learning_rate": 0.00011682539682539684,
      "loss": 0.1513,
      "step": 267
    },
    {
      "epoch": 4.2204724409448815,
      "grad_norm": 5.186434745788574,
      "learning_rate": 0.00011650793650793652,
      "loss": 0.1818,
      "step": 268
    },
    {
      "epoch": 4.2362204724409445,
      "grad_norm": 2.8618242740631104,
      "learning_rate": 0.00011619047619047621,
      "loss": 0.0863,
      "step": 269
    },
    {
      "epoch": 4.251968503937007,
      "grad_norm": 2.451477527618408,
      "learning_rate": 0.0001158730158730159,
      "loss": 0.1008,
      "step": 270
    },
    {
      "epoch": 4.267716535433071,
      "grad_norm": 2.538677930831909,
      "learning_rate": 0.00011555555555555555,
      "loss": 0.0577,
      "step": 271
    },
    {
      "epoch": 4.283464566929134,
      "grad_norm": 2.3942956924438477,
      "learning_rate": 0.00011523809523809524,
      "loss": 0.0729,
      "step": 272
    },
    {
      "epoch": 4.299212598425197,
      "grad_norm": 2.677741765975952,
      "learning_rate": 0.00011492063492063491,
      "loss": 0.0952,
      "step": 273
    },
    {
      "epoch": 4.31496062992126,
      "grad_norm": 2.8582143783569336,
      "learning_rate": 0.0001146031746031746,
      "loss": 0.0569,
      "step": 274
    },
    {
      "epoch": 4.330708661417323,
      "grad_norm": 2.9401369094848633,
      "learning_rate": 0.00011428571428571428,
      "loss": 0.1998,
      "step": 275
    },
    {
      "epoch": 4.346456692913386,
      "grad_norm": 2.267676591873169,
      "learning_rate": 0.00011396825396825397,
      "loss": 0.0852,
      "step": 276
    },
    {
      "epoch": 4.362204724409449,
      "grad_norm": 2.2944488525390625,
      "learning_rate": 0.00011365079365079366,
      "loss": 0.0881,
      "step": 277
    },
    {
      "epoch": 4.377952755905512,
      "grad_norm": 3.8387904167175293,
      "learning_rate": 0.00011333333333333334,
      "loss": 0.1483,
      "step": 278
    },
    {
      "epoch": 4.393700787401575,
      "grad_norm": 2.83909273147583,
      "learning_rate": 0.00011301587301587301,
      "loss": 0.1412,
      "step": 279
    },
    {
      "epoch": 4.409448818897638,
      "grad_norm": 2.0770342350006104,
      "learning_rate": 0.0001126984126984127,
      "loss": 0.0799,
      "step": 280
    },
    {
      "epoch": 4.425196850393701,
      "grad_norm": 2.755004405975342,
      "learning_rate": 0.00011238095238095239,
      "loss": 0.1403,
      "step": 281
    },
    {
      "epoch": 4.440944881889764,
      "grad_norm": 2.8330864906311035,
      "learning_rate": 0.00011206349206349207,
      "loss": 0.1389,
      "step": 282
    },
    {
      "epoch": 4.456692913385827,
      "grad_norm": 1.5948964357376099,
      "learning_rate": 0.00011174603174603176,
      "loss": 0.0647,
      "step": 283
    },
    {
      "epoch": 4.47244094488189,
      "grad_norm": 2.00209641456604,
      "learning_rate": 0.00011142857142857144,
      "loss": 0.1081,
      "step": 284
    },
    {
      "epoch": 4.488188976377953,
      "grad_norm": 1.5841600894927979,
      "learning_rate": 0.00011111111111111112,
      "loss": 0.0632,
      "step": 285
    },
    {
      "epoch": 4.503937007874016,
      "grad_norm": 2.3097243309020996,
      "learning_rate": 0.0001107936507936508,
      "loss": 0.1116,
      "step": 286
    },
    {
      "epoch": 4.519685039370079,
      "grad_norm": 1.4527640342712402,
      "learning_rate": 0.00011047619047619049,
      "loss": 0.0781,
      "step": 287
    },
    {
      "epoch": 4.535433070866142,
      "grad_norm": 1.455188512802124,
      "learning_rate": 0.00011015873015873017,
      "loss": 0.0564,
      "step": 288
    },
    {
      "epoch": 4.551181102362205,
      "grad_norm": 2.404175281524658,
      "learning_rate": 0.00010984126984126986,
      "loss": 0.1014,
      "step": 289
    },
    {
      "epoch": 4.566929133858268,
      "grad_norm": 2.160230875015259,
      "learning_rate": 0.00010952380952380953,
      "loss": 0.1126,
      "step": 290
    },
    {
      "epoch": 4.582677165354331,
      "grad_norm": 2.2459936141967773,
      "learning_rate": 0.0001092063492063492,
      "loss": 0.1127,
      "step": 291
    },
    {
      "epoch": 4.5984251968503935,
      "grad_norm": 2.851334810256958,
      "learning_rate": 0.00010888888888888889,
      "loss": 0.0729,
      "step": 292
    },
    {
      "epoch": 4.6141732283464565,
      "grad_norm": 2.4301764965057373,
      "learning_rate": 0.00010857142857142856,
      "loss": 0.09,
      "step": 293
    },
    {
      "epoch": 4.6299212598425195,
      "grad_norm": 2.299379348754883,
      "learning_rate": 0.00010825396825396825,
      "loss": 0.1033,
      "step": 294
    },
    {
      "epoch": 4.645669291338582,
      "grad_norm": 3.309659719467163,
      "learning_rate": 0.00010793650793650794,
      "loss": 0.1127,
      "step": 295
    },
    {
      "epoch": 4.661417322834645,
      "grad_norm": 3.0536205768585205,
      "learning_rate": 0.00010761904761904762,
      "loss": 0.1811,
      "step": 296
    },
    {
      "epoch": 4.677165354330708,
      "grad_norm": 2.100969076156616,
      "learning_rate": 0.00010730158730158731,
      "loss": 0.0766,
      "step": 297
    },
    {
      "epoch": 4.692913385826771,
      "grad_norm": 2.931156873703003,
      "learning_rate": 0.00010698412698412698,
      "loss": 0.1543,
      "step": 298
    },
    {
      "epoch": 4.708661417322834,
      "grad_norm": 2.1213793754577637,
      "learning_rate": 0.00010666666666666667,
      "loss": 0.102,
      "step": 299
    },
    {
      "epoch": 4.724409448818898,
      "grad_norm": 1.7984623908996582,
      "learning_rate": 0.00010634920634920635,
      "loss": 0.0617,
      "step": 300
    },
    {
      "epoch": 4.740157480314961,
      "grad_norm": 2.2517194747924805,
      "learning_rate": 0.00010603174603174604,
      "loss": 0.0869,
      "step": 301
    },
    {
      "epoch": 4.755905511811024,
      "grad_norm": 4.158993721008301,
      "learning_rate": 0.00010571428571428572,
      "loss": 0.1719,
      "step": 302
    },
    {
      "epoch": 4.771653543307087,
      "grad_norm": 3.3434484004974365,
      "learning_rate": 0.00010539682539682541,
      "loss": 0.1332,
      "step": 303
    },
    {
      "epoch": 4.78740157480315,
      "grad_norm": 4.068320274353027,
      "learning_rate": 0.00010507936507936508,
      "loss": 0.1435,
      "step": 304
    },
    {
      "epoch": 4.803149606299213,
      "grad_norm": 2.987314462661743,
      "learning_rate": 0.00010476190476190477,
      "loss": 0.1199,
      "step": 305
    },
    {
      "epoch": 4.818897637795276,
      "grad_norm": 3.0690412521362305,
      "learning_rate": 0.00010444444444444445,
      "loss": 0.13,
      "step": 306
    },
    {
      "epoch": 4.834645669291339,
      "grad_norm": 2.723945379257202,
      "learning_rate": 0.00010412698412698414,
      "loss": 0.0999,
      "step": 307
    },
    {
      "epoch": 4.850393700787402,
      "grad_norm": 1.9285935163497925,
      "learning_rate": 0.00010380952380952383,
      "loss": 0.0797,
      "step": 308
    },
    {
      "epoch": 4.866141732283465,
      "grad_norm": 2.920489549636841,
      "learning_rate": 0.00010349206349206351,
      "loss": 0.1398,
      "step": 309
    },
    {
      "epoch": 4.881889763779528,
      "grad_norm": 2.3432583808898926,
      "learning_rate": 0.00010317460317460319,
      "loss": 0.1486,
      "step": 310
    },
    {
      "epoch": 4.897637795275591,
      "grad_norm": 2.608624219894409,
      "learning_rate": 0.00010285714285714286,
      "loss": 0.1534,
      "step": 311
    },
    {
      "epoch": 4.913385826771654,
      "grad_norm": 1.3617128133773804,
      "learning_rate": 0.00010253968253968253,
      "loss": 0.0553,
      "step": 312
    },
    {
      "epoch": 4.929133858267717,
      "grad_norm": 1.9522525072097778,
      "learning_rate": 0.00010222222222222222,
      "loss": 0.1257,
      "step": 313
    },
    {
      "epoch": 4.94488188976378,
      "grad_norm": 2.691802978515625,
      "learning_rate": 0.0001019047619047619,
      "loss": 0.1596,
      "step": 314
    },
    {
      "epoch": 4.960629921259843,
      "grad_norm": 2.2672581672668457,
      "learning_rate": 0.00010158730158730159,
      "loss": 0.1125,
      "step": 315
    },
    {
      "epoch": 4.9763779527559056,
      "grad_norm": 2.719090700149536,
      "learning_rate": 0.00010126984126984127,
      "loss": 0.1105,
      "step": 316
    },
    {
      "epoch": 4.9921259842519685,
      "grad_norm": 2.5638182163238525,
      "learning_rate": 0.00010095238095238096,
      "loss": 0.1371,
      "step": 317
    },
    {
      "epoch": 5.0078740157480315,
      "grad_norm": 1.7504723072052002,
      "learning_rate": 0.00010063492063492063,
      "loss": 0.0796,
      "step": 318
    },
    {
      "epoch": 5.0236220472440944,
      "grad_norm": 1.8346753120422363,
      "learning_rate": 0.00010031746031746032,
      "loss": 0.0749,
      "step": 319
    },
    {
      "epoch": 5.039370078740157,
      "grad_norm": 2.038266897201538,
      "learning_rate": 0.0001,
      "loss": 0.0781,
      "step": 320
    },
    {
      "epoch": 5.05511811023622,
      "grad_norm": 2.571553945541382,
      "learning_rate": 9.968253968253969e-05,
      "loss": 0.0957,
      "step": 321
    },
    {
      "epoch": 5.070866141732283,
      "grad_norm": 2.0144317150115967,
      "learning_rate": 9.936507936507938e-05,
      "loss": 0.0825,
      "step": 322
    },
    {
      "epoch": 5.086614173228346,
      "grad_norm": 2.0049824714660645,
      "learning_rate": 9.904761904761905e-05,
      "loss": 0.0552,
      "step": 323
    },
    {
      "epoch": 5.102362204724409,
      "grad_norm": 3.8221776485443115,
      "learning_rate": 9.873015873015874e-05,
      "loss": 0.1535,
      "step": 324
    },
    {
      "epoch": 5.118110236220472,
      "grad_norm": 1.7345818281173706,
      "learning_rate": 9.841269841269841e-05,
      "loss": 0.08,
      "step": 325
    },
    {
      "epoch": 5.133858267716535,
      "grad_norm": 1.6578491926193237,
      "learning_rate": 9.80952380952381e-05,
      "loss": 0.0474,
      "step": 326
    },
    {
      "epoch": 5.149606299212598,
      "grad_norm": 2.7101285457611084,
      "learning_rate": 9.777777777777778e-05,
      "loss": 0.091,
      "step": 327
    },
    {
      "epoch": 5.165354330708661,
      "grad_norm": 2.927738904953003,
      "learning_rate": 9.746031746031747e-05,
      "loss": 0.1035,
      "step": 328
    },
    {
      "epoch": 5.181102362204724,
      "grad_norm": 2.1340513229370117,
      "learning_rate": 9.714285714285715e-05,
      "loss": 0.0596,
      "step": 329
    },
    {
      "epoch": 5.196850393700787,
      "grad_norm": 1.978243350982666,
      "learning_rate": 9.682539682539682e-05,
      "loss": 0.0789,
      "step": 330
    },
    {
      "epoch": 5.21259842519685,
      "grad_norm": 2.247227668762207,
      "learning_rate": 9.650793650793651e-05,
      "loss": 0.0605,
      "step": 331
    },
    {
      "epoch": 5.228346456692913,
      "grad_norm": 2.3290998935699463,
      "learning_rate": 9.61904761904762e-05,
      "loss": 0.0646,
      "step": 332
    },
    {
      "epoch": 5.244094488188976,
      "grad_norm": 3.161952495574951,
      "learning_rate": 9.587301587301588e-05,
      "loss": 0.1059,
      "step": 333
    },
    {
      "epoch": 5.259842519685039,
      "grad_norm": 2.387181520462036,
      "learning_rate": 9.555555555555557e-05,
      "loss": 0.0652,
      "step": 334
    },
    {
      "epoch": 5.275590551181103,
      "grad_norm": 2.982703685760498,
      "learning_rate": 9.523809523809524e-05,
      "loss": 0.1127,
      "step": 335
    },
    {
      "epoch": 5.291338582677166,
      "grad_norm": 2.8215324878692627,
      "learning_rate": 9.492063492063493e-05,
      "loss": 0.107,
      "step": 336
    },
    {
      "epoch": 5.307086614173229,
      "grad_norm": 3.458400011062622,
      "learning_rate": 9.46031746031746e-05,
      "loss": 0.1004,
      "step": 337
    },
    {
      "epoch": 5.322834645669292,
      "grad_norm": 2.8570799827575684,
      "learning_rate": 9.428571428571429e-05,
      "loss": 0.1228,
      "step": 338
    },
    {
      "epoch": 5.338582677165355,
      "grad_norm": 2.8397457599639893,
      "learning_rate": 9.396825396825397e-05,
      "loss": 0.1158,
      "step": 339
    },
    {
      "epoch": 5.354330708661418,
      "grad_norm": 1.8148747682571411,
      "learning_rate": 9.365079365079366e-05,
      "loss": 0.0501,
      "step": 340
    },
    {
      "epoch": 5.3700787401574805,
      "grad_norm": 2.0934431552886963,
      "learning_rate": 9.333333333333334e-05,
      "loss": 0.082,
      "step": 341
    },
    {
      "epoch": 5.3858267716535435,
      "grad_norm": 2.094406843185425,
      "learning_rate": 9.301587301587303e-05,
      "loss": 0.0783,
      "step": 342
    },
    {
      "epoch": 5.4015748031496065,
      "grad_norm": 3.2693967819213867,
      "learning_rate": 9.26984126984127e-05,
      "loss": 0.098,
      "step": 343
    },
    {
      "epoch": 5.417322834645669,
      "grad_norm": 2.16178035736084,
      "learning_rate": 9.238095238095239e-05,
      "loss": 0.0666,
      "step": 344
    },
    {
      "epoch": 5.433070866141732,
      "grad_norm": 3.1209566593170166,
      "learning_rate": 9.206349206349206e-05,
      "loss": 0.1282,
      "step": 345
    },
    {
      "epoch": 5.448818897637795,
      "grad_norm": 2.8764946460723877,
      "learning_rate": 9.174603174603175e-05,
      "loss": 0.0912,
      "step": 346
    },
    {
      "epoch": 5.464566929133858,
      "grad_norm": 2.6075356006622314,
      "learning_rate": 9.142857142857143e-05,
      "loss": 0.1074,
      "step": 347
    },
    {
      "epoch": 5.480314960629921,
      "grad_norm": 1.6161152124404907,
      "learning_rate": 9.111111111111112e-05,
      "loss": 0.0559,
      "step": 348
    },
    {
      "epoch": 5.496062992125984,
      "grad_norm": 2.4605462551116943,
      "learning_rate": 9.079365079365079e-05,
      "loss": 0.106,
      "step": 349
    },
    {
      "epoch": 5.511811023622047,
      "grad_norm": 2.787132740020752,
      "learning_rate": 9.047619047619048e-05,
      "loss": 0.091,
      "step": 350
    },
    {
      "epoch": 5.52755905511811,
      "grad_norm": 3.079501152038574,
      "learning_rate": 9.015873015873016e-05,
      "loss": 0.0789,
      "step": 351
    },
    {
      "epoch": 5.543307086614173,
      "grad_norm": 2.2224392890930176,
      "learning_rate": 8.984126984126985e-05,
      "loss": 0.0777,
      "step": 352
    },
    {
      "epoch": 5.559055118110236,
      "grad_norm": 2.0627851486206055,
      "learning_rate": 8.952380952380953e-05,
      "loss": 0.0745,
      "step": 353
    },
    {
      "epoch": 5.574803149606299,
      "grad_norm": 2.2175848484039307,
      "learning_rate": 8.920634920634922e-05,
      "loss": 0.0971,
      "step": 354
    },
    {
      "epoch": 5.590551181102362,
      "grad_norm": 2.9124791622161865,
      "learning_rate": 8.888888888888889e-05,
      "loss": 0.0808,
      "step": 355
    },
    {
      "epoch": 5.606299212598425,
      "grad_norm": 2.715285301208496,
      "learning_rate": 8.857142857142857e-05,
      "loss": 0.0903,
      "step": 356
    },
    {
      "epoch": 5.622047244094488,
      "grad_norm": 2.1868250370025635,
      "learning_rate": 8.825396825396825e-05,
      "loss": 0.0654,
      "step": 357
    },
    {
      "epoch": 5.637795275590551,
      "grad_norm": 3.12479305267334,
      "learning_rate": 8.793650793650794e-05,
      "loss": 0.1265,
      "step": 358
    },
    {
      "epoch": 5.653543307086614,
      "grad_norm": 3.839902400970459,
      "learning_rate": 8.761904761904762e-05,
      "loss": 0.0947,
      "step": 359
    },
    {
      "epoch": 5.669291338582677,
      "grad_norm": 2.2247307300567627,
      "learning_rate": 8.730158730158731e-05,
      "loss": 0.0709,
      "step": 360
    },
    {
      "epoch": 5.68503937007874,
      "grad_norm": 4.4834394454956055,
      "learning_rate": 8.6984126984127e-05,
      "loss": 0.12,
      "step": 361
    },
    {
      "epoch": 5.700787401574803,
      "grad_norm": 2.0977816581726074,
      "learning_rate": 8.666666666666667e-05,
      "loss": 0.0552,
      "step": 362
    },
    {
      "epoch": 5.716535433070866,
      "grad_norm": 2.1505539417266846,
      "learning_rate": 8.634920634920635e-05,
      "loss": 0.0604,
      "step": 363
    },
    {
      "epoch": 5.73228346456693,
      "grad_norm": 2.49243426322937,
      "learning_rate": 8.603174603174604e-05,
      "loss": 0.0845,
      "step": 364
    },
    {
      "epoch": 5.748031496062993,
      "grad_norm": 2.6426658630371094,
      "learning_rate": 8.571428571428571e-05,
      "loss": 0.1005,
      "step": 365
    },
    {
      "epoch": 5.7637795275590555,
      "grad_norm": 1.2246333360671997,
      "learning_rate": 8.53968253968254e-05,
      "loss": 0.0432,
      "step": 366
    },
    {
      "epoch": 5.7795275590551185,
      "grad_norm": 2.7640161514282227,
      "learning_rate": 8.507936507936508e-05,
      "loss": 0.0947,
      "step": 367
    },
    {
      "epoch": 5.7952755905511815,
      "grad_norm": 2.7801103591918945,
      "learning_rate": 8.476190476190477e-05,
      "loss": 0.0871,
      "step": 368
    },
    {
      "epoch": 5.811023622047244,
      "grad_norm": 2.6730072498321533,
      "learning_rate": 8.444444444444444e-05,
      "loss": 0.0997,
      "step": 369
    },
    {
      "epoch": 5.826771653543307,
      "grad_norm": 1.521018624305725,
      "learning_rate": 8.412698412698413e-05,
      "loss": 0.044,
      "step": 370
    },
    {
      "epoch": 5.84251968503937,
      "grad_norm": 1.2006453275680542,
      "learning_rate": 8.380952380952382e-05,
      "loss": 0.0372,
      "step": 371
    },
    {
      "epoch": 5.858267716535433,
      "grad_norm": 3.046778917312622,
      "learning_rate": 8.34920634920635e-05,
      "loss": 0.0997,
      "step": 372
    },
    {
      "epoch": 5.874015748031496,
      "grad_norm": 3.0756680965423584,
      "learning_rate": 8.317460317460319e-05,
      "loss": 0.1041,
      "step": 373
    },
    {
      "epoch": 5.889763779527559,
      "grad_norm": 3.759709596633911,
      "learning_rate": 8.285714285714287e-05,
      "loss": 0.1312,
      "step": 374
    },
    {
      "epoch": 5.905511811023622,
      "grad_norm": 4.06998872756958,
      "learning_rate": 8.253968253968255e-05,
      "loss": 0.0904,
      "step": 375
    },
    {
      "epoch": 5.921259842519685,
      "grad_norm": 2.1585495471954346,
      "learning_rate": 8.222222222222222e-05,
      "loss": 0.0763,
      "step": 376
    },
    {
      "epoch": 5.937007874015748,
      "grad_norm": 2.6719541549682617,
      "learning_rate": 8.19047619047619e-05,
      "loss": 0.077,
      "step": 377
    },
    {
      "epoch": 5.952755905511811,
      "grad_norm": 2.232661485671997,
      "learning_rate": 8.158730158730159e-05,
      "loss": 0.0739,
      "step": 378
    },
    {
      "epoch": 5.968503937007874,
      "grad_norm": 2.5117149353027344,
      "learning_rate": 8.126984126984128e-05,
      "loss": 0.0832,
      "step": 379
    },
    {
      "epoch": 5.984251968503937,
      "grad_norm": 2.941861867904663,
      "learning_rate": 8.095238095238096e-05,
      "loss": 0.1328,
      "step": 380
    },
    {
      "epoch": 6.0,
      "grad_norm": 1.6214423179626465,
      "learning_rate": 8.063492063492063e-05,
      "loss": 0.0553,
      "step": 381
    },
    {
      "epoch": 6.015748031496063,
      "grad_norm": 1.11016845703125,
      "learning_rate": 8.031746031746032e-05,
      "loss": 0.0402,
      "step": 382
    },
    {
      "epoch": 6.031496062992126,
      "grad_norm": 2.040609121322632,
      "learning_rate": 8e-05,
      "loss": 0.0676,
      "step": 383
    },
    {
      "epoch": 6.047244094488189,
      "grad_norm": 1.8848873376846313,
      "learning_rate": 7.968253968253968e-05,
      "loss": 0.0689,
      "step": 384
    },
    {
      "epoch": 6.062992125984252,
      "grad_norm": 1.8620370626449585,
      "learning_rate": 7.936507936507937e-05,
      "loss": 0.0755,
      "step": 385
    },
    {
      "epoch": 6.078740157480315,
      "grad_norm": 3.7586655616760254,
      "learning_rate": 7.904761904761905e-05,
      "loss": 0.0928,
      "step": 386
    },
    {
      "epoch": 6.094488188976378,
      "grad_norm": 4.334752082824707,
      "learning_rate": 7.873015873015874e-05,
      "loss": 0.0706,
      "step": 387
    },
    {
      "epoch": 6.110236220472441,
      "grad_norm": 1.8674089908599854,
      "learning_rate": 7.841269841269841e-05,
      "loss": 0.0635,
      "step": 388
    },
    {
      "epoch": 6.125984251968504,
      "grad_norm": 2.1795570850372314,
      "learning_rate": 7.80952380952381e-05,
      "loss": 0.0648,
      "step": 389
    },
    {
      "epoch": 6.141732283464567,
      "grad_norm": 1.2410826683044434,
      "learning_rate": 7.777777777777778e-05,
      "loss": 0.0419,
      "step": 390
    },
    {
      "epoch": 6.15748031496063,
      "grad_norm": 2.1392593383789062,
      "learning_rate": 7.746031746031747e-05,
      "loss": 0.0616,
      "step": 391
    },
    {
      "epoch": 6.173228346456693,
      "grad_norm": 1.4880917072296143,
      "learning_rate": 7.714285714285715e-05,
      "loss": 0.0365,
      "step": 392
    },
    {
      "epoch": 6.188976377952756,
      "grad_norm": 2.2227964401245117,
      "learning_rate": 7.682539682539684e-05,
      "loss": 0.0714,
      "step": 393
    },
    {
      "epoch": 6.2047244094488185,
      "grad_norm": 1.9290335178375244,
      "learning_rate": 7.650793650793651e-05,
      "loss": 0.0399,
      "step": 394
    },
    {
      "epoch": 6.2204724409448815,
      "grad_norm": 3.7631077766418457,
      "learning_rate": 7.619047619047618e-05,
      "loss": 0.1159,
      "step": 395
    },
    {
      "epoch": 6.2362204724409445,
      "grad_norm": 3.2462539672851562,
      "learning_rate": 7.587301587301587e-05,
      "loss": 0.0836,
      "step": 396
    },
    {
      "epoch": 6.251968503937007,
      "grad_norm": 2.5382208824157715,
      "learning_rate": 7.555555555555556e-05,
      "loss": 0.0568,
      "step": 397
    },
    {
      "epoch": 6.267716535433071,
      "grad_norm": 2.3497397899627686,
      "learning_rate": 7.523809523809524e-05,
      "loss": 0.0726,
      "step": 398
    },
    {
      "epoch": 6.283464566929134,
      "grad_norm": 1.7231727838516235,
      "learning_rate": 7.492063492063493e-05,
      "loss": 0.0494,
      "step": 399
    },
    {
      "epoch": 6.299212598425197,
      "grad_norm": 3.016780376434326,
      "learning_rate": 7.460317460317461e-05,
      "loss": 0.0838,
      "step": 400
    },
    {
      "epoch": 6.31496062992126,
      "grad_norm": 2.2743008136749268,
      "learning_rate": 7.428571428571429e-05,
      "loss": 0.0673,
      "step": 401
    },
    {
      "epoch": 6.330708661417323,
      "grad_norm": 2.0253746509552,
      "learning_rate": 7.396825396825397e-05,
      "loss": 0.0607,
      "step": 402
    },
    {
      "epoch": 6.346456692913386,
      "grad_norm": 3.311447858810425,
      "learning_rate": 7.365079365079366e-05,
      "loss": 0.0842,
      "step": 403
    },
    {
      "epoch": 6.362204724409449,
      "grad_norm": 4.037237644195557,
      "learning_rate": 7.333333333333333e-05,
      "loss": 0.108,
      "step": 404
    },
    {
      "epoch": 6.377952755905512,
      "grad_norm": 1.998012661933899,
      "learning_rate": 7.301587301587302e-05,
      "loss": 0.0686,
      "step": 405
    },
    {
      "epoch": 6.393700787401575,
      "grad_norm": 4.830350399017334,
      "learning_rate": 7.26984126984127e-05,
      "loss": 0.1163,
      "step": 406
    },
    {
      "epoch": 6.409448818897638,
      "grad_norm": 1.7389729022979736,
      "learning_rate": 7.238095238095238e-05,
      "loss": 0.055,
      "step": 407
    },
    {
      "epoch": 6.425196850393701,
      "grad_norm": 1.7543238401412964,
      "learning_rate": 7.206349206349206e-05,
      "loss": 0.056,
      "step": 408
    },
    {
      "epoch": 6.440944881889764,
      "grad_norm": 1.396700382232666,
      "learning_rate": 7.174603174603175e-05,
      "loss": 0.0519,
      "step": 409
    },
    {
      "epoch": 6.456692913385827,
      "grad_norm": 2.58795428276062,
      "learning_rate": 7.142857142857143e-05,
      "loss": 0.0688,
      "step": 410
    },
    {
      "epoch": 6.47244094488189,
      "grad_norm": 1.4508365392684937,
      "learning_rate": 7.111111111111112e-05,
      "loss": 0.0377,
      "step": 411
    },
    {
      "epoch": 6.488188976377953,
      "grad_norm": 3.298555612564087,
      "learning_rate": 7.07936507936508e-05,
      "loss": 0.0995,
      "step": 412
    },
    {
      "epoch": 6.503937007874016,
      "grad_norm": 2.3193650245666504,
      "learning_rate": 7.047619047619048e-05,
      "loss": 0.0699,
      "step": 413
    },
    {
      "epoch": 6.519685039370079,
      "grad_norm": 7.11144495010376,
      "learning_rate": 7.015873015873015e-05,
      "loss": 0.1092,
      "step": 414
    },
    {
      "epoch": 6.535433070866142,
      "grad_norm": 1.8318068981170654,
      "learning_rate": 6.984126984126984e-05,
      "loss": 0.0613,
      "step": 415
    },
    {
      "epoch": 6.551181102362205,
      "grad_norm": 0.9665482640266418,
      "learning_rate": 6.952380952380952e-05,
      "loss": 0.0376,
      "step": 416
    },
    {
      "epoch": 6.566929133858268,
      "grad_norm": 3.8483638763427734,
      "learning_rate": 6.920634920634921e-05,
      "loss": 0.1132,
      "step": 417
    },
    {
      "epoch": 6.582677165354331,
      "grad_norm": 3.121279001235962,
      "learning_rate": 6.88888888888889e-05,
      "loss": 0.0888,
      "step": 418
    },
    {
      "epoch": 6.5984251968503935,
      "grad_norm": 2.832247734069824,
      "learning_rate": 6.857142857142858e-05,
      "loss": 0.0951,
      "step": 419
    },
    {
      "epoch": 6.6141732283464565,
      "grad_norm": 2.081009864807129,
      "learning_rate": 6.825396825396825e-05,
      "loss": 0.0556,
      "step": 420
    },
    {
      "epoch": 6.6299212598425195,
      "grad_norm": 2.9731974601745605,
      "learning_rate": 6.793650793650794e-05,
      "loss": 0.071,
      "step": 421
    },
    {
      "epoch": 6.645669291338582,
      "grad_norm": 4.3703508377075195,
      "learning_rate": 6.761904761904763e-05,
      "loss": 0.0858,
      "step": 422
    },
    {
      "epoch": 6.661417322834645,
      "grad_norm": 3.73081636428833,
      "learning_rate": 6.730158730158731e-05,
      "loss": 0.0845,
      "step": 423
    },
    {
      "epoch": 6.677165354330708,
      "grad_norm": 1.5303919315338135,
      "learning_rate": 6.698412698412698e-05,
      "loss": 0.0385,
      "step": 424
    },
    {
      "epoch": 6.692913385826771,
      "grad_norm": 1.83735191822052,
      "learning_rate": 6.666666666666667e-05,
      "loss": 0.0503,
      "step": 425
    },
    {
      "epoch": 6.708661417322834,
      "grad_norm": 2.5308382511138916,
      "learning_rate": 6.634920634920636e-05,
      "loss": 0.0722,
      "step": 426
    },
    {
      "epoch": 6.724409448818898,
      "grad_norm": 1.5730465650558472,
      "learning_rate": 6.603174603174603e-05,
      "loss": 0.0434,
      "step": 427
    },
    {
      "epoch": 6.740157480314961,
      "grad_norm": 1.940611720085144,
      "learning_rate": 6.571428571428571e-05,
      "loss": 0.0818,
      "step": 428
    },
    {
      "epoch": 6.755905511811024,
      "grad_norm": 2.6083528995513916,
      "learning_rate": 6.53968253968254e-05,
      "loss": 0.0901,
      "step": 429
    },
    {
      "epoch": 6.771653543307087,
      "grad_norm": 3.288074254989624,
      "learning_rate": 6.507936507936509e-05,
      "loss": 0.0613,
      "step": 430
    },
    {
      "epoch": 6.78740157480315,
      "grad_norm": 1.983947992324829,
      "learning_rate": 6.476190476190477e-05,
      "loss": 0.0636,
      "step": 431
    },
    {
      "epoch": 6.803149606299213,
      "grad_norm": 1.5501693487167358,
      "learning_rate": 6.444444444444446e-05,
      "loss": 0.0391,
      "step": 432
    },
    {
      "epoch": 6.818897637795276,
      "grad_norm": 2.0409324169158936,
      "learning_rate": 6.412698412698413e-05,
      "loss": 0.0661,
      "step": 433
    },
    {
      "epoch": 6.834645669291339,
      "grad_norm": 2.4924938678741455,
      "learning_rate": 6.38095238095238e-05,
      "loss": 0.0672,
      "step": 434
    },
    {
      "epoch": 6.850393700787402,
      "grad_norm": 2.916896343231201,
      "learning_rate": 6.349206349206349e-05,
      "loss": 0.0917,
      "step": 435
    },
    {
      "epoch": 6.866141732283465,
      "grad_norm": 3.998598098754883,
      "learning_rate": 6.317460317460318e-05,
      "loss": 0.1073,
      "step": 436
    },
    {
      "epoch": 6.881889763779528,
      "grad_norm": 2.0622780323028564,
      "learning_rate": 6.285714285714286e-05,
      "loss": 0.0647,
      "step": 437
    },
    {
      "epoch": 6.897637795275591,
      "grad_norm": 2.029952049255371,
      "learning_rate": 6.253968253968255e-05,
      "loss": 0.0725,
      "step": 438
    },
    {
      "epoch": 6.913385826771654,
      "grad_norm": 1.9981027841567993,
      "learning_rate": 6.222222222222222e-05,
      "loss": 0.0554,
      "step": 439
    },
    {
      "epoch": 6.929133858267717,
      "grad_norm": 3.094128131866455,
      "learning_rate": 6.19047619047619e-05,
      "loss": 0.066,
      "step": 440
    },
    {
      "epoch": 6.94488188976378,
      "grad_norm": 2.581409215927124,
      "learning_rate": 6.158730158730159e-05,
      "loss": 0.0898,
      "step": 441
    },
    {
      "epoch": 6.960629921259843,
      "grad_norm": 1.7877960205078125,
      "learning_rate": 6.126984126984128e-05,
      "loss": 0.07,
      "step": 442
    },
    {
      "epoch": 6.9763779527559056,
      "grad_norm": 3.422689437866211,
      "learning_rate": 6.0952380952380964e-05,
      "loss": 0.0775,
      "step": 443
    },
    {
      "epoch": 6.9921259842519685,
      "grad_norm": 2.2832536697387695,
      "learning_rate": 6.0634920634920636e-05,
      "loss": 0.0534,
      "step": 444
    },
    {
      "epoch": 7.0078740157480315,
      "grad_norm": 2.1231939792633057,
      "learning_rate": 6.0317460317460316e-05,
      "loss": 0.0603,
      "step": 445
    },
    {
      "epoch": 7.0236220472440944,
      "grad_norm": 1.6743394136428833,
      "learning_rate": 6e-05,
      "loss": 0.0552,
      "step": 446
    },
    {
      "epoch": 7.039370078740157,
      "grad_norm": 2.504108190536499,
      "learning_rate": 5.968253968253969e-05,
      "loss": 0.0558,
      "step": 447
    },
    {
      "epoch": 7.05511811023622,
      "grad_norm": 2.112156391143799,
      "learning_rate": 5.936507936507937e-05,
      "loss": 0.0447,
      "step": 448
    },
    {
      "epoch": 7.070866141732283,
      "grad_norm": 1.6711369752883911,
      "learning_rate": 5.904761904761905e-05,
      "loss": 0.039,
      "step": 449
    },
    {
      "epoch": 7.086614173228346,
      "grad_norm": 2.5084762573242188,
      "learning_rate": 5.873015873015873e-05,
      "loss": 0.056,
      "step": 450
    },
    {
      "epoch": 7.102362204724409,
      "grad_norm": 2.7744224071502686,
      "learning_rate": 5.841269841269842e-05,
      "loss": 0.0754,
      "step": 451
    },
    {
      "epoch": 7.118110236220472,
      "grad_norm": 1.6176737546920776,
      "learning_rate": 5.8095238095238104e-05,
      "loss": 0.0479,
      "step": 452
    },
    {
      "epoch": 7.133858267716535,
      "grad_norm": 1.0286383628845215,
      "learning_rate": 5.7777777777777776e-05,
      "loss": 0.0317,
      "step": 453
    },
    {
      "epoch": 7.149606299212598,
      "grad_norm": 2.541877031326294,
      "learning_rate": 5.7460317460317456e-05,
      "loss": 0.0844,
      "step": 454
    },
    {
      "epoch": 7.165354330708661,
      "grad_norm": 1.9036186933517456,
      "learning_rate": 5.714285714285714e-05,
      "loss": 0.0589,
      "step": 455
    },
    {
      "epoch": 7.181102362204724,
      "grad_norm": 2.733015298843384,
      "learning_rate": 5.682539682539683e-05,
      "loss": 0.0553,
      "step": 456
    },
    {
      "epoch": 7.196850393700787,
      "grad_norm": 3.2294533252716064,
      "learning_rate": 5.650793650793651e-05,
      "loss": 0.058,
      "step": 457
    },
    {
      "epoch": 7.21259842519685,
      "grad_norm": 2.59871244430542,
      "learning_rate": 5.619047619047619e-05,
      "loss": 0.0665,
      "step": 458
    },
    {
      "epoch": 7.228346456692913,
      "grad_norm": 1.8687788248062134,
      "learning_rate": 5.587301587301588e-05,
      "loss": 0.0505,
      "step": 459
    },
    {
      "epoch": 7.244094488188976,
      "grad_norm": 1.479793667793274,
      "learning_rate": 5.555555555555556e-05,
      "loss": 0.0354,
      "step": 460
    },
    {
      "epoch": 7.259842519685039,
      "grad_norm": 2.265528440475464,
      "learning_rate": 5.5238095238095244e-05,
      "loss": 0.0674,
      "step": 461
    },
    {
      "epoch": 7.275590551181103,
      "grad_norm": 1.2904274463653564,
      "learning_rate": 5.492063492063493e-05,
      "loss": 0.0359,
      "step": 462
    },
    {
      "epoch": 7.291338582677166,
      "grad_norm": 1.8576534986495972,
      "learning_rate": 5.46031746031746e-05,
      "loss": 0.0425,
      "step": 463
    },
    {
      "epoch": 7.307086614173229,
      "grad_norm": 2.383652687072754,
      "learning_rate": 5.428571428571428e-05,
      "loss": 0.0562,
      "step": 464
    },
    {
      "epoch": 7.322834645669292,
      "grad_norm": 2.1850521564483643,
      "learning_rate": 5.396825396825397e-05,
      "loss": 0.0463,
      "step": 465
    },
    {
      "epoch": 7.338582677165355,
      "grad_norm": 2.2176506519317627,
      "learning_rate": 5.3650793650793654e-05,
      "loss": 0.0517,
      "step": 466
    },
    {
      "epoch": 7.354330708661418,
      "grad_norm": 2.0646283626556396,
      "learning_rate": 5.333333333333333e-05,
      "loss": 0.0514,
      "step": 467
    },
    {
      "epoch": 7.3700787401574805,
      "grad_norm": 2.710444450378418,
      "learning_rate": 5.301587301587302e-05,
      "loss": 0.0515,
      "step": 468
    },
    {
      "epoch": 7.3858267716535435,
      "grad_norm": 2.384864568710327,
      "learning_rate": 5.2698412698412705e-05,
      "loss": 0.0365,
      "step": 469
    },
    {
      "epoch": 7.4015748031496065,
      "grad_norm": 2.597791910171509,
      "learning_rate": 5.2380952380952384e-05,
      "loss": 0.0834,
      "step": 470
    },
    {
      "epoch": 7.417322834645669,
      "grad_norm": 3.4892995357513428,
      "learning_rate": 5.206349206349207e-05,
      "loss": 0.0926,
      "step": 471
    },
    {
      "epoch": 7.433070866141732,
      "grad_norm": 0.9745161533355713,
      "learning_rate": 5.1746031746031756e-05,
      "loss": 0.0333,
      "step": 472
    },
    {
      "epoch": 7.448818897637795,
      "grad_norm": 3.166733741760254,
      "learning_rate": 5.142857142857143e-05,
      "loss": 0.0748,
      "step": 473
    },
    {
      "epoch": 7.464566929133858,
      "grad_norm": 1.8379173278808594,
      "learning_rate": 5.111111111111111e-05,
      "loss": 0.0564,
      "step": 474
    },
    {
      "epoch": 7.480314960629921,
      "grad_norm": 2.1740925312042236,
      "learning_rate": 5.0793650793650794e-05,
      "loss": 0.0573,
      "step": 475
    },
    {
      "epoch": 7.496062992125984,
      "grad_norm": 1.0810235738754272,
      "learning_rate": 5.047619047619048e-05,
      "loss": 0.0314,
      "step": 476
    },
    {
      "epoch": 7.511811023622047,
      "grad_norm": 1.5786348581314087,
      "learning_rate": 5.015873015873016e-05,
      "loss": 0.0462,
      "step": 477
    },
    {
      "epoch": 7.52755905511811,
      "grad_norm": 2.2168502807617188,
      "learning_rate": 4.9841269841269845e-05,
      "loss": 0.0639,
      "step": 478
    },
    {
      "epoch": 7.543307086614173,
      "grad_norm": 3.0064573287963867,
      "learning_rate": 4.9523809523809525e-05,
      "loss": 0.0685,
      "step": 479
    },
    {
      "epoch": 7.559055118110236,
      "grad_norm": 2.7508742809295654,
      "learning_rate": 4.9206349206349204e-05,
      "loss": 0.0829,
      "step": 480
    },
    {
      "epoch": 7.574803149606299,
      "grad_norm": 2.143519639968872,
      "learning_rate": 4.888888888888889e-05,
      "loss": 0.0663,
      "step": 481
    },
    {
      "epoch": 7.590551181102362,
      "grad_norm": 4.233808994293213,
      "learning_rate": 4.8571428571428576e-05,
      "loss": 0.0691,
      "step": 482
    },
    {
      "epoch": 7.606299212598425,
      "grad_norm": 2.050644874572754,
      "learning_rate": 4.8253968253968255e-05,
      "loss": 0.0632,
      "step": 483
    },
    {
      "epoch": 7.622047244094488,
      "grad_norm": 2.0356483459472656,
      "learning_rate": 4.793650793650794e-05,
      "loss": 0.0621,
      "step": 484
    },
    {
      "epoch": 7.637795275590551,
      "grad_norm": 1.4364875555038452,
      "learning_rate": 4.761904761904762e-05,
      "loss": 0.0387,
      "step": 485
    },
    {
      "epoch": 7.653543307086614,
      "grad_norm": 1.8473832607269287,
      "learning_rate": 4.73015873015873e-05,
      "loss": 0.0639,
      "step": 486
    },
    {
      "epoch": 7.669291338582677,
      "grad_norm": 1.8580067157745361,
      "learning_rate": 4.6984126984126986e-05,
      "loss": 0.0391,
      "step": 487
    },
    {
      "epoch": 7.68503937007874,
      "grad_norm": 2.767592191696167,
      "learning_rate": 4.666666666666667e-05,
      "loss": 0.0745,
      "step": 488
    },
    {
      "epoch": 7.700787401574803,
      "grad_norm": 2.346900701522827,
      "learning_rate": 4.634920634920635e-05,
      "loss": 0.0494,
      "step": 489
    },
    {
      "epoch": 7.716535433070866,
      "grad_norm": 1.851671576499939,
      "learning_rate": 4.603174603174603e-05,
      "loss": 0.067,
      "step": 490
    },
    {
      "epoch": 7.73228346456693,
      "grad_norm": 1.8094067573547363,
      "learning_rate": 4.5714285714285716e-05,
      "loss": 0.0462,
      "step": 491
    },
    {
      "epoch": 7.748031496062993,
      "grad_norm": 2.4940598011016846,
      "learning_rate": 4.5396825396825395e-05,
      "loss": 0.0612,
      "step": 492
    },
    {
      "epoch": 7.7637795275590555,
      "grad_norm": 1.8592227697372437,
      "learning_rate": 4.507936507936508e-05,
      "loss": 0.0502,
      "step": 493
    },
    {
      "epoch": 7.7795275590551185,
      "grad_norm": 2.6595776081085205,
      "learning_rate": 4.476190476190477e-05,
      "loss": 0.0678,
      "step": 494
    },
    {
      "epoch": 7.7952755905511815,
      "grad_norm": 2.7442445755004883,
      "learning_rate": 4.4444444444444447e-05,
      "loss": 0.0847,
      "step": 495
    },
    {
      "epoch": 7.811023622047244,
      "grad_norm": 1.5332573652267456,
      "learning_rate": 4.4126984126984126e-05,
      "loss": 0.0447,
      "step": 496
    },
    {
      "epoch": 7.826771653543307,
      "grad_norm": 3.267883777618408,
      "learning_rate": 4.380952380952381e-05,
      "loss": 0.0781,
      "step": 497
    },
    {
      "epoch": 7.84251968503937,
      "grad_norm": 1.8700013160705566,
      "learning_rate": 4.34920634920635e-05,
      "loss": 0.0596,
      "step": 498
    },
    {
      "epoch": 7.858267716535433,
      "grad_norm": 1.774317979812622,
      "learning_rate": 4.317460317460318e-05,
      "loss": 0.054,
      "step": 499
    },
    {
      "epoch": 7.874015748031496,
      "grad_norm": 2.5844216346740723,
      "learning_rate": 4.2857142857142856e-05,
      "loss": 0.0669,
      "step": 500
    },
    {
      "epoch": 7.874015748031496,
      "eval_loss": 0.2612713873386383,
      "eval_runtime": 21.6575,
      "eval_samples_per_second": 2.955,
      "eval_steps_per_second": 0.369,
      "step": 500
    },
    {
      "epoch": 7.889763779527559,
      "grad_norm": 1.7064733505249023,
      "learning_rate": 4.253968253968254e-05,
      "loss": 0.0597,
      "step": 501
    },
    {
      "epoch": 7.905511811023622,
      "grad_norm": 1.9450037479400635,
      "learning_rate": 4.222222222222222e-05,
      "loss": 0.0603,
      "step": 502
    },
    {
      "epoch": 7.921259842519685,
      "grad_norm": 2.6481454372406006,
      "learning_rate": 4.190476190476191e-05,
      "loss": 0.0652,
      "step": 503
    },
    {
      "epoch": 7.937007874015748,
      "grad_norm": 1.2371290922164917,
      "learning_rate": 4.1587301587301594e-05,
      "loss": 0.0357,
      "step": 504
    },
    {
      "epoch": 7.952755905511811,
      "grad_norm": 2.797078847885132,
      "learning_rate": 4.126984126984127e-05,
      "loss": 0.0817,
      "step": 505
    },
    {
      "epoch": 7.968503937007874,
      "grad_norm": 1.6661362648010254,
      "learning_rate": 4.095238095238095e-05,
      "loss": 0.0511,
      "step": 506
    },
    {
      "epoch": 7.984251968503937,
      "grad_norm": 1.6574702262878418,
      "learning_rate": 4.063492063492064e-05,
      "loss": 0.0589,
      "step": 507
    },
    {
      "epoch": 8.0,
      "grad_norm": 1.9499527215957642,
      "learning_rate": 4.031746031746032e-05,
      "loss": 0.0516,
      "step": 508
    },
    {
      "epoch": 8.015748031496063,
      "grad_norm": 1.1899126768112183,
      "learning_rate": 4e-05,
      "loss": 0.0341,
      "step": 509
    },
    {
      "epoch": 8.031496062992126,
      "grad_norm": 1.2641419172286987,
      "learning_rate": 3.968253968253968e-05,
      "loss": 0.0378,
      "step": 510
    },
    {
      "epoch": 8.047244094488189,
      "grad_norm": 1.2355155944824219,
      "learning_rate": 3.936507936507937e-05,
      "loss": 0.0376,
      "step": 511
    },
    {
      "epoch": 8.062992125984252,
      "grad_norm": 1.7464691400527954,
      "learning_rate": 3.904761904761905e-05,
      "loss": 0.0443,
      "step": 512
    },
    {
      "epoch": 8.078740157480315,
      "grad_norm": 2.0570523738861084,
      "learning_rate": 3.8730158730158734e-05,
      "loss": 0.0551,
      "step": 513
    },
    {
      "epoch": 8.094488188976378,
      "grad_norm": 1.29805588722229,
      "learning_rate": 3.841269841269842e-05,
      "loss": 0.039,
      "step": 514
    },
    {
      "epoch": 8.11023622047244,
      "grad_norm": 2.512799024581909,
      "learning_rate": 3.809523809523809e-05,
      "loss": 0.047,
      "step": 515
    },
    {
      "epoch": 8.125984251968504,
      "grad_norm": 3.337170362472534,
      "learning_rate": 3.777777777777778e-05,
      "loss": 0.0341,
      "step": 516
    },
    {
      "epoch": 8.141732283464567,
      "grad_norm": 2.3449106216430664,
      "learning_rate": 3.7460317460317464e-05,
      "loss": 0.0448,
      "step": 517
    },
    {
      "epoch": 8.15748031496063,
      "grad_norm": 1.8002575635910034,
      "learning_rate": 3.7142857142857143e-05,
      "loss": 0.0434,
      "step": 518
    },
    {
      "epoch": 8.173228346456693,
      "grad_norm": 1.4089127779006958,
      "learning_rate": 3.682539682539683e-05,
      "loss": 0.0454,
      "step": 519
    },
    {
      "epoch": 8.188976377952756,
      "grad_norm": 2.1421875953674316,
      "learning_rate": 3.650793650793651e-05,
      "loss": 0.0637,
      "step": 520
    },
    {
      "epoch": 8.204724409448819,
      "grad_norm": 1.7673617601394653,
      "learning_rate": 3.619047619047619e-05,
      "loss": 0.0485,
      "step": 521
    },
    {
      "epoch": 8.220472440944881,
      "grad_norm": 1.4105192422866821,
      "learning_rate": 3.5873015873015874e-05,
      "loss": 0.044,
      "step": 522
    },
    {
      "epoch": 8.236220472440944,
      "grad_norm": 5.387270450592041,
      "learning_rate": 3.555555555555556e-05,
      "loss": 0.0501,
      "step": 523
    },
    {
      "epoch": 8.251968503937007,
      "grad_norm": 1.7804691791534424,
      "learning_rate": 3.523809523809524e-05,
      "loss": 0.0384,
      "step": 524
    },
    {
      "epoch": 8.26771653543307,
      "grad_norm": 1.6330279111862183,
      "learning_rate": 3.492063492063492e-05,
      "loss": 0.0434,
      "step": 525
    },
    {
      "epoch": 8.283464566929133,
      "grad_norm": 1.9107962846755981,
      "learning_rate": 3.4603174603174604e-05,
      "loss": 0.0363,
      "step": 526
    },
    {
      "epoch": 8.299212598425196,
      "grad_norm": 2.040088415145874,
      "learning_rate": 3.428571428571429e-05,
      "loss": 0.0554,
      "step": 527
    },
    {
      "epoch": 8.31496062992126,
      "grad_norm": 2.077286720275879,
      "learning_rate": 3.396825396825397e-05,
      "loss": 0.0479,
      "step": 528
    },
    {
      "epoch": 8.330708661417322,
      "grad_norm": 1.44839346408844,
      "learning_rate": 3.3650793650793656e-05,
      "loss": 0.0411,
      "step": 529
    },
    {
      "epoch": 8.346456692913385,
      "grad_norm": 1.7733572721481323,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 0.0346,
      "step": 530
    },
    {
      "epoch": 8.362204724409448,
      "grad_norm": 1.2276086807250977,
      "learning_rate": 3.3015873015873014e-05,
      "loss": 0.0428,
      "step": 531
    },
    {
      "epoch": 8.377952755905511,
      "grad_norm": 3.093677282333374,
      "learning_rate": 3.26984126984127e-05,
      "loss": 0.1001,
      "step": 532
    },
    {
      "epoch": 8.393700787401574,
      "grad_norm": 1.6303534507751465,
      "learning_rate": 3.2380952380952386e-05,
      "loss": 0.0423,
      "step": 533
    },
    {
      "epoch": 8.409448818897637,
      "grad_norm": 2.195538282394409,
      "learning_rate": 3.2063492063492065e-05,
      "loss": 0.0525,
      "step": 534
    },
    {
      "epoch": 8.4251968503937,
      "grad_norm": 2.2096023559570312,
      "learning_rate": 3.1746031746031745e-05,
      "loss": 0.0547,
      "step": 535
    },
    {
      "epoch": 8.440944881889763,
      "grad_norm": 1.7774571180343628,
      "learning_rate": 3.142857142857143e-05,
      "loss": 0.0573,
      "step": 536
    },
    {
      "epoch": 8.456692913385826,
      "grad_norm": 2.284461498260498,
      "learning_rate": 3.111111111111111e-05,
      "loss": 0.0482,
      "step": 537
    },
    {
      "epoch": 8.472440944881889,
      "grad_norm": 2.587062358856201,
      "learning_rate": 3.0793650793650796e-05,
      "loss": 0.0535,
      "step": 538
    },
    {
      "epoch": 8.488188976377952,
      "grad_norm": 2.109731912612915,
      "learning_rate": 3.0476190476190482e-05,
      "loss": 0.0561,
      "step": 539
    },
    {
      "epoch": 8.503937007874015,
      "grad_norm": 2.232177734375,
      "learning_rate": 3.0158730158730158e-05,
      "loss": 0.0544,
      "step": 540
    },
    {
      "epoch": 8.519685039370078,
      "grad_norm": 1.873109221458435,
      "learning_rate": 2.9841269841269844e-05,
      "loss": 0.0441,
      "step": 541
    },
    {
      "epoch": 8.535433070866143,
      "grad_norm": 2.0851778984069824,
      "learning_rate": 2.9523809523809526e-05,
      "loss": 0.0518,
      "step": 542
    },
    {
      "epoch": 8.551181102362206,
      "grad_norm": 1.6063165664672852,
      "learning_rate": 2.920634920634921e-05,
      "loss": 0.0412,
      "step": 543
    },
    {
      "epoch": 8.566929133858268,
      "grad_norm": 2.391376495361328,
      "learning_rate": 2.8888888888888888e-05,
      "loss": 0.0707,
      "step": 544
    },
    {
      "epoch": 8.582677165354331,
      "grad_norm": 1.0743218660354614,
      "learning_rate": 2.857142857142857e-05,
      "loss": 0.0434,
      "step": 545
    },
    {
      "epoch": 8.598425196850394,
      "grad_norm": 1.6495745182037354,
      "learning_rate": 2.8253968253968253e-05,
      "loss": 0.046,
      "step": 546
    },
    {
      "epoch": 8.614173228346457,
      "grad_norm": 1.5505526065826416,
      "learning_rate": 2.793650793650794e-05,
      "loss": 0.0493,
      "step": 547
    },
    {
      "epoch": 8.62992125984252,
      "grad_norm": 2.091827869415283,
      "learning_rate": 2.7619047619047622e-05,
      "loss": 0.0711,
      "step": 548
    },
    {
      "epoch": 8.645669291338583,
      "grad_norm": 1.6031221151351929,
      "learning_rate": 2.73015873015873e-05,
      "loss": 0.0431,
      "step": 549
    },
    {
      "epoch": 8.661417322834646,
      "grad_norm": 2.657233238220215,
      "learning_rate": 2.6984126984126984e-05,
      "loss": 0.0495,
      "step": 550
    },
    {
      "epoch": 8.67716535433071,
      "grad_norm": 1.5111783742904663,
      "learning_rate": 2.6666666666666667e-05,
      "loss": 0.0391,
      "step": 551
    },
    {
      "epoch": 8.692913385826772,
      "grad_norm": 1.8742907047271729,
      "learning_rate": 2.6349206349206353e-05,
      "loss": 0.0365,
      "step": 552
    },
    {
      "epoch": 8.708661417322835,
      "grad_norm": 1.3074357509613037,
      "learning_rate": 2.6031746031746035e-05,
      "loss": 0.0457,
      "step": 553
    },
    {
      "epoch": 8.724409448818898,
      "grad_norm": 1.4582730531692505,
      "learning_rate": 2.5714285714285714e-05,
      "loss": 0.0442,
      "step": 554
    },
    {
      "epoch": 8.740157480314961,
      "grad_norm": 1.8088982105255127,
      "learning_rate": 2.5396825396825397e-05,
      "loss": 0.0458,
      "step": 555
    },
    {
      "epoch": 8.755905511811024,
      "grad_norm": 2.2896173000335693,
      "learning_rate": 2.507936507936508e-05,
      "loss": 0.0649,
      "step": 556
    },
    {
      "epoch": 8.771653543307087,
      "grad_norm": 2.611921787261963,
      "learning_rate": 2.4761904761904762e-05,
      "loss": 0.0458,
      "step": 557
    },
    {
      "epoch": 8.78740157480315,
      "grad_norm": 1.5734719038009644,
      "learning_rate": 2.4444444444444445e-05,
      "loss": 0.0544,
      "step": 558
    },
    {
      "epoch": 8.803149606299213,
      "grad_norm": 2.7637977600097656,
      "learning_rate": 2.4126984126984128e-05,
      "loss": 0.0544,
      "step": 559
    },
    {
      "epoch": 8.818897637795276,
      "grad_norm": 1.2042934894561768,
      "learning_rate": 2.380952380952381e-05,
      "loss": 0.0374,
      "step": 560
    },
    {
      "epoch": 8.834645669291339,
      "grad_norm": 1.30482816696167,
      "learning_rate": 2.3492063492063493e-05,
      "loss": 0.0453,
      "step": 561
    },
    {
      "epoch": 8.850393700787402,
      "grad_norm": 3.3566031455993652,
      "learning_rate": 2.3174603174603175e-05,
      "loss": 0.0466,
      "step": 562
    },
    {
      "epoch": 8.866141732283465,
      "grad_norm": 3.4217185974121094,
      "learning_rate": 2.2857142857142858e-05,
      "loss": 0.0524,
      "step": 563
    },
    {
      "epoch": 8.881889763779528,
      "grad_norm": 1.3176681995391846,
      "learning_rate": 2.253968253968254e-05,
      "loss": 0.039,
      "step": 564
    },
    {
      "epoch": 8.89763779527559,
      "grad_norm": 2.0422184467315674,
      "learning_rate": 2.2222222222222223e-05,
      "loss": 0.0575,
      "step": 565
    },
    {
      "epoch": 8.913385826771654,
      "grad_norm": 1.1718577146530151,
      "learning_rate": 2.1904761904761906e-05,
      "loss": 0.0373,
      "step": 566
    },
    {
      "epoch": 8.929133858267717,
      "grad_norm": 2.6834802627563477,
      "learning_rate": 2.158730158730159e-05,
      "loss": 0.072,
      "step": 567
    },
    {
      "epoch": 8.94488188976378,
      "grad_norm": 1.822182297706604,
      "learning_rate": 2.126984126984127e-05,
      "loss": 0.044,
      "step": 568
    },
    {
      "epoch": 8.960629921259843,
      "grad_norm": 1.8544002771377563,
      "learning_rate": 2.0952380952380954e-05,
      "loss": 0.0419,
      "step": 569
    },
    {
      "epoch": 8.976377952755906,
      "grad_norm": 1.360914707183838,
      "learning_rate": 2.0634920634920636e-05,
      "loss": 0.0344,
      "step": 570
    },
    {
      "epoch": 8.992125984251969,
      "grad_norm": 1.9901163578033447,
      "learning_rate": 2.031746031746032e-05,
      "loss": 0.0605,
      "step": 571
    },
    {
      "epoch": 9.007874015748031,
      "grad_norm": 2.2954354286193848,
      "learning_rate": 2e-05,
      "loss": 0.0461,
      "step": 572
    },
    {
      "epoch": 9.023622047244094,
      "grad_norm": 0.9337184429168701,
      "learning_rate": 1.9682539682539684e-05,
      "loss": 0.0328,
      "step": 573
    },
    {
      "epoch": 9.039370078740157,
      "grad_norm": 1.4115442037582397,
      "learning_rate": 1.9365079365079367e-05,
      "loss": 0.0337,
      "step": 574
    },
    {
      "epoch": 9.05511811023622,
      "grad_norm": 1.1217656135559082,
      "learning_rate": 1.9047619047619046e-05,
      "loss": 0.0318,
      "step": 575
    },
    {
      "epoch": 9.070866141732283,
      "grad_norm": 1.008010745048523,
      "learning_rate": 1.8730158730158732e-05,
      "loss": 0.0353,
      "step": 576
    },
    {
      "epoch": 9.086614173228346,
      "grad_norm": 3.152632236480713,
      "learning_rate": 1.8412698412698415e-05,
      "loss": 0.0594,
      "step": 577
    },
    {
      "epoch": 9.10236220472441,
      "grad_norm": 1.2215298414230347,
      "learning_rate": 1.8095238095238094e-05,
      "loss": 0.0381,
      "step": 578
    },
    {
      "epoch": 9.118110236220472,
      "grad_norm": 1.9573944807052612,
      "learning_rate": 1.777777777777778e-05,
      "loss": 0.0467,
      "step": 579
    },
    {
      "epoch": 9.133858267716535,
      "grad_norm": 1.5528653860092163,
      "learning_rate": 1.746031746031746e-05,
      "loss": 0.0503,
      "step": 580
    },
    {
      "epoch": 9.149606299212598,
      "grad_norm": 1.1518275737762451,
      "learning_rate": 1.7142857142857145e-05,
      "loss": 0.0329,
      "step": 581
    },
    {
      "epoch": 9.165354330708661,
      "grad_norm": 1.9555214643478394,
      "learning_rate": 1.6825396825396828e-05,
      "loss": 0.049,
      "step": 582
    },
    {
      "epoch": 9.181102362204724,
      "grad_norm": 2.0796003341674805,
      "learning_rate": 1.6507936507936507e-05,
      "loss": 0.0462,
      "step": 583
    },
    {
      "epoch": 9.196850393700787,
      "grad_norm": 1.369624376296997,
      "learning_rate": 1.6190476190476193e-05,
      "loss": 0.0361,
      "step": 584
    },
    {
      "epoch": 9.21259842519685,
      "grad_norm": 1.6113625764846802,
      "learning_rate": 1.5873015873015872e-05,
      "loss": 0.042,
      "step": 585
    },
    {
      "epoch": 9.228346456692913,
      "grad_norm": 2.035371780395508,
      "learning_rate": 1.5555555555555555e-05,
      "loss": 0.0393,
      "step": 586
    },
    {
      "epoch": 9.244094488188976,
      "grad_norm": 2.365569829940796,
      "learning_rate": 1.5238095238095241e-05,
      "loss": 0.0371,
      "step": 587
    },
    {
      "epoch": 9.259842519685039,
      "grad_norm": 1.0499751567840576,
      "learning_rate": 1.4920634920634922e-05,
      "loss": 0.037,
      "step": 588
    },
    {
      "epoch": 9.275590551181102,
      "grad_norm": 1.5478347539901733,
      "learning_rate": 1.4603174603174605e-05,
      "loss": 0.0436,
      "step": 589
    },
    {
      "epoch": 9.291338582677165,
      "grad_norm": 1.1099342107772827,
      "learning_rate": 1.4285714285714285e-05,
      "loss": 0.0404,
      "step": 590
    },
    {
      "epoch": 9.307086614173228,
      "grad_norm": 1.4756035804748535,
      "learning_rate": 1.396825396825397e-05,
      "loss": 0.0343,
      "step": 591
    },
    {
      "epoch": 9.32283464566929,
      "grad_norm": 1.58535897731781,
      "learning_rate": 1.365079365079365e-05,
      "loss": 0.0316,
      "step": 592
    },
    {
      "epoch": 9.338582677165354,
      "grad_norm": 1.6492764949798584,
      "learning_rate": 1.3333333333333333e-05,
      "loss": 0.0411,
      "step": 593
    },
    {
      "epoch": 9.354330708661417,
      "grad_norm": 1.6652787923812866,
      "learning_rate": 1.3015873015873018e-05,
      "loss": 0.0421,
      "step": 594
    },
    {
      "epoch": 9.37007874015748,
      "grad_norm": 1.291868805885315,
      "learning_rate": 1.2698412698412699e-05,
      "loss": 0.0328,
      "step": 595
    },
    {
      "epoch": 9.385826771653543,
      "grad_norm": 1.8582468032836914,
      "learning_rate": 1.2380952380952381e-05,
      "loss": 0.0367,
      "step": 596
    },
    {
      "epoch": 9.401574803149606,
      "grad_norm": 1.8856614828109741,
      "learning_rate": 1.2063492063492064e-05,
      "loss": 0.0578,
      "step": 597
    },
    {
      "epoch": 9.417322834645669,
      "grad_norm": 1.956943154335022,
      "learning_rate": 1.1746031746031746e-05,
      "loss": 0.0451,
      "step": 598
    },
    {
      "epoch": 9.433070866141732,
      "grad_norm": 2.906054973602295,
      "learning_rate": 1.1428571428571429e-05,
      "loss": 0.0467,
      "step": 599
    },
    {
      "epoch": 9.448818897637794,
      "grad_norm": 2.488290786743164,
      "learning_rate": 1.1111111111111112e-05,
      "loss": 0.058,
      "step": 600
    },
    {
      "epoch": 9.464566929133857,
      "grad_norm": 2.2739510536193848,
      "learning_rate": 1.0793650793650794e-05,
      "loss": 0.0488,
      "step": 601
    },
    {
      "epoch": 9.48031496062992,
      "grad_norm": 2.036039352416992,
      "learning_rate": 1.0476190476190477e-05,
      "loss": 0.0511,
      "step": 602
    },
    {
      "epoch": 9.496062992125985,
      "grad_norm": 1.7022149562835693,
      "learning_rate": 1.015873015873016e-05,
      "loss": 0.0391,
      "step": 603
    },
    {
      "epoch": 9.511811023622048,
      "grad_norm": 2.33685040473938,
      "learning_rate": 9.841269841269842e-06,
      "loss": 0.0398,
      "step": 604
    },
    {
      "epoch": 9.527559055118111,
      "grad_norm": 2.1218082904815674,
      "learning_rate": 9.523809523809523e-06,
      "loss": 0.0405,
      "step": 605
    },
    {
      "epoch": 9.543307086614174,
      "grad_norm": 1.425999402999878,
      "learning_rate": 9.206349206349207e-06,
      "loss": 0.0483,
      "step": 606
    },
    {
      "epoch": 9.559055118110237,
      "grad_norm": 1.7567709684371948,
      "learning_rate": 8.88888888888889e-06,
      "loss": 0.0326,
      "step": 607
    },
    {
      "epoch": 9.5748031496063,
      "grad_norm": 1.3061392307281494,
      "learning_rate": 8.571428571428573e-06,
      "loss": 0.049,
      "step": 608
    },
    {
      "epoch": 9.590551181102363,
      "grad_norm": 1.8873316049575806,
      "learning_rate": 8.253968253968254e-06,
      "loss": 0.0529,
      "step": 609
    },
    {
      "epoch": 9.606299212598426,
      "grad_norm": 2.600581169128418,
      "learning_rate": 7.936507936507936e-06,
      "loss": 0.0458,
      "step": 610
    },
    {
      "epoch": 9.622047244094489,
      "grad_norm": 1.2196403741836548,
      "learning_rate": 7.6190476190476205e-06,
      "loss": 0.0362,
      "step": 611
    },
    {
      "epoch": 9.637795275590552,
      "grad_norm": 3.1539254188537598,
      "learning_rate": 7.301587301587302e-06,
      "loss": 0.0576,
      "step": 612
    },
    {
      "epoch": 9.653543307086615,
      "grad_norm": 1.5741316080093384,
      "learning_rate": 6.984126984126985e-06,
      "loss": 0.0424,
      "step": 613
    },
    {
      "epoch": 9.669291338582678,
      "grad_norm": 1.8623982667922974,
      "learning_rate": 6.666666666666667e-06,
      "loss": 0.0416,
      "step": 614
    },
    {
      "epoch": 9.68503937007874,
      "grad_norm": 1.8562568426132202,
      "learning_rate": 6.349206349206349e-06,
      "loss": 0.0432,
      "step": 615
    },
    {
      "epoch": 9.700787401574804,
      "grad_norm": 2.048351764678955,
      "learning_rate": 6.031746031746032e-06,
      "loss": 0.0378,
      "step": 616
    },
    {
      "epoch": 9.716535433070867,
      "grad_norm": 2.2787129878997803,
      "learning_rate": 5.7142857142857145e-06,
      "loss": 0.0632,
      "step": 617
    },
    {
      "epoch": 9.73228346456693,
      "grad_norm": 2.128246307373047,
      "learning_rate": 5.396825396825397e-06,
      "loss": 0.0619,
      "step": 618
    },
    {
      "epoch": 9.748031496062993,
      "grad_norm": 1.2656891345977783,
      "learning_rate": 5.07936507936508e-06,
      "loss": 0.0343,
      "step": 619
    },
    {
      "epoch": 9.763779527559056,
      "grad_norm": 2.0870344638824463,
      "learning_rate": 4.7619047619047615e-06,
      "loss": 0.0408,
      "step": 620
    },
    {
      "epoch": 9.779527559055119,
      "grad_norm": 1.4776500463485718,
      "learning_rate": 4.444444444444445e-06,
      "loss": 0.0347,
      "step": 621
    },
    {
      "epoch": 9.795275590551181,
      "grad_norm": 2.1384265422821045,
      "learning_rate": 4.126984126984127e-06,
      "loss": 0.0526,
      "step": 622
    },
    {
      "epoch": 9.811023622047244,
      "grad_norm": 1.8905365467071533,
      "learning_rate": 3.8095238095238102e-06,
      "loss": 0.0305,
      "step": 623
    },
    {
      "epoch": 9.826771653543307,
      "grad_norm": 1.6220651865005493,
      "learning_rate": 3.4920634920634924e-06,
      "loss": 0.0383,
      "step": 624
    },
    {
      "epoch": 9.84251968503937,
      "grad_norm": 1.5884040594100952,
      "learning_rate": 3.1746031746031746e-06,
      "loss": 0.0423,
      "step": 625
    },
    {
      "epoch": 9.858267716535433,
      "grad_norm": 1.928929328918457,
      "learning_rate": 2.8571428571428573e-06,
      "loss": 0.0405,
      "step": 626
    },
    {
      "epoch": 9.874015748031496,
      "grad_norm": 1.3656039237976074,
      "learning_rate": 2.53968253968254e-06,
      "loss": 0.0409,
      "step": 627
    },
    {
      "epoch": 9.88976377952756,
      "grad_norm": 1.1011022329330444,
      "learning_rate": 2.2222222222222225e-06,
      "loss": 0.0405,
      "step": 628
    },
    {
      "epoch": 9.905511811023622,
      "grad_norm": 2.2035796642303467,
      "learning_rate": 1.9047619047619051e-06,
      "loss": 0.0439,
      "step": 629
    },
    {
      "epoch": 9.921259842519685,
      "grad_norm": 1.2772020101547241,
      "learning_rate": 1.5873015873015873e-06,
      "loss": 0.0332,
      "step": 630
    }
  ],
  "logging_steps": 1,
  "max_steps": 630,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10,
  "save_steps": 500,
  "stateful_callbacks": {
    "EarlyStoppingCallback": {
      "args": {
        "early_stopping_patience": 2,
        "early_stopping_threshold": 0.0
      },
      "attributes": {
        "early_stopping_patience_counter": 0
      }
    },
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.753656516509696e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
